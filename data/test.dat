developing parallel sense-tagged corpora with wordnets eshley huini gao , hazel shuwen mok , jeanette yiwen tan
semantically annotated corpora play an important role in natural language processing . this paper presents the results of a pilot study on building a sense-tagged parallel corpus , part of ongoing construction of aligned corpora for four languages ( english , chinese , japanese , and indonesian ) in four domains ( story , essay , news , and tourism ) from the ntu-multilingual corpus . each subcorpus is first sensetagged using a wordnet and then these synsets are linked . upon the completion of this project , all annotated corpora will be made freely available . the multilingual corpora are designed to not only provide data for nlp tasks like machine translation , but also to contribute to the study of translation shift and bilingual lexicography as well as the improvement of monolingual wordnets .

chinese word segmentation based on mixing model
this paper presents our recent work for participation in the second international chinese word segmentation bakeoff . according to difficulties , we divide word segmentation into several sub-tasks , which are solved by mixed language models , so as to take advantage of each approach in addressing special problems . the experiment indicated that this system achieved 96.7 % and 97.2 % in f-measure in pku and msr open test respectively .

unsupervised grammar induction by distribution and attachment
distributional approaches to grammar induction are typically inefficient , enumerating large numbers of candidate constituents . in this paper , we describe a simplified model of distributional analysis which uses heuristics to reduce the number of candidate constituents under consideration . we apply this model to a large corpus of over 400000 words of written english , and evaluate the results using evalb . we show that the performance of this approach is limited , providing a detailed analysis of learned structure and a comparison with actual constituentcontext distributions . this motivates a more structured approach , using a process of attachment to form constituents from their distributional components . our findings suggest that distributional methods do not generalize enough to learn syntax effectively from raw text , but that attachment methods are more successful .

using sketches to estimate associations
we should not have to look at the entire corpus ( e.g. , the web ) to know if two words are associated or not.1 a powerful sampling technique called sketches was originally introduced to remove duplicate web pages . we generalize sketches to estimate contingency tables and associations , using a maximum likelihood estimator to find the most likely contingency table given the sample , the margins ( document frequencies ) and the size of the collection . not unsurprisingly , computational work and statistical accuracy ( variance or errors ) depend on sampling rate , as will be shown both theoretically and empirically . sampling methods become more and more important with larger and larger collections . at web scale , sampling rates as low as 104 may suffice .

integrating collocation features in chinese word sense
the selection of features is critical in providing discriminative information for classifiers in word sense disambiguation ( wsd ) . uninformative features will degrade the performance of classifiers . based on the strong evidence that an ambiguous word expresses a unique sense in a given collocation , this paper reports our experiments on automatic wsd using collocation as local features based on the corpus extracted from peoples daily news ( pdn ) as well as the standard senseval-3 data set . using the nave bayes classifier as our core algorithm , we have implemented a classifier using a feature set combining both local collocation features and topical features . the average precision on the pdn corpus has 3.2 % improvement compared to 81.5 % of the baseline system where collocation features are not considered . for the senseval-3 data , we have reached the precision rate of 37.6 % by integrating collocation features into contextual features , to achieve 37 % improvement over 26.7 % of precision in the baseline system . our experiments have shown that collocation features can be used to reduce the size of human tagged corpus .

a transitive model for extracting translation equivalents of web queries through anchor text mining
one of the existing difficulties of cross-language information retrieval ( clir ) and web search is the lack of appropriate translations of new terminology and proper names . different from conventional approaches , in our previous research we developed an approach for exploiting web anchor texts as live bilingual corpora and reducing the existing difficulties of query term translation . although web anchor texts , undoubtedly , are very valuable multilingual and wide-scoped hypertext resources , not every particular pair of languages contains sufficient anchor texts in the web to extract corresponding translations in the language pair . for more generalized applications , in this paper we extend our previous approach by adding a phase of transitive ( indirect ) translation via an intermediate ( third ) language , and propose a transitive model to further exploit anchor-text mining in term translation extraction applications . preliminary experimental results show that many query translations which can not be obtained using the previous approach can be extracted with the improved approach .

ecnu : expression- and message-level sentiment orientation classification in twitter using multiple effective features , tian tian zhu
microblogging websites ( such as twitter , facebook ) are rich sources of data for opinion mining and sentiment analysis . in this paper , we describe our approaches used for sentiment analysis in twitter ( task 9 ) organized in semeval 2014. this task tries to determine whether the sentiment orientations conveyed by the whole tweets or pieces of tweets are positive , negative or neutral . to solve this problem , we extracted several simple and basic features considering the following aspects : surface text , syntax , sentiment score and twitter characteristic . then we exploited these features to build a classifier using svm algorithm . despite the simplicity of features , our systems rank above the average .

exploiting non-local features for spoken language understanding
in this paper , we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding ( slu ) problem . the statistical natural language parsers trained on text perform unreliably to encode non-local information on spoken language . an alternative method we propose is to use trigger pairs that are automatically extracted by a feature induction algorithm . we describe a light version of the inducer in which a simple modification is efficient and successful . we evaluate our method on an slu task and show an error reduction of up to 27 % over the base local model .

detecting structural metadata with decision trees and
the regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech . detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts . this paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources . specifically , combinations of decision trees and language models are used to predict sentence ends and interruption points and , given these events , transformationbased learning is used to detect edit disfluencies and conversational fillers . results are reported on human and automatic transcripts of conversational telephone speech .

unsupervised morphological segmentation with log-linear models
morphological segmentation breaks words into morphemes ( the basic semantic units ) . it is a key component for natural language processing systems . unsupervised morphological segmentation is attractive , because in every language there are virtually unlimited supplies of text , but very few labeled resources . however , most existing model-based systems for unsupervised morphological segmentation use directed generative models , making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning . in this paper , we present the first log-linear model for unsupervised morphological segmentation . our model uses overlapping features such as morphemes and their contexts , and incorporates exponential priors inspired by the minimum description length ( mdl ) principle . we present efficient algorithms for learning and inference by combining contrastive estimation with sampling . our system , based on monolingual features only , outperforms a state-of-the-art system by a large margin , even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence . on the arabic penn treebank , our system reduces f1 error by 11 % compared to morfessor .

hiddenvariable models for discriminative reranking
we describe a new method for the representation of nlp structures within reranking approaches . we make use of a conditional loglinear model , with hidden variables representing the assignment of lexical items to word clusters or word senses . the model learns to automatically make these assignments based on a discriminative training criterion . training and decoding with the model requires summing over an exponential number of hidden variable assignments : the required summations can be computed efficiently and exactly using dynamic programming . as a case study , we apply the model to parse reranking . the model gives an f measure improvement of 1.25 % beyond the base parser , and an 0.25 % improvement beyond the collins ( 2000 ) reranker . although our experiments are focused on parsing , the techniques described generalize naturally to nlp structures other than parse trees .

generating referring expressions in open domains advaith siddharthan ann copestake
we present an algorithm for generating referring expressions in open domains . existing algorithms work at the semantic level and assume the availability of a classification for attributes , which is only feasible for restricted domains . our alternative works at the realisation level , relies on wordnet synonym and antonym sets , and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches can not handle . we believe that ours is also the first algorithm that allows for the incremental incorporation of relations . we present a novel corpus-evaluation using referring expressions from the penn wall street journal treebank .

a patent document retrieval system addressing naoyuki tokuda hisahiro adachi
combining the principle of differential latent semantic index ( dlsi ) ( chen et al. , 2001 ) and the template matching technique ( tokuda and chen , 2001 ) , we propose a new user queries-based patent document retrieval system by nlp technology . the dlsi method first narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploiting the words-based template matching scheme by syntactic search . compared with the synonymous search scheme by thesaurus dictionaries , the new method results in an improved overall retrieval efficiency of patent documents .

parsing german with latent variable grammars
we describe experiments on learning latent variable grammars for various german treebanks , using a language-agnostic statistical approach . in our method , a minimal initial grammar is hierarchically refined using an adaptive split-and-merge em procedure , giving compact , accurate grammars . the learning procedure directly maximizes the likelihood of the training treebank , without the use of any language specific or linguistically constrained features . nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three german treebanks .

a systematic bayesian treatment of the ibm alignment models
the dominant yet ageing ibm and hmm word alignment models underpin most popular statistical machine translation implementations in use today . though beset by the limitations of implausible independence assumptions , intractable optimisation problems , and an excess of tunable parameters , these models provide a scalable and reliable starting point for inducing translation systems . in this paper we build upon this venerable base by recasting these models in the non-parametric bayesian framework . by replacing the categorical distributions at their core with hierarchical pitman-yor processes , and through the use of collapsed gibbs sampling , we provide a more flexible formulation and sidestep the original heuristic optimisation techniques . the resulting models are highly extendible ,

ranking multidocument event descriptions for building thematic timelines
this paper tackles the problem of timeline generation from traditional news sources . our system builds thematic timelines for a general-domain topic defined by a user query . the system selects and ranks events relevant to the input query . each event is represented by a one-sentence description in the output timeline . we present an inter-cluster ranking algorithm that takes events from multiple clusters as input and that selects the most salient and relevant events . a cluster , in our work , contains all the events happening in a specific date . our algorithm utilizes the temporal information derived from a large collection of extensively temporal analyzed texts . such temporal information is combined with textual contents into an event scoring model in order to rank events based on their salience and query-relevance .

a language-independent method for the extraction of rdf verbalization
with the rise of the semantic web more and more data become available encoded using the semantic web standard rdf . rdf is faced towards machines : designed to be easily processable by machines it is difficult to be understood by casual users . transforming rdf data into human-comprehensible text would facilitate non-experts to assess this information . in this paper we present a languageindependent method for extracting rdf verbalization templates from a parallel corpus of text and data . our method is based on distant-supervised simultaneous multi-relation learning and frequent maximal subgraph pattern mining . we demonstrate the feasibility of our method on a parallel corpus of wikipedia articles and dbpedia data for english and german .

cngl : grading student answers by acts of translation
we invent referential translation machines ( rtms ) , a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain , which can be used for automatically grading student answers . rtms make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics . an mtpp ( machine translation performance predictor ) model derives features measuring the closeness of the test sentences to the training data , the difficulty of translating them , and the presence of acts of translation involved . we view question answering as translation from the question to the answer , from the question to the reference answer , from the answer to the reference answer , or from the question and the answer to the reference answer . each view is modeled by an rtm model , giving us a new perspective on the ternary relationship between the question , the answer , and the reference answer . we show that all rtm models contribute and a prediction model based on all four perspectives performs the best . our prediction model is the 2nd best system on some tasks according to the official results of the student response analysis ( sra 2013 ) challenge .

soft dependency constraints for reordering in hierarchical phrase-based translation
long-distance reordering remains one of the biggest challenges facing machine translation . we derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model . our approach significantly improves chineseenglish machine translation on a large-scale task by 0.84 bleu points on average . moreover , when we switch the tuning function from bleu to the lrscore which promotes reordering , we observe total improvements of 1.21 bleu , 1.30 lrscore and 3.36 ter over the baseline . on average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points , respectively , and is found to be especially effective for long-distance reodering .

re-evaluating machine translation results with paraphrase support
in this paper , we present paraeval , an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations . previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching . paraeval addresses three important issues : support for paraphrase/synonym matching , recall measurement , and correlation with human judgments . we show that paraeval correlates significantly better than bleu with human assessment in measurements for both fluency and adequacy .

efficient dynamic programming search algorithms for phrase-based smt
this paper presents a series of efficient dynamic-programming ( dp ) based algorithms for phrase-based decoding and alignment computation in statistical machine translation ( smt ) . the dp-based decoding algorithms are analyzed in terms of shortest path-finding algorithms , where the similarity to dp-based decoding algorithms in speech recognition is demonstrated . the paper contains the following original contributions : 1 ) the dp-based decoding algorithm in ( tillmann and ney , 2003 ) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2 ) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment . under certain restrictions , both algorithms handle mt-related problems efficiently that are generally np complete ( knight , 1999 ) .

a methodology for extrinsic evaluation of text summarization : does rouge correlate
this paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement , relevance-prediction , which compares subjects judgments on summaries with their own judgments on full text documents . we demonstrate that , because this measure is more reliable than previous gold-standard measures , we are able to make stronger statistical statements about the benefits of summarization . we found positive correlations between rouge scores and two different summary types , where only weak or negative correlations were found using other agreement measures . however , we show that rouge may be sensitive to the choice of summarization style . we discuss the importance of these results and the implications for future summarization evaluations .

reference reversibility with reference domain theory
in this paper we present a reference model based on reference domain theory that can work both in interpretation and generation . we introduce a formalization of key concepts of rdt , the interpretation and generation algorithms and show an example of behavior in the dynamic , asymmetric and multimodal give environment .

a generative model for unsupervised discovery of relations and argument classes from clinical texts
this paper presents a generative model for the automatic discovery of relations between entities in electronic medical records . the model discovers relation instances and their types by determining which context tokens express the relation . additionally , the valid semantic classes for each type of relation are determined . we show that the model produces clusters of relation trigger words which better correspond with manually annotated relations than several existing clustering techniques . the discovered relations reveal some of the implicit semantic structure present in patient records .

error detection using linguistic features
recognition errors hinder the proliferation of speech recognition ( sr ) systems . based on the observation that recognition errors may result in ungrammatical sentences , especially in dictation application where an acceptable level of accuracy of generated documents is indispensable , we propose to incorporate two kinds of linguistic features into error detection : lexical features of words , and syntactic features from a robust lexicalized parser . transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features . the experimental results on a dictation data corpus show that linguistic features alone are not as useful as word confidence scores in detecting errors . however , linguistic features provide complementary information when combined with word confidence scores , which collectively reduce the classification error rate by 12.30 % and improve the f measure by 53.62 % .

shallow semantic parsing of chinese
in this paper we address the question of assigning semantic roles to sentences in chinese . we show that good semantic parsing results for chinese can be achieved with a small 1100-sentence training set . in order to extract features from chinese , we describe porting the collins parser to chinese , resulting in the best performance currently reported on chinese syntactic parsing ; we include our headrules in the appendix . finally , we compare english and chinese semantic-parsing performance . while slight differences in argument labeling make a perfect comparison impossible , our results nonetheless suggest significantly better performance for chinese . we show that much of this difference is due to grammatical differences between english and chinese , such as the prevalence of passive in english , and the strict word order constraints on adjuncts in chinese .

co-dispersion : a windowless approach to lexical association
we introduce an alternative approach to extracting word pair associations from corpora , based purely on surface distances in the text . we contrast it with the prevailing windowbased co-occurrence model and show it to be more statistically robust and to disclose a broader selection of significant associative relationships - owing largely to the property of scale-independence . in the process we provide insights into the limiting characteristics of window-based methods which complement the sometimes conflicting application-oriented literature in this area .

a unified representation for morphological , syntactic , semantic , and
this paper reports on the syn-ra ( syntax-based reference annotation ) project , an on-going project of annotating german newspaper texts with referential relations . the project has developed an inventory of anaphoric and coreference relations for german in the context of a unified , xml-based annotation scheme for combining morphological , syntactic , semantic , and anaphoric information . the paper discusses how this unified annotation scheme relates to other formats currently discussed in the literature , in particular the annotation graph model of bird and liberman ( 2001 ) and the pie-in-thesky scheme for semantic annotation .

learning computational grammars
this paper reports on the learning computational grammars ( lcg ) project , a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use . we were interested in a more systematic survey to understand the relevance of many factors to the success of learning , esp . the availability of annotated data , the kind of dependencies in the data , and the availability of knowledge bases ( grammars ) . we focused on syntax , esp . noun phrase ( np ) syntax .

modelling the internal variability of mwes
the issue of flexibility of multiword expressions ( mwes ) is crucial towards their identification and extraction in running text , as well as their better understanding from a linguistic perspective . if we project a large mwe lexicon onto a corpus , projecting fixed forms suffers from low recall , while an unconstrained flexible search for lemmas yields a loss in precision . in this talk , i will describe a method aimed at maximising precision in the identification of mwes in flexible mode , building on the idea that internal variability can be modelled via so-called variation patterns . i will discuss the advantages and limitations of using variation patterns , compare their performance to that of association measures , and explore their usability in mwe extraction , too . about the speaker malvina nissim is a tenured researcher in computational linguistics at the university of bologna . her research focuses on the computational handling of several lexical semantics and discourse phenomena , such as the choice of referring expressions , semantic relations within compounds and in argument structure , multiword expressions , and , more recently , on the annotation and automatic detection of modality . she is also a co-founder and promoter of the senso comune project , devoted to the creation of a common knowledge base for italian via crowdsourcing . she graduated in linguistics from the university of pisa , and obtained her phd in linguistics from the university of pavia . before joining the university of bologna she was a post-doc at the university of edinburgh and at the institute for cognitive science and technology in rome . 51

self-training for enhancement and domain adaptation of statistical parsers trained on small datasets
creating large amounts of annotated data to train statistical pcfg parsers is expensive , and the performance of such parsers declines when training and test data are taken from different domains . in this paper we use selftraining in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . we report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario . in particular , we achieve 50 % reduction in annotation cost for the in-domain case , yielding an improvement of 66 % over previous work , and a 20-33 % reduction for the domain adaptation case . this is the first time that self-training with small labeled datasets is applied successfully to these tasks . we were also able to formulate a characterization of when selftraining is valuable .

types in chinese text
discriminating sentences that denote modalities and speech acts from the ones that describe or report events is a fundamental task for accurate event processing . however , little attention has been paid on this issue . no chinese corpus is available by now with all different types of sentences annotated with their main functionalities in terms of modality , speech act or event . this paper describes a chinese corpus with all the information annotated . based on the five event types that are usually adopted in previous studies of event classification , namely state , activity , achievement , accomplishment and semelfactive , we further provide finer-grained categories , considering that each of the finer-grained event types has different semantic entailments . to differentiate them is useful for deep semantic processing and will thus benefit nlp applications such as question answering and machine translation , etc . we also provide experiments to show that the different types of sentences are differentiable with a promising performance .

cross-lingual induction of selectional preferences with bilingual vector spaces
we describe a cross-lingual method for the induction of selectional preferences for resourcepoor languages , where no accurate monolingual models are available . the method uses bilingual vector spaces to translate foreign language predicate-argument structures into a resource-rich language like english . the only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language , although the model can profit from ( even noisy ) syntactic knowledge . our experiments show that the cross-lingual predictions correlate well with human ratings , clearly outperforming monolingual baseline models .

a method for automatic pos guessing of chinese unknown words tsinghua science park , zhongguancun
this paper proposes a method for automatic pos ( part-of-speech ) guessing of chinese unknown words . it contains two models . the first model uses a machinelearning method to predict the pos of unknown words based on their internal component features . the credibility of the results of the first model is then measured . for low-credibility words , the second model is used to revise the first models results based on the global context information of those words . the experiments show that the first model achieves 93.40 % precision for all words and 86.60 % for disyllabic words , which is a significant improvement over the best results reported in previous studies , which were 89 % precision for all words and 74 % for disyllabic words . further , the second model improves the results by 0.80 % precision for all words and 1.30 % for disyllabic words .

generating example contexts to illustrate a target word sense jack mostow weisi duan
learning a vocabulary word requires seeing it in multiple informative contexts . we describe a system to generate such contexts for a given word sense . rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus , we compile information about the word sense into the context generation process . to evaluate the sense-appropriateness of the generated contexts compared to wordnet examples , three human judges chose which word sense ( s ) fit each example , blind to its source and intended sense . on average , one judge rated the generated examples as sense-appropriate , compared to two judges for the wordnet examples . although the systems precision was only half of wordnets , its recall was actually higher than wordnets , thanks to covering many senses for which wordnet lacks examples .

a discriminative graph-based parser for the abstract meaning representation
abstract meaning representation ( amr ) is a semantic formalism for which a growing set of annotated examples is available . we introduce the first approach to parse sentences into this representation , providing a strong baseline for future improvement . the method is based on a novel algorithm for finding a maximum spanning , connected subgraph , embedded within a lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints . our approach is described in the general framework of structured prediction , allowing future incorporation of additional features and constraints , and may extend to other formalisms as well . our open-source system , jamr , is available at : http : //github.com/jflanigan/jamr

named entity transliteration with comparable corpora
in this paper we investigate chineseenglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics and therefore share references to named entities but are not translations of each other . we present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs . each of these approaches works quite well , but by combining the approaches one can achieve even better results . we then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs . this propagation method achieves further improvement over the best results from the previous step .

language-independent probabilistic answer ranking for question answering
this paper presents a language-independent probabilistic answer ranking framework for question answering . the framework estimates the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidates for the question . our approach was evaluated by comparing the candidate answer sets generated by chinese and japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework . empirical results from testing on ntcir factoid questions show a 40 % performance improvement in chinese answer selection and a 45 % improvement in japanese answer selection .

extraction of tree adjoining grammars from a treebank for korean
we present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from korean sejong treebank . we report on some practical experiments where we extract tag grammars and tree schemata . above all , full-scale syntactic tags and well-formed morphological analysis in sejong treebank allow us to extract syntactic features . in addition , we modify treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars .

retrieval for information extraction tasks irisa cnrs
this paper describes the information extraction techniques developed in the framework of the participation of irisatexmex to the following bionlp-st13 tasks : bacterial biotope subtasks 1 and 2 , and graph regulation network . the approaches developed are general-purpose ones and do not rely on specialized preprocessing , nor specialized external data , and they are expected to work independently of the domain of the texts processed . they are classically based on machine learning techniques , but we put the emphasis on the use of similarity measures inherited from the information retrieval domain ( okapi-bm25 ( robertson et al , 1998 ) , language modeling ( hiemstra , 1998 ) ) . through the good results obtained for these tasks , we show that these simple settings are competitive provided that the representation and similarity chosen are well suited for the task .

acquisition system for arabic noun morphology
many papers have discussed different aspects of arabic verb morphology . some of them used patterns ; others used patterns and affixes . but very few have discussed arabic noun morphology particularly for nouns that are not derived from verbs . in this paper we describe a learning system that can analyze arabic nouns to produce their morphological information and their paradigms with respect to both gender and number using a rule base that uses suffix analysis as well as pattern analysis . the system utilizes user-feedback to classify the noun and identify the group that it belongs to .

towards the automatic creation of a wordnet from a term-based lexical
the work described here aims to create a wordnet automatically from a semantic network based on terms . so , a clustering procedure is ran over a synonymy network , in order to obtain synsets . then , the term arguments of each relational triple are assigned to the latter , originating a wordnet . experiments towards our goal are reported and their results validated .

capturing salience with a trainable cache model for zero-anaphora kentaro inui yuji matsumoto
this paper explores how to apply the notion of caching introduced by walker ( 1996 ) to the task of zero-anaphora resolution . we propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent . our empirical evaluation with japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it .

the use of metadata , web-derived answer patterns and passage context to improve reading comprehension performance
a reading comprehension ( rc ) system attempts to understand a document and returns an answer sentence when posed with a question . rc resembles the ad hoc question answering ( qa ) task that aims to extract an answer from a collection of documents when posed with a question . however , since rc focuses only on a single document , the system needs to draw upon external knowledge sources to achieve deep analysis of passage sentences for answer sentence extraction . this paper proposes an approach towards rc that attempts to utilize external knowledge to improve performance beyond the baseline set by the bag-of-words ( bow ) approach . our approach emphasizes matching of metadata ( i.e . verbs , named entities and base noun phrases ) in passage context utilization and answer sentence extraction . we have also devised an automatic acquisition process for web-derived answer patterns ( ap ) which utilizes question-answer pairs from trec qa , the google search engine and the web . this approach gave improved rc performances for both the remedia and chunghwa corpora , attaining humsent accuracies of 42 % and 69 % respectively . in particular , performance analysis based on remedia shows that relative performances of 20.7 % is due to metadata matching and a further 10.9 % is due to the application of web-derived answer patterns .

learning strategies for open-domain natural language question
this work presents a model for learning inference procedures for story comprehension through inductive generalization and reinforcement learning , based on classified examples . the learned inference procedures ( or strategies ) are represented as of sequences of transformation rules . the approach is compared to three prior systems , and experimental results are presented demonstrating the efficacy of the model .

the hinoki sensebank a large-scale word sense tagged corpus of japanese
semantic information is important for precise word sense disambiguation system and the kind of semantic analysis used in sophisticated natural language processing such as machine translation , question answering , etc . there are at least two kinds of semantic information : lexical semantics for words and phrases and structural semantics for phrases and sentences . we have built a japanese corpus of over three million words with both lexical and structural semantic information . in this paper , we focus on our method of annotating the lexical semantics , that is building a word sense tagged corpus and its properties .

determining the specificity of terms using compositional and contextual information
this paper introduces new specificity determining methods for terms using compositional and contextual information . specificity of terms is the quantity of domain specific information that is contained in the terms . the methods are modeled as information theory like measures . as the methods dont use domain specific information , they can be applied to other domains without extra processes . experiments showed very promising result with the precision of 82.0 % when the methods were applied to the terms in mesh thesaurus .

closing the gap : learning-based information extraction
in this paper , we present a learning approach to the scenario template task of information extraction , where information filling one template could come from multiple sentences . when tested on the muc4 task , our learning approach achieves accuracy competitive to the best of the muc-4 systems , which were all built with manually engineered rules . our analysis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance . to our knowledge , this is the first research to have demonstrated that a learning approach to the full-scale information extraction task could achieve performance rivaling that of the knowledgeengineering approach .

automatic induction of a ccg grammar for turkish
this paper presents the results of automatically inducing a combinatory categorial grammar ( ccg ) lexicon from a turkish dependency treebank . the fact that turkish is an agglutinating free wordorder language presents a challenge for language theories . we explored possible ways to obtain a compact lexicon , consistent with ccg principles , from a treebank which is an order of magnitude smaller than penn wsj .

a discourse resource for turkish : annotating discourse connectives in the metu corpus
this paper describes first steps towards extending the metu turkish corpus from a sentence-level language resource to a discourse-level resource by annotating its discourse connectives and their arguments . the project is based on the same principles as the penn discourse treebank and is supported by tubitak , the scientific and technological research council of turkey . we first present the goals of the project and the metu turkish corpus . we then describe how we decided what to take as explicit discourse connectives and the range of syntactic classes they come from . with representative examples of each class , we examine explicit connectives , their linear ordering , and types of syntactic units that can serve as their arguments . we then touch upon connectives with respect to free word order in turkish and punctuation , as well as the important issue of how much material is needed to specify an argument . we close with a brief discussion of current plans .

you are what you say : using meeting participants speech to detect their roles and expertise
our goal is to automatically detect the functional roles that meeting participants play , as well as the expertise they bring to meetings . to perform this task , we build decision tree classifiers that use a combination of simple speech features ( speech lengths and spoken keywords ) extracted from the participants speech in meetings . we show that this algorithm results in a role detection accuracy of 83 % on unseen test data , where the random baseline is 33.3 % . we also introduce a simple aggregation mechanism that combines evidence of the participants expertise from multiple meetings . we show that this aggregation mechanism improves the role detection accuracy from 66.7 % ( when aggregating over a single meeting ) to 83 % ( when aggregating over 5 meetings ) .

syntactic kernels for natural language learning
in this paper , we use tree kernels to exploit deep syntactic parsing information for natural language applications . we study the properties of different kernels and we provide algorithms for their computation in linear average time . the experiments with svms on the task of predicate argument classification provide empirical data that validates our methods .

learning to translate with source and target syntax
statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years . these models make use of varying amounts of information from linguistic theory : some use none at all , some use information about the grammar of the target language , some use information about the grammar of the source language . but progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language . we discuss the reasons why this has been a challenge , review existing attempts to meet this challenge , and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy .

deep learning for chinese word segmentation and pos tagging
this study explores the feasibility of performing chinese word segmentation ( cws ) and pos tagging by deep learning . we try to avoid task-specific feature engineering , and use deep layers of neural networks to discover relevant features to the tasks . we leverage large-scale unlabeled data to improve internal representation of chinese characters , and use these improved representations to enhance supervised word segmentation and pos tagging models . our networks achieved close to state-of-theart performance with minimal computational cost . we also describe a perceptron-style algorithm for training the neural networks , as an alternative to maximum-likelihood method , to speed up the training process and make the learning algorithm easier to be implemented .

a supervised learning based chunking in thai using categorial grammar human language technology ,
one of the challenging problems in thai nlp is to manage a problem on a syntactical analysis of a long sentence . this paper applies conditional random field and categorical grammar to develop a chunking method , which can group words into larger unit . based on the experiment , we found the impressive results . we gain around 74.17 % on sentence level chunking . furthermore we got a more correct parsed tree based on our technique . around 50 % of tree can be added . finally , we solved the problem on implicit sentential np which is one of the difficult thai language processing . 58.65 % of sentential np is correctly detected .

a syntax-directed translator with extended domain of locality
a syntax-directed translator first parses the source-language input into a parsetree , and then recursively converts the tree into a string in the target-language . we model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side , which gives our system more expressive power and flexibility . we also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation . the model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models . we devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring . initial experimental results on english-to-chinese translation are presented .

searching the audio notebook : keyword search in recorded conversations
mits audio notebook added great value to the note-taking process by retaining audio recordings , e.g . during lectures or interviews . the key was to provide users ways to quickly and easily access portions of interest in a recording . several non-speech-recognition based techniques were employed . in this paper we present a system to search directly the audio recordings by key phrases . we have identified the user requirements as accurate ranking of phrase matches , domain independence , and reasonable response time . we address these requirements by a hybrid word/phoneme search in lattices , and a supporting indexing scheme . we will introduce the ranking criterion , a unified hybrid posterior-lattice representation , and the indexing algorithm for hybrid lattices . we present results for five different recording sets , including meetings , telephone conversations , and interviews . our results show an average search accuracy of 84 % , which is dramatically better than a direct search in speech recognition transcripts ( less than 40 % search accuracy ) .

amazon mechanical turk for subjectivity word sense disambiguation
amazon mechanical turk ( mturk ) is a marketplace for so-called human intelligence tasks ( hits ) , or tasks that are easy for humans but currently difficult for automated processes . providers upload tasks to mturk which workers then complete . natural language annotation is one such human intelligence task . in this paper , we investigate using mturk to collect annotations for subjectivity word sense disambiguation ( swsd ) , a coarse-grained word sense disambiguation task . we investigate whether we can use mturk to acquire good annotations with respect to gold-standard data , whether we can filter out low-quality workers ( spammers ) , and whether there is a learning effect associated with repeatedly completing the same kind of task . while our results with respect to spammers are inconclusive , we are able to obtain high-quality annotations for the swsd task . these results suggest a greater role for mturk with respect to constructing a large scale swsd system in the future , promising substantial improvement in subjectivity and sentiment analysis .

efficient extraction of grammatical relations and ted briscoe
we present a novel approach for applying the inside-outside algorithm to a packed parse forest produced by a unificationbased parser . the approach allows a node in the forest to be assigned multiple inside and outside probabilities , enabling a set of weighted grs to be computed directly from the forest . the approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted grs , or places extra constraints on which nodes can be packed , leading to less compact forests . our experiments demonstrate substantial increases in parser accuracy and throughput for weighted gr output .

knowledge intensive word alignment with knowa
in this paper we present knowa , an english/italian word aligner , developed at itc-irst , which relies mostly on information contained in bilingual dictionaries . the performances of knowa are compared with those of giza++ , a state of the art statistics-based alignment algorithm . the two algorithms are evaluated on the eurocor and multisemcor tasks , that is on two english/italian publicly available parallel corpora . the results of the evaluation show that , given the nature and the size of the available english-italian parallel corpora , a language-resource-based word aligner such as knowa can outperform a fully statistics-based algorithm such as giza++ .

soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions
in this paper , we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features . rather than directly using treebank categories as in previous studies , we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed , word-aligned parallel corpus , based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side . in our model , each x nonterminal in a scfg rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories . these feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the scfg rules that are applied to derive translations . our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints .

unsupervised training for overlapping ambiguity resolution in chinese word segmentation
this paper proposes an unsupervised training approach to resolving overlapping ambiguities in chinese word segmentation . we present an ensemble of adapted nave bayesian classifiers that can be trained using an unlabelled chinese text corpus . these classifiers differ in that they use context words within windows of different sizes as features . the performance of our approach is evaluated on a manually annotated test set . experimental results show that the proposed approach achieves an accuracy of 94.3 % , rivaling the rule-based and supervised training methods .

using a random forest classifier to recognise translations of biomedical terms across languages
we present a novel method to recognise semantic equivalents of biomedical terms in language pairs . we hypothesise that biomedical term are formed by semantically similar textual units across languages . based on this hypothesis , we employ a random forest ( rf ) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples . we apply our method on two language pairs : one that uses the same character set and another with a different script , english-french and englishchinese , respectively . we show that english-french pairs of terms are highly transliterated in contrast to the englishchinese pairs . nonetheless , our method performs robustly on both cases . we evaluate rf against a state-of-the-art alignment method , giza++ , and we report a statistically significant improvement . finally , we compare rf against support vector machines and analyse our results .

iitb-sentiment-analysts : participation in sentiment analysis
we propose a method for using discourse relations for polarity detection of tweets . we have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and pos-taggers dont work properly . we have showed how conjunctions , connectives , modals and conditionals affect the sentiments in tweets . we have also handled the commonly used abbreviations , slangs and collocations which are usually used in short text messages like tweets . this work focuses on a web based application which produces results in real time . this approach is an extension of the previous work ( mukherjee et al 2012 ) .

unsupervised topic modelling for multi-party spoken discourse
we present a method for unsupervised topic modelling which adapts methods used in document classification to unsegmented multi-party discourse transcripts . we show how bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( galley et al , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges . we also show that this method appears robust in the face of off-topic dialogue and speech recognition errors .

weakly supervised methods
in this paper we consider the problem of labeling the languages of words in mixed-language documents . this problem is approached in a weakly supervised fashion , as a sequence labeling problem with monolingual text samples for training data . among the approaches evaluated , a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied .

adaptive language modeling for word prediction
we present the development and tuning of a topic-adapted language model for word prediction , which improves keystroke savings over a comparable baseline . we outline our plans to develop and integrate style adaptations , building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts .

a probabilistic model for learning multi-prototype word embeddings
distributed word representations have been widely used and proven to be useful in quite a few natural language processing and text mining tasks . most of existing word embedding models aim at generating only one embedding vector for each individual word , which , however , limits their effectiveness because huge amounts of words are polysemous ( such as bank and star ) . to address this problem , it is necessary to build multi embedding vectors to represent different meanings of a word respectively . some recent studies attempted to train multi-prototype word embeddings through clustering context window features of the word . however , due to a large number of parameters to train , these methods yield limited scalability and are inefficient to be trained with big data . in this paper , we introduce a much more efficient method for learning multi embedding vectors for polysemous words . in particular , we first propose to model word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous skip-gram model . under this framework , we design an expectation-maximization algorithm to learn the words multi embedding vectors . with much less parameters to train , our model can achieve comparable or even better results on word-similarity tasks compared with conventional methods .

using second-order vectors in a knowledge-based method for acronym disambiguation
in this paper , we introduce a knowledge-based method to disambiguate biomedical acronyms using second-order co-occurrence vectors . we create these vectors using information about a long-form obtained from the unified medical language system and medline . we evaluate this method on a dataset of 18 acronyms found in biomedical text . our method achieves an overall accuracy of 89 % . the results show that using second-order features provide a distinct representation of the long-form and potentially enhances automated disambiguation .

annotating the propositions in the penn chinese treebank
in this paper , we describe an approach to annotate the propositions in the penn chinese treebank . we describe how diathesis alternation patterns can be used to make coarse sense distinctions for chinese verbs as a necessary step in annotating the predicate-structure of chinese verbs . we then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates . we discuss several complications for this type of annotation and describe our solutions . we then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation . finally , we discuss possible applications for this resource .

dialectal to standard arabic paraphrasing to improve arabic-english statistical machine translation
this paper is about improving the quality of arabic-english statistical machine translation ( smt ) on dialectal arabic text using morphological knowledge . we present a light-weight rule-based approach to producing modern standard arabic ( msa ) paraphrases of dialectal arabic out-of-vocabulary ( oov ) words and low frequency words . our approach extends an existing msa analyzer with a small number of morphological clitics , and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrasebased smt system . this approach improves bleu scores on a blind test set by 0.56 absolute bleu ( or 1.5 % relative ) . a manual error analysis of translated dialectal words shows that our system produces correct translations in 74 % of the time for oovs and 60 % of the time for low frequency words .

investigating multilingual dependency parsing
in this paper , we describe a system for the conll-x shared task of multilingual dependency parsing . it uses a baseline nivres parser ( nivre , 2003 ) that first identifies the parse actions and then labels the dependency arcs . these two steps are implemented as svm classifiers using libsvm . features take into account the static context as well as relations dynamically built during parsing . we experimented two main additions to our implementation of nivres parser : n best search and bidirectional parsing . we trained the parser in both left-right and right-left directions and we combined the results . to construct a single-head , rooted , and cycle-free tree , we applied the chuliu/edmonds optimization algorithm . we ran the same algorithm with the same parameters on all the languages .

creating sentiment dictionaries via triangulation
the paper presents a semi-automatic approach to creating sentiment dictionaries in many languages . we first produced high-level goldstandard sentiment dictionaries for two languages and then translated them automatically into third languages . those words that can be found in both target language word lists are likely to be useful because their word senses are likely to be similar to that of the two source languages . these dictionaries can be further corrected , extended and improved . in this paper , we present results that verify our triangulation hypothesis , by evaluating triangulated lists and comparing them to nontriangulated machine-translated word lists .

extraction phenomena in synchronous tag syntax and semantics
we present a proposal for the structure of noun phrases in synchronous treeadjoining grammar ( stag ) syntax and semantics that permits an elegant and uniform analysis of a variety of phenomena , including quantifier scope and extraction phenomena such as wh-questions with both moved and in-place wh-words , pied-piping , stranding of prepositions , and topicalization . the tight coupling between syntax and semantics enforced by the stag helps to illuminate the critical relationships and filter out analyses that may be appealing for either syntax or semantics alone but do not allow for a meaningful relationship between them .

using robust minimal recursion semantics and an ontology and francis bond
we design and test a sentence comparison method using the framework of robust minimal recursion semantics which allows us to utilise the deep parse information produced by jacy , a japanese hpsg based parser and the lexical information available in our ontology . our method was used for both paraphrase detection and also for answer sentence selection for question answering . in both tasks , results showed an improvement over bag-of-words , as well as providing extra information useful to the applications .

dependency parsing of japanese spoken monologue based on clause boundaries
spoken monologues feature greater sentence length and structural complexity than do spoken dialogues . to achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units . this paper proposes a method for dependency parsing of japanese monologues based on sentence segmentation . in this method , the dependency parsing is executed in two stages : at the clause level and the sentence level . first , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause . next , the dependencies over clause boundaries are identified stochastically , and the dependency structure of the entire sentence is thus completed . an experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of japanese monologue sentences .

phrasal : a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features
we present a new java-based open source toolkit for phrase-based machine translation . the key innovation provided by the toolkit is to use apis for integrating new features ( /knowledge sources ) into the decoding model and for extracting feature statistics from aligned bitexts . the package includes a number of useful features written to these apis including features for hierarchical reordering , discriminatively trained linear distortion , and syntax based language models . other useful utilities packaged with the toolkit include : a conditional phrase extraction system that builds a phrase table just for a specific dataset ; and an implementation of mert that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics ( e.g. , terp , bleu , meteor ) .

construction of an objective hierarchy of abstract concepts
the method of organization of word meanings is a crucial issue with lexical databases . our purpose in this research is to extract word hierarchies from corpora automatically . our initial task to this end is to determine adjective hyperonyms . in order to find adjective hyperonyms , we utilize abstract nouns . we constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map , which is a neural network model ( kohonen 1995 ) . in this paper we describe how to hierarchically organize abstract nouns ( adjective hyperonyms ) in a semantic map mainly using csm . we compare three hierarchical organizations of abstract nouns , according to csm , frequency ( tf.csm ) and an alternative similarity measure based on coefficient overlap , to estimate hyperonym relations between words .

improved arabic base phrase chunking with a new enriched pos tag set
base phrase chunking ( bpc ) or shallow syntactic parsing is proving to be a task of interest to many natural language processing applications . in this paper , a bpc system is introduced that improves over state of the art performance in bpc using a new part of speech tag ( pos ) set . the new pos tag set , erts , reflects some of the morphological features specific to modern standard arabic . erts explicitly encodes definiteness , number and gender information increasing the number of tags from 25 in the standard ldc reduced tag set to 75 tags . for the bpc task , we introduce a more language specific set of definitions for the base phrase annotations . we employ a support vector machine approach for both the pos tagging and the bpc processes . the pos tagging performance using this enriched tag set , erts , is at 96.13 % accuracy . in the bpc experiments , we vary the feature set alng two factors : the pos tag set and a set of explicitly encoded morphological features . using the erts pos tagset , bpc achieves the highest overall f=1 of 96.33 % on 10 different chunk types outperforming the use of the standard pos tag set even when explicit morphological features are present .

factored markov translation with robust modeling information sciences institue
phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases . in this paper we present a new method to model correlations between phrases as a markov model and meanwhile employ a robust smoothing strategy to provide better generalization . this method defines a recursive estimation process and backs off in parallel paths to infer richer structures . our evaluation shows an 1.13.2 % bleu improvement over competitive baselines for chinese-english and arabic-english translation .

parsing linear context-free rewriting systems
we describe four different parsing algorithms for linear context-free rewriting systems . the algorithms are described as deduction systems , and possible optimizations are discussed .

making tree kernels practical for natural language learning
in recent years tree kernels have been proposed for the automatic learning of natural language applications . unfortunately , they show ( a ) an inherent super linear complexity and ( b ) a lower accuracy than traditional attribute/value methods . in this paper , we show that tree kernels are very helpful in the processing of natural language as ( a ) we provide a simple algorithm to compute tree kernels in linear average running time and ( b ) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods . experiments with support vector machines on the predicate argument classification task provide empirical support to our thesis .

a log-linear model for unsupervised text normalization
we present a unified unsupervised statistical model for text normalization . the relationship between standard and non-standard tokens is characterized by a log-linear model , permitting arbitrary features . the weights of these features are trained in a maximumlikelihood framework , employing a novel sequential monte carlo training algorithm to overcome the large label space , which would be impractical for traditional dynamic programming solutions . this model is implemented in a normalization system called unlol , which achieves the best known results on two normalization datasets , outperforming more complex systems . we use the output of unlol to automatically normalize a large corpus of social media text , revealing a set of coherent orthographic styles that underlie online language variation .

tagging with hidden markov models using ambiguous tags
part of speech taggers based on hidden markov models rely on a series of hypotheses which make certain errors inevitable . the idea developed in this paper consists in allowing a limited , controlled ambiguity in the output of the tagger in order to avoid a number of errors . the ambiguity takes the form of ambiguous tags which denote subsets of the tagset . these tags are used when the tagger hesitates between the different components of the ambiguous tags . they are introduced in an existing lexicon and 3-gram database . their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents , using impurity functions . the tagging process itself , based on the viterbi algorithm , is unchanged . experiments conducted on the brown corpus show a recall of 0.982 , for an ambiguity rate of 1.233 which is to be compared with a baseline recall of 0.978 for an ambiguity rate of 1.414 using the same ambiguous tags and with a recall of 0.955 corresponding to the one best solution of standard tagging ( without ambiguous tags ) .

towards a semantic classication of spanish verbs based on eva esteve ferrer
we present experiments aiming at an automatic classification of spanish verbs into lexical semantic classes . we apply well-known techniques that have been developed for the english language to spanish , proving that empirical methods can be re-used through languages without substantial changes in the methodology . our results on subcategorisation acquisition compare favourably to the state of the art for english . for the verb classification task , we use a hierarchical clustering algorithm , and we compare the output clusters to a manually constructed classification .

active deep networks for semi-supervised
this paper presents a novel semisupervised learning algorithm called active deep networks ( adn ) , to address the semi-supervised sentiment classification problem with active learning . first , we propose the semi-supervised learning method of adn . adn is constructed by restricted boltzmann machines ( rbm ) with unsupervised learning using labeled data and abundant of unlabeled data . then the constructed structure is finetuned by gradient-descent based supervised learning with an exponential loss function . second , we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data . then adn architecture is trained by the selected labeled data and all unlabeled data . experiments on five sentiment classification datasets show that adn outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification .

is it worth submitting this run assess your rte system with a good sparring partner
we address two issues related to the development of systems for recognizing textual entailment . the first is the impossibility to capitalize on lessons learned over the different datasets available , due to the changing nature of traditional rte evaluation settings . the second is the lack of simple ways to assess the results achieved by our system on a given training corpus , and figure out its real potential on unseen test data . our contribution is the extension of an open-source rte package with an automatic way to explore the large search space of possible configurations , in order to select the most promising one over a given dataset . from the developers point of view , the efficiency and ease of use of the system , together with the good results achieved on all previous rte datasets , represent a useful support , providing an immediate term of comparison to position the results of their approach .

using higher-level linguistic knowledge for speech recognition error
speech interface is often required in many application environments such as telephonebased information retrieval , car navigation systems , and user-friendly interfaces , but the low speech recognition rate makes it difficult to extend its application to new fields . several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results , but previous approaches were mainly lexical-oriented ones in post error correction . we suggest an improved syllable-based model and a new semantic-oriented approach to correct both semantic and lexical errors , which is also more accurate for especially domain-specific speech error correction . through extensive experiments using a speech-driven in-vehicle telematics information retrieval , we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches .

phrase chunking using entropy guided transformation learning ccero nogueira dos santos centro tecnologico do exercito
entropy guided transformation learning ( etl ) is a new machine learning strategy that combines the advantages of decision trees ( dt ) and transformation based learning ( tbl ) . in this work , we apply the etl framework to four phrase chunking tasks : portuguese noun phrase chunking , english base noun phrase chunking , english text chunking and hindi text chunking . in all four tasks , etl shows better results than decision trees and also than tbl with hand-crafted templates . etl provides a new training strategy that accelerates transformation learning . for the english text chunking task this corresponds to a factor of five speedup . for portuguese noun phrase chunking , etl shows the best reported results for the task . for the other three linguistic tasks , etl shows state-of-theart competitive results and maintains the advantages of using a rule based system .

features for detecting hedge cues
we present a sequential labeling approach to hedge cue detection submitted to the biological portion of task 1 for the conll2010 shared task . our main approach is as follows . we make use of partial syntactic information together with features obtained from the unlabeled corpus , and convert the task into one of sequential biotagging . if a cue is found , a sentence is classified as uncertain and certain otherwise . to examine a large number of feature combinations , we employ a genetic algorithm . while some features obtained by this method are difficult to interpret , they were shown to improve the performance of the final system .

an examination of regret in bullying tweets
social media users who post bullying related tweets may later experience regret , potentially causing them to delete their posts . in this paper , we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted . we then conduct exploratory analysis in order to isolate factors associated with deleted posts . finally , we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret .

the difficulties of taxonomic name extraction and a solution guido sautter klemens bhm
in modern biology , digitization of biosystematics publications is an important task . extraction of taxonomic names from such documents is one of its major issues . this is because these names identify the various genera and species . this article reports on our experiences with learning techniques for this particular task . we say why established named-entity recognition techniques are somewhat difficult to use in our context . one reason is that we have only very little training data available . our experiments show that a combining approach that relies on regular expressions , heuristics , and word-level language recognition achieves very high precision and recall and allows to cope with those difficulties .

generating elliptic coordination
in this paper , we focus on the task of generating elliptic sentences . we extract from the data provided by the surface realisation ( sr ) task ( belz et al , 2011 ) 2398 input whose corresponding output sentence contain an ellipsis . we show that 9 % of the data contains an ellipsis and that both coverage and bleu score markedly decrease for elliptic input ( from 82.3 % coverage for non-elliptic sentences to 65.3 % for elliptic sentences and from 0.60 bleu score to 0.47 ) . we argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the sr data . finally , we evaluate an existing surface realiser on the resulting dataset . we show that , after rewriting , the generator achieves a coverage of 76 % and a bleu score of 0.74 on the elliptical data .

in-car multi-domain spoken dialogs : a wizard of oz study speech dialogue systems
mobile internet access via smartphones puts demands on in-car infotainment systems , as more and more drivers like to access the internet while driving . spoken dialog systems support the user by less distracting interaction than visual/hapticbased dialog systems . to develop an intuitive and usable spoken dialog system , an extensive analysis of the interaction concept is necessary . we conducted a wizard of oz study to investigate how users will carry out tasks which involve multiple applications in a speech-only , user-initiative infotainment system while driving . results show that users are not aware of different applications and use anaphoric expressions in task switches . speaking styles vary and depend on type of task and dialog state . users interact efficiently and provide multiple semantic concepts in one utterance . this sets high demands for future spoken dialog systems .

hindi and marathi to english cross language information
in this paper , we present our hindi - > english and marathi - > english clir systems developed as part of our participation in the clef 2007 ad-hoc bilingual task . we take a query translation based approach using bi-lingual dictionaries . query words not found in the dictionary are transliterated using a simple lookup table based transliteration approach . the resultant transliteration is then compared with the index items of the corpus to return the `k ' closest english index words of the given hindi/marathi word . the resulting multiple translation/transliteration choices for each query word are disambiguated using an iterative page-rank style algorithm , proposed in the literature , which makes use of term-term co-occurrence statistics to produce the final translated query . using the above approach , for hindi , we achieve a mean average precision ( map ) of 0.2366 in title which is 61.36 % of monolingual performance and a map of 0.2952 in title and description which is 67.06 % of monolingual performance . for marathi , we achieve a map of 0.2163 in title which is 56.09 % of monolingual performance .

query expansion for khmer information retrieval
this paper presents the proposed query expansion ( qe ) techniques based on khmer specific characteristics to improve the retrieval performance of khmer information retrieval ( ir ) system . four types of khmer specific characteristics : spelling variants , synonyms , derivative words and reduplicative words have been investigated in this research . in order to evaluate the effectiveness and the efficiency of the proposed qe techniques , a prototype of khmer ir system has been implemented . the system is built on top of the popular open source information retrieval software library lucene 1 . the khmer word segmentation tool ( chea et al. , 2007 ) is also implemented into the system to improve the accuracy of indexing as well as searching . furthermore , the google web search engine is also used in the evaluation process . the results show the proposed qe techniques improve the retrieval performance both of the proposed system and the google web search engine . with the reduplicative word qe technique , an improvement of 17.93 % of recall can be achieved to the proposed system .

lolo : a system based on terminology for multilingual extraction
an unsupervised learning method , based on corpus linguistics and special language terminology , is described that can extract time-varying information from text streams . the method is shown to be language-independent in that its use leads to sets of regular-expressions that can be used to extract the information in typologically distinct languages like english and arabic . the method uses the information related to the distribution of ngrams , for automatically extracting meaning bearing patterns of usage in a training corpus . the analysis of an english news wire corpus ( 1,720,142 tokens ) and arabic news wire corpus ( 1,720,154 tokens ) show encouraging results .

grounded language modeling for automatic speech recognition of sports video
grounded language models represent the relationship between words and the non-linguistic context in which they are said . this paper describes how they are learned from large corpora of unlabeled video , and are applied to the task of automatic speech recognition of sports video . results show that grounded language models improve perplexity and word error rate over text based language models , and further , support video information retrieval better than human generated speech transcriptions .

discriminative joint modeling of lexical variation and acoustic confusion for automated narrative retelling assessment
automatically assessing the fidelity of a retelling to the original narrative a task of growing clinical importance is challenging , given extensive paraphrasing during retelling along with cascading automatic speech recognition ( asr ) errors . we present a word tagging approach using conditional random fields ( crfs ) that allows a diversity of features to be considered during inference , including some capturing acoustic confusions encoded in word confusion networks . we evaluate the approach under several scenarios , including both supervised and unsupervised training , the latter achieved by training on the output of a baseline automatic word-alignment model . we also adapt the asr models to the domain , and evaluate the impact of error rate on performance . we find strong robustness to asr errors , even using just the 1-best system output . a hybrid approach making use of both automatic alignment and crfs trained tagging models achieves the best performance , yielding strong improvements over using either approach alone .

multi-word expressions in textual inference : much ado about nothing
multi-word expressions ( mwe ) have seen much attention from the nlp community . in this paper , we investigate their impact on the recognition of textual entailment ( rte ) . using the manual microsoft research annotations , we first manually count and classify mwes in rte data . we find few , most of which are arguably unlikely to cause processing problems . we then consider the impact of mwes on a current rte system . we are unable to confirm that entailment recognition suffers from wrongly aligned mwes . in addition , mwe alignment is difficult to improve , since mwes are poorly represented in state-of-the-art paraphrase resources , the only available sources for multi-word similarities . we conclude that rte should concentrate on other phenomena impacting entailment , and that paraphrase knowledge is best understood as capturing general lexico-syntactic variation .

active learning and the total cost of annotation
active learning ( al ) promises to reduce the cost of annotating labeled datasets for trainable human language technologies . contrary to expectations , when creating labeled training material for hpsg parse selection and later reusing it with other models , gains from al may be negligible or even negative . this has serious implications for using al , showing that additional cost-saving strategies may need to be adopted . we explore one such strategy : using a model during annotation to automate some of the decisions . our best results show an 80 % reduction in annotation cost compared with labeling randomly selected data with a single model .

blueprint for a high performance nlp infrastructure
natural language processing ( nlp ) system developers face a number of new challenges . interest is increasing for real-world systems that use nlp tools and techniques . the quantity of text now available for training and processing is increasing dramatically . also , the range of languages and tasks being researched continues to grow rapidly . thus it is an ideal time to consider the development of new experimental frameworks . we describe the requirements , initial design and exploratory implementation of a high performance nlp infrastructure .

optimal head-driven parsing complexity for linear context-free rewriting systems
we study the problem of finding the best headdriven parsing strategy for linear contextfree rewriting system productions . a headdriven strategy must begin with a specified righthand-side nonterminal ( the head ) and add the remaining nonterminals one at a time in any order . we show that it is np-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing .

constituent reordering and syntax models for english-tojapanese statistical machine translation
we present a constituent parsing-based reordering technique that improves the performance of the state-of-the-art english-to-japanese phrase translation system that includes distortion models by 4.76 bleu points . the phrase translation model with reordering applied at the pre-processing stage outperforms a syntax-based translation system that incorporates a phrase translation model , a hierarchical phrase-based translation model and a tree-to-string grammar . we also show that combining constituent reordering and the syntax model improves the translation quality by additional 0.84 bleu points .

edinburghs syntax-based machine translation systems
we present the syntax-based string-totree statistical machine translation systems built for the wmt 2013 shared translation task . systems were developed for four language pairs . we report on adapting parameters , targeted reduction of the tuning set , and post-evaluation experiments on rule binarization and preventing dropping of verbs .

unsupervised solution post identification from discussion forums
discussion forums have evolved into a dependable source of knowledge to solve common problems . however , only a minority of the posts in discussion forums are solution posts . identifying solution posts from discussion forums , hence , is an important research problem . in this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions . we use translation models and language models to exploit lexical correlations and solution post character respectively . our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums . our clustering-based iterative solution identification approach based on the em-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin . we also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .

modelling semantic role plausibility in human sentence processing
we present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it . we also evaluate our model on the related role-labelling task , and compare it with a standard role labeller . for both tasks , our model benefits from classbased smoothing , which allows it to make correct argument-specific predictions despite a severe sparse data problem . the standard labeller suffers from sparse data and a strong reliance on syntactic cues , especially in the prediction task .

the same-head heuristic for coreference
we investigate coreference relationships between nps with the same head noun . it is relatively common in unsupervised work to assume that such pairs are coreferent but this is not always true , especially if realistic mention detection is used . we describe the distribution of noncoreferent same-head pairs in news text , and present an unsupervised generative model which learns not to link some samehead nps using syntactic features , improving precision .

automatic annotation of parameters from nanodevice development
in utilizing nanodevice development research papers to assist in experimental planning and design , it is useful to identify and annotate characteristic categories of information contained in those papers such as source material , evaluation parameter , etc . in order to support this annotation process , we have been working to construct a nanodevice development corpus and a complementary automatic annotation scheme . due to the variations of terms , however , recall of the automatic annotation in some information categories was not adequate . in this paper , we propose to use a basic physical quantities list to extract parameter information . we confirmed the efficiency of this method to improve the annotation of parameters . recall for parameters increases between 4 % and 7 % depending on the type of parameter and analysis metric .

the organization of the lexicon the polysemy of grow and disambiguation yukiko sasaki alam
this paper demonstrates that the polysemy of the verb grow is a result of natural extension of individual meanings connoted by its basic literal meaning and that the polysemy of grow , as such , can be disambiguated by applying simple rules of elimination to the argument structures , which are the contexts that make particular senses viable .

parallel implementations of word alignment tool language technology institution
training word alignment models on large corpora is a very time-consuming processes . this paper describes two parallel implementations of giza++ that accelerate this word alignment process . one of the implementations runs on computer clusters , the other runs on multi-processor system using multi-threading technology . results show a near-linear speedup according to the number of cpus used , and alignment quality is preserved .

combining formal and distributional models of temporal and intensional semantics
we outline a vision for computational semantics in which formal compositional semantics is combined with a powerful , structured lexical semantics derived from distributional statistics . we consider how existing work ( lewis and steedman , 2013 ) could be extended with a much richer lexical semantics using recent techniques for modelling processes ( scaria et al. , 2013 ) for example , learning that visiting events start with arriving and end with leaving . we show how to closely integrate this information with theories of formal semantics , allowing complex compositional inferences such as is visitinghas arrived in but will leave , which requires interpreting both the function and content words . this will allow machine reading systems to understand not just what has happened , but when .

towards empirical evaluation of affective tactical nlg
one major aim of research in affective natural language generation is to be able to use language intelligently to induce effects on the emotions of the reader/ hearer . although varying the content of generated language ( strategic choices ) might be expected to change the effect on emotions , it is not obvious that varying the form of the language ( tactical choices ) can do this . indeed , previous experiments have been unable to show emotional effects of tactical variations . building on what has been discovered in previous experiments , we present a new experiment which does demonstrate such effects . this represents an important step towards the empirical evaluation of affective nlg systems .

using character overlap to improve language transformation
language transformation can be defined as translating between diachronically distinct language variants . we investigate the transformation of middle dutch into modern dutch by means of machine translation . we demonstrate that by using character overlap the performance of the machine translation process can be improved for this task .

natural logic for textual inference
this paper presents the first use of a computational model of natural logica system of logical inference which operates over natural languagefor textual inference . most current approaches to the pascal rte textual inference task achieve robustness by sacrificing semantic precision ; while broadly effective , they are easily confounded by ubiquitous inferences involving monotonicity . at the other extreme , systems which rely on first-order logic and theorem proving are precise , but excessively brittle . this work aims at a middle way . our system finds a low-cost edit sequence which transforms the premise into the hypothesis ; learns to classify entailment relations across atomic edits ; and composes atomic entailments into a top-level entailment judgment . we provide the first reported results for any system on the fracas test suite . we also evaluate on rte3 data , and show that hybridizing an existing rte system with our natural logic system yields significant performance gains .

englishhindi transliteration using context-informed pb-smt : rejwanul haque , sandipan dandapat , ankit kumar srivastava , sudip kumar naskar and andy way
this paper presents englishhindi transliteration in the news 2009 machine transliteration shared task adding source context modeling into state-of-the-art log-linear phrase-based statistical machine translation ( pb-smt ) . source context features enable us to exploit source similarity in addition to target similarity , as modelled by the language model . we use a memory-based classification framework that enables efficient estimation of these features while avoiding data sparseness problems.we carried out experiments both at character and transliteration unit ( tu ) level . position-dependent source context features produce significant improvements in terms of all evaluation metrics .

the benefits of a model of annotation
this paper presents a case study of a difficult and important categorical annotation task ( word sense ) to demonstrate a probabilistic annotation model applied to crowdsourced data . it is argued that standard ( chance-adjusted ) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels . compared to conventional agreement measures , application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost .

propositions , questions , and adjectives : a rich type theoretic approach
we consider how to develop types corresponding to propositions and questions . starting with the conception of propositions as types , we consider two empirical challenges for this doctrine . the first relates to the putative need for a single type encompassing questions and propositions in order to deal with boolean operations . the second relates to adjectival modification of question and propositional entities . we partly defuse the boolean challenge by showing that the data actually argue against a single type covering questions and propositions . we show that by analyzing both propositions and questions as records within type theory with records ( ttr ) , we can define boolean operations over these distinct semantic types . we account for the adjectival challenge by embedding the record types defined to deal with boolean operations within a theory of semantic frames formulated within ttr .

domain adaptation with artificial data for semantic parsing of speech lonneke van der plas
we adapt a semantic role parser to the domain of goal-directed speech by creating an artificial treebank from an existing text treebank . we use a three-component model that includes distributional models from both target and source domains . we show that we improve the parsers performance on utterances collected from human-machine dialogues by training on the artificially created data without loss of performance on the text treebank .

probabilistic models for disambiguation of an hpsg-based chart generator
we describe probabilistic models for a chart generator based on hpsg . within the research field of parsing with lexicalized grammars such as hpsg , recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models . the focus of this paper is to show that two essential techniques model estimation on packed parse forests and beam search during parsing are successfully exported to the task of natural language generation . additionally , we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data .

the exploration of deterministic and efficient dependency parsing
in this paper , we propose a three-step multilingual dependency parser , which generalizes an efficient parsing algorithm at first phase , a root parser and postprocessor at the second and third stages . the main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing . the experimental results show that our method outperforms maltparser in 13 languages . we expect that such an efficient model is applicable for most languages .

a unified graph model for sentence-based opinion retrieval
there is a growing research interest in opinion retrieval as on-line users opinions are becoming more and more popular in business , social networks , etc . practically speaking , the goal of opinion retrieval is to retrieve documents , which entail opinions or comments , relevant to a target subject specified by the users query . a fundamental challenge in opinion retrieval is information representation . existing research focuses on document-based approaches and documents are represented by bag-of-word . however , due to loss of contextual information , this representation fails to capture the associative information between an opinion and its corresponding target . it can not distinguish different degrees of a sentiment word when associated with different targets . this in turn seriously affects opinion retrieval performance . in this paper , we propose a sentence-based approach based on a new information representation , namely topic-sentiment word pair , to capture intra-sentence contextual information between an opinion and its target . additionally , we consider inter-sentence information to capture the relationships among the opinions on the same topic . finally , the two types of information are combined in a unified graph-based model , which can effectively rank the documents .

clustering comparable corpora for bilingual lexicon extraction
we study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora . we introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus , and finally preserves most of the vocabulary of the original corpus . our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches .

swat : cross-lingual lexical substitution using local context matching ,
we present two systems that select the most appropriate spanish substitutes for a marked word in an english test sentence . these systems were official entries to the semeval-2010 cross-lingual lexical substitution task . the first system , swat-e , finds spanish substitutions by first finding english substitutions in the english sentence and then translating these substitutions into spanish using an english-spanish dictionary . the second system , swat-s , translates each english sentence into spanish and then finds the spanish substitutions in the spanish sentence . both systems exceeded the baseline and all other participating systems by a wide margin using one of the two official scoring metrics .

a bayesian method for robust estimation of distributional similarities
existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words context profiles obtained from a limited amount of data . this paper proposes a bayesian method for robust distributional word similarities . the method uses a distribution of context profiles obtained by bayesian estimation and takes the expectation of a base similarity measure under that distribution . when the context profiles are multinomial distributions , the priors are dirichlet , and the base measure is the bhattacharyya coefficient , we can derive an analytical form that allows efficient calculation . for the task of word similarity estimation using a large amount of web data in japanese , we show that the proposed measure gives better accuracies than other well-known similarity measures .

clustering and matching headlines for automatic paraphrase acquisition
for developing a data-driven text rewriting algorithm for paraphrasing , it is essential to have a monolingual corpus of aligned paraphrased sentences . news article headlines are a rich source of paraphrases ; they tend to describe the same event in various different ways , and can easily be obtained from the web . we compare two methods of aligning headlines to construct such an aligned corpus of paraphrases , one based on clustering , and the other on pairwise similarity-based matching . we show that the latter performs best on the task of aligning paraphrastic headlines .

query-based text normalization selection models for enhanced retrieval
text normalization transforms words into a base form so that terms from common equivalent classes match . traditionally , information retrieval systems employ stemming techniques to remove derivational affixes . depluralization , the transformation of plurals into singular forms , is also used as a low-level text normalization technique to preserve more precise lexical semantics of text . experiment results suggest that the choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy . this paper proposes a hybrid approach , constructing a query-based selection model to select the appropriate text normalization technique ( stemming , depluralization , or not doing any text normalization ) . the selection model utilized ambiguity properties extracted from queries to train a composite of support vector regression ( svr ) models to predict a text normalization technique that yields the highest mean average precision ( map ) . based on our study , such a selection model holds promise in improving retrieval accuracy .

hierarchical directed acyclic graph kernel : methods for structured natural language data
this paper proposes the hierarchical directed acyclic graph ( hdag ) kernel for structured natural language data . the hdag kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the hdags . we applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function . the results of the experiments demonstrate that the hdag kernel is superior to other kernel functions and baseline methods .

proactive learning for building machine translation systems for minority
building machine translation ( mt ) for many minority languages in the world is a serious challenge . for many minor languages there is little machine readable text , few knowledgeable linguists , and little money available for mt development . for these reasons , it becomes very important for an mt system to make best use of its resources , both labeled and unlabeled , in building a quality system . in this paper we argue that traditional active learning setup may not be the right fit for seeking annotations required for building a syntax based mt system for minority languages . we posit that a relatively new variant of active learning , proactive learning , is more suitable for this task .

crf-based hybrid model for word segmentation , ner and event
this paper presents systems submitted to the close track of fourth sighan bakeoff . we built up three systems based on conditional random field for chinese word segmentation , named entity recognition and part-of-speech tagging respectively . our systems employed basic features as well as a large number of linguistic features . for segmentation task , we adjusted the bio tags according to confidence of each character . our final system achieve a f-score of 94.18 at ctb , 92.86 at ncc , 94.59 at sxu on segmentation , 85.26 at msra on named entity recognition , and 90.65 at pku on part-of-speech tagging .

minimalist parsing of subjects displaced from embedded clauses in free word order languages
in sayeed and szpakowicz ( 2004 ) , we proposed a parser inspired by some aspects of the minimalist program . this incremental parser was designed specifically to handle discontinuous constituency phenomena for nps in latin . we take a look at the application of this parser to a specific kind of apparent island violation in latin involving the extraction of constituents , including subjects , from tensed embedded clauses . we make use of ideas about the left periphery from rizzi ( 1997 ) to modify our parser in order to handle apparently violated subject islands and similar phenomena .

extractive summarization using continuous vector space models computer science & engineering
automatic summarization can help users extract the most important pieces of information from the vast amount of text digitized into electronic form everyday . central to automatic summarization is the notion of similarity between sentences in text . in this paper we propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity . we evaluate different compositions for sentence representation on a standard dataset using the rouge evaluation measures . our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of continuous word vector representations for automatic summarization .

the sammie system : multimodal in-car dialogue
the sammie1 system is an in-car multimodal dialogue system for an mp3 application . it is used as a testing environment for our research in natural , intuitive mixed-initiative interaction , with particular emphasis on multimodal output planning and realization aimed to produce output adapted to the context , including the drivers attention state w.r.t . the primary driving task .

a latent variable model for generative dependency parsing
we propose a generative dependency parsing model which uses binary latent variables to induce conditioning features . to define this model we use a recently proposed class of bayesian networks for structured prediction , incremental sigmoid belief networks . we demonstrate that the proposed model achieves state-of-the-art results on three different languages . we also demonstrate that the features induced by the isbns latent variables are crucial to this success , and show that the proposed model is particularly good on long dependencies .

a hedgehop over a max-margin framework using hedge cues
in this paper , we describe the experimental settings we adopted in the context of the 2010 conll shared task for detecting sentences containing uncertainty . the classification results reported on are obtained using discriminative learning with features essentially incorporating lexical information . hyper-parameters are tuned for each domain : using bioscope training data for the biomedical domain and wikipedia training data for the wikipedia test set . by allowing an efficient handling of combinations of large-scale input features , the discriminative approach we adopted showed highly competitive empirical results for hedge detection on the wikipedia dataset : our system is ranked as the first with an f-score of 60.17 % .

gpkex : genetically programmed keyphrase extraction from croatian texts
we describe gpkex , a keyphrase extraction method based on genetic programming . we represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text . we apply and evaluate gpkex on croatian newspaper articles . we show that gpkex can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for croatian .

evaluation of a system for noun concepts acquisition from utterances about images ( sinca ) using daily conversation data
for a robot working in an open environment , a task-oriented language capability will not be sufficient . in order to adapt to the environment , such a robot will have to learn language dynamically . we developed a system for noun concepts acquisition from utterances about images , sinca in short . it is a language acquisition system without knowledge of grammar and vocabulary , which learns noun concepts from user utterances . we recorded a video of a childs daily life to collect dialogue data that was spoken to and around him . the child is a member of a family consisting of the parents and his sister . we evaluated the performance of sinca using the collected data . in this paper , we describe the algorithms of sinca and an evaluation experiment . we work on japanese language acquisition , however our method can easily be adapted to other languages .

a probabilistic morphological analyzer for syriac peter mcclanahan , george busby , robbie haertel , kristian heal ,
we define a probabilistic morphological analyzer using a data-driven approach for syriac in order to facilitate the creation of an annotated corpus . syriac is an under-resourced semitic language for which there are no available language tools such as morphological analyzers . we introduce novel probabilistic models for segmentation , dictionary linkage , and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data . we explore the performance of models with varying amounts of training data and find that with about 34,500 labeled tokens , we can outperform a reasonable baseline trained on over 99,000 tokens and achieve an accuracy of just over 80 % . when trained on all available training data , our joint model achieves 86.47 % accuracy , a 29.7 % reduction in error rate over the baseline .

automata for transliteration and machine translation
automata theory , transliteration , and machine translation ( mt ) have an interesting and intertwined history . finite-state string automata theory became a powerful tool for speech and language after

which asr should i choose for my dialogue system
we present an analysis of several publicly available automatic speech recognizers ( asrs ) in terms of their suitability for use in different types of dialogue systems . we focus in particular on cloud based asrs that recently have become available to the community . we include features of asr systems and desiderata and requirements for different dialogue systems , taking into account the dialogue genre , type of user , and other features . we then present speech recognition results for six different dialogue systems . the most interesting result is that different asr systems perform best on the data sets . we also show that there is an improvement over a previous generation of recognizers on some of these data sets . we also investigate language understanding ( nlu ) on the asr output , and explore the relationship between asr and nlu performance .

linguistically motivated language resources for sentiment analysis
computational approaches to sentiment analysis focus on the identification , extraction , summarization and visualization of emotion and opinion expressed in texts . these tasks require large-scale language resources ( lrs ) developed either manually or semi-automatically . building them from scratch , however , is a laborious and costly task , and re-using and repurposing already existing ones is a solution to this bottleneck . we hereby present work aimed at the extension and enrichment of existing general-purpose lrs , namely a set of computational lexica , and their integration in a new emotion lexicon that would be applicable for a number of natural language processing applications beyond mere syntactic parsing .

automatic compilation of travel information from automatically identified travel blogs
in this paper , we propose a method for compiling travel information automatically . for the compilation , we focus on travel blogs , which are defined as travel journals written by bloggers in diary form . we consider that travel blogs are a useful information source for obtaining travel information , because many bloggers ' travel experiences are written in this form . therefore , we identified travel blogs in a blog database and extracted travel information from them . we have confirmed the effectiveness of our method by experiment . for the identification of travel blogs , we obtained scores of 38.1 % for recall and 86.7 % for precision . in the extraction of travel information from travel blogs , we obtained 74.0 % for precision at the top 100 extracted local products , thereby confirming that travel blogs are a useful source of travel information .

exploiting subjective annotations human media interaction rieks op den akker human media interaction
many interesting phenomena in conversation can only be annotated as a subjective task , requiring interpretative judgements from annotators . this leads to data which is annotated with lower levels of agreement not only due to errors in the annotation , but also due to the differences in how annotators interpret conversations . this paper constitutes an attempt to find out how subjective annotations with a low level of agreement can profitably be used for machine learning purposes . we analyse the ( dis ) agreements between annotators for two different cases in a multimodal annotated corpus and explicitly relate the results to the way machinelearning algorithms perform on the annotated data . finally we present two new concepts , namely subjective entity classifiers resp . consensus objective classifiers , and give recommendations for using subjective data in machine-learning applications .

hidden markov tree model in dependency-based machine translation
we would like to draw attention to hidden markov tree models ( hmtm ) , which are to our knowledge still unexploited in the field of computational linguistics , in spite of highly successful hidden markov ( chain ) models . in dependency trees , the independence assumptions made by hmtm correspond to the intuition of linguistic dependency . therefore we suggest to use hmtm and tree-modified viterbi algorithm for tasks interpretable as labeling nodes of dependency trees . in particular , we show that the transfer phase in a machine translation system based on tectogrammatical dependency trees can be seen as a task suitable for hmtm . when using the hmtm approach for the english-czech translation , we reach a moderate improvement over the baseline .

using syntax to improve word alignment precision for syntax-based
word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntaxbased machine translation . we present an algorithm for identifying and deleting incorrect word alignment links , using features of the extracted rules . we obtain gains in both alignment quality and translation quality in chinese-english and arabic-english translation experiments relative to a giza++ union baseline .

automatic extraction of cue phrases for cross-corpus dialogue act classification
in this paper , we present an investigation into the use of cue phrases as a basis for dialogue act classification . we define what we mean by cue phrases , and describe how we extract them from a manually labelled corpus of dialogue . we describe one method of evaluating the usefulness of such cue phrases , by applying them directly as a classifier to unseen utterances . once we have extracted cue phrases from one corpus , we determine if these phrases are general in nature , by applying them directly as a classification mechanism to a different corpus to that from which they were extracted . finally , we experiment with increasingly restrictive methods for selecting cue phrases , and demonstrate that there are a small number of core cue phrases that are useful for dialogue act classification .

searching the web by voice
spoken queries are a natural medium for searching the web in settings where typing on a keyboard is not practical . this paper describes a speech interface to the google search engine . we present experiments with various statistical language models , concluding that a unigram model with collocations provides the best combination of broad coverage , predictive power , and real-time performance . we also report accuracy results of the prototype system .

opinion summarization with integer linear programming formulation for sentence extraction and ordering
in this paper we propose a novel algorithm for opinion summarization that takes account of content and coherence , simultaneously . we consider a summary as a sequence of sentences and directly acquire the optimum sequence from multiple review documents by extracting and ordering the sentences . we achieve this with a novel integer linear programming ( ilp ) formulation . our proposed formulation is a powerful mixture of the maximum coverage problem and the traveling salesman problem , and is widely applicable to text generation and summarization tasks . we score each candidate sequence according to its content and coherence . since our research goal is to summarize reviews , the content score is defined by opinions and the coherence score is developed in training against the review document corpus . we evaluate our method using the reviews of commodities and restaurants . our method outperforms existing opinion summarizers as indicated by its rouge score . we also report the results of human readability experiments .

wordform- and class-based prediction of the components of german nominal compounds
in word prediction systems for augmentative and alternative communication ( aac ) , productive wordformation processes such as compounding pose a serious problem . we present a model that predicts german nominal compounds by splitting them into their modifier and head components , instead of trying to predict them as a whole . the model is improved further by the use of class-based modifierhead bigrams constructed using semantic classes automatically extracted from a corpus . the evaluation shows that the split compound model with class bigrams leads to an improvement in keystroke savings of more than 15 % over a no split compound baseline model . we also present preliminary results obtained with a word prediction model integrating compound and simple word prediction .

chinese named entity recognition combining a statistical model
named entity recognition is one of the key techniques in the fields of natural language processing , information retrieval , question answering and so on . unfortunately , chinese named entity recognition ( ner ) is more difficult for the lack of capitalization information and the uncertainty in word segmentation . in this paper , we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well . in order to avoid data sparseness problem , we employ a back-off model and /tong yi ci ci lin , a chinese thesaurus , to smooth the parameters in the model . the f-measure of person names , location names , and organization names on the newswire test data for the 1999 ieer evaluation in mandarin is 86.84 % , 84.40 % and 76.22 % respectively .

a comparative study for query translation using
in cross language information retrieval ( clir ) , query terms can be translated to the document language using bilingual dictionaries ( bds ) or statistical translation models ( stms ) . combining different translation resources can also be used to improve the performance . unfortunately , the most studies on combining multiple resources use simple methods such as linear combination . in this paper , we drew up a comparative study between linear combination and confidence measures to combine multiple translation resources for the purpose of clir . we show that the linear combination method is unable to combine correctly different types of resources such as bds and stms . while the confidence measure method is able to re-weight the translation candidate more radically than in linear combination . it reconsiders each translation candidate proposed by different resources with respect to additional features . we tested the two methods on different test clir collections and the results show that the confidence measure outperforms the linear combination method .

bagpack : a general framework to represent semantic relations
we introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently . the resulting features are of sufficient generality to allow us , with the help of a standard supervised machine learning algorithm , to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring .

unsupervised translation induction for chinese abbreviations using monolingual corpora
chinese abbreviations are widely used in modern chinese texts . compared with english abbreviations ( which are mostly acronyms and truncations ) , the formation of chinese abbreviations is much more complex . due to the richness of chinese abbreviations , many of them may not appear in available parallel corpora , in which case current machine translation systems simply treat them as unknown words and leave them untranslated . in this paper , we present a novel unsupervised method that automatically extracts the relation between a full-form phrase and its abbreviation from monolingual corpora , and induces translation entries for the abbreviation by using its full-form as a bridge . our method does not require any additional annotated data other than the data that a regular translation system uses . we integrate our method into a state-ofthe-art baseline translation system and show that it consistently improves the performance of the baseline system on various nist mt test sets .

what humour tells us about discourse theories
many verbal jokes , like garden path sentences , pose difficulties to models of discourse since the initially primed interpretation needs to be discarded and a new one created based on subsequent statements . the effect of the joke depends on the fact that the second ( correct ) interpretation was not visible earlier . existing models of discourse semantics in principle generate all interpretations of discourse fragments and carry these until contradicted , and thus the dissonance criteria in humour can not be met . computationally , maintaining all possible worlds in a discourse is very inefficient , thus computing only the maximum-likelihood interpretation seems to be a more efficient choice on average . in this work we outline a probabilistic lexicon based lexical semantics approach which seems to be a reasonable construct for discourse in general and use some examples from humour to demonstrate its working .

practical issues in compiling typed unification grammars for speech
current alternatives for language modeling are statistical techniques based on large amounts of training data , and hand-crafted context-free or finite-state grammars that are difficult to build and maintain . one way to address the problems of the grammar-based approach is to compile recognition grammars from grammars written in a more expressive formalism . while theoretically straight-forward , the compilation process can exceed memory and time bounds , and might not always result in accurate and efficient speech recognition . we will describe and evaluate two approaches to this compilation problem . we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model .

plans toward automated chat summarization
we describe the beginning stages of our work on summarizing chat , which is motivated by our observations concerning the information overload of us navy watchstanders . we describe the challenges of summarizing chat and focus on two chat-specific types of summarizations we are interested in : thread summaries and temporal summaries . we then discuss our plans for addressing these challenges and evaluation issues .

dynamically generating a protein entity dictionary using online resources
knowledge stored in free text , natural language processing ( nlp ) has received much attention recently to make the task of managing information recorded in free text more feasible . one requirement for most nlp systems is the ability to accurately recognize biological entity terms in free text and the ability to map these terms to corresponding records in databases . such task is called biological named entity tagging . in this paper , we present a system that automatically constructs a protein entity dictionary , which contains gene or protein names associated with uniprot identifiers using online resources . the system can run periodically to always keep up-to-date with these online resources . using online resources that were available on dec. 25 , 2004 , we obtained 4,046,733 terms for 1,640,082 entities . the dictionary can be accessed from the following website : http : //biocreative.ifsm.umbc.edu/biothesauru s/ . contact : hfliu @ umbc.edu

unsupervised concept annotation using latent dirichlet allocation and
training efficient statistical approaches for natural language understanding generally requires data with segmental semantic annotations . unfortunately , building such resources is costly . in this paper , we propose an approach that produces annotations in an unsupervised way . the first step is an implementation of latent dirichlet alocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence . this knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models ( ibm models ) to produce the final semantic annotation . the relation between automaticallyderived topics and task-dependent concepts is evaluated on a spoken dialogue task with an available reference annotation .

building a large chinese corpus annotated with semantic dependency
at present most of corpora are annotated mainly with syntactic knowledge . in this paper , we attempt to build a large corpus and annotate semantic knowledge with dependency grammar . we believe that words are the basic units of semantics , and the structure and meaning of a sentence consist mainly of a series of semantic dependencies between individual words . a 1,000,000-wordscale corpus annotated with semantic dependency has been built . compared with syntactic knowledge , semantic knowledge is more difficult to annotate , for ambiguity problem is more serious . in the paper , the strategy to improve consistency is addressed , and congruence is defined to measure the consistency of tagged corpus.. finally , we will compare our corpus with other well-known corpora .

a web-based geo-resolution annotation and evaluation tool
in this paper we present the edinburgh geo-annotator , a web-based annotation tool for the manual geo-resolution of location mentions in text using a gazetteer . the annotation tool has an interlinked text and map interface which lets annotators pick correct candidates within the gazetteer more easily . the geo-annotator can be used to correct the output of a geoparser or to create gold standard geo-resolution data . we include accompanying scoring software for geo-resolution evaluation .

the new thot toolkit for fully-automatic and interactive statistical
we present the new thot toolkit for fullyautomatic and interactive statistical machine translation ( smt ) . initial public versions of thot date back to 2005 and did only include estimation of phrase-based models . by contrast , the new version offers several new features that had not been previously incorporated . the key innovations provided by the toolkit are computeraided translation , including post-editing and interactive smt , incremental learning and robust generation of alignments at phrase level . in addition to this , the toolkit also provides standard smt features such as fully-automatic translation , scalable and parallel algorithms for model training , client-server implementation of the translation functionality , etc . the toolkit can be compiled in unix-like and windows platforms and it is released under the gnu lesser general public license ( lgpl ) .

context-sensitive utterance planning for ccg
the paper presents an approach to utterance planning , which can dynamically use context information about the environment in which a dialogue is situated . the approach is functional in nature , using systemic networks to specify its planning grammar . the planner takes a description of a communicative goal as input , and produces one or more logical forms that can express that goal in a contextually appropriate way . both the goal and the resulting logical forms are expressed in a single formalism as ontologically rich , relational structures . to realize the logical forms , openccg is used . the paper focuses primarily on the implementation , but also discusses how the planning grammar can be based on the grammar used in openccg , and trained on ( parseable ) data .

logarithmic opinion pools for conditional random fields division of informatics and software engineering division of informatics
recent work on conditional random fields ( crfs ) has demonstrated the need for regularisation to counter the tendency of these models to overfit . the standard approach to regularising crfs involves a prior distribution over the model parameters , typically requiring search over a hyperparameter space . in this paper we address the overfitting problem from a different perspective , by factoring the crf distribution into a weighted product of individual expert crf distributions . we call this model a logarithmic opinion pool ( lop ) of crfs ( lop-crfs ) . we apply the lop-crf to two sequencing tasks . our results show that unregularised expert crfs with an unregularised crf under a lop can outperform the unregularised crf , and attain a performance level close to the regularised crf . lop-crfs therefore provide a viable alternative to crf regularisation without the need for hyperparameter search .

experiments with word alignment , normalization and clause reordering for smt between english and german
this paper presents the liu system for the wmt 2011 shared task for translation between german and english . for english german we attempted to improve the translation tables with a combination of standard statistical word alignments and phrase-based word alignments . for germanenglish translation we tried to make the german text more similar to the english text by normalizing german morphology and performing rule-based clause reordering of the german text . this resulted in small improvements for both translation directions .

learning bigrams from unigrams
traditional wisdom holds that once documents are turned into bag-of-words ( unigram count ) vectors , word orders are completely lost . we introduce an approach that , perhaps surprisingly , is able to learn a bigram language model from a set of bag-of-words documents . at its heart , our approach is an em algorithm that seeks a model which maximizes the regularized marginal likelihood of the bagof-words documents . in experiments on seven corpora , we observed that our learned bigram language models : i ) achieve better test set perplexity than unigram models trained on the same bag-of-words documents , and are not far behind oracle bigram models trained on the corresponding ordered documents ; ii ) assign higher probabilities to sensible bigram word pairs ; iii ) improve the accuracy of ordereddocument recovery from a bag-of-words . our approach opens the door to novel phenomena , for example , privacy leakage from index files .

a viable method for rapid discovery of arabic nicknames chiara higgins elizabeth mcgrath lailla moretto
this paper presents findings on using crowdsourcing via amazon mechanical turk ( mturk ) to obtain arabic nicknames as a contribution to exiting named entity ( ne ) lexicons . it demonstrates a strategy for increasing mturk participation from arab countries . the researchers validate the nicknames using experts , mturk workers , and google search and then compare them against the database of arabic names ( dan ) . additionally , the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where mturk workers respond to existing work batches , called human intelligence tasks ( hits ) , more quickly once similar higher paying hits are posted .

automatic question generation for vocabulary assessment
in the reap system , users are automatically provided with texts to read targeted to their individual reading levels . to find appropriate texts , the users vocabulary knowledge must be assessed . we describe an approach to automatically generating questions for vocabulary assessment . traditionally , these assessments have been hand-written . using data from wordnet , we generate 6 types of vocabulary questions . they can have several forms , including wordbank and multiple-choice . we present experimental results that suggest that these automatically-generated questions give a measure of vocabulary skill that correlates well with subject performance on independently developed humanwritten questions . in addition , strong correlations with standardized vocabulary tests point to the validity of our approach to automatic assessment of word knowledge .

learning dependency-based compositional semantics
compositional question answering begins by mapping questions to logical forms , but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms . in this paper , we learn to map questions to answers via latent logical forms , which are induced automatically from question-answer pairs . in tackling this challenging learning problem , we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms . on two standard semantic parsing benchmarks ( geo and jobs ) , our system obtains the highest published accuracies , despite requiring no annotated logical forms .

joint and conditional estimation of tagging and parsing models
this paper compares two different ways of estimating statistical language models . many statistical nlp tagging and parsing models are estimated by maximizing the ( joint ) likelihood of the fully-observed training data . however , since these applications only require the conditional probability distributions , these distributions can in principle be learnt by maximizing the conditional likelihood of the training data . perhaps somewhat surprisingly , models estimated by maximizing the joint were superior to models estimated by maximizing the conditional , even though some of the latter models intuitively had access to more information .

data-defined kernels for parse reranking derived from probabilistic models
previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees , which are hand-crafted based on domain knowledge and computational considerations . in this paper we propose a method for defining kernels in terms of a probabilistic model of parsing . this model is then trained , so that the parameters of the probabilistic model reflect the generalizations in the training data . the method we propose then uses these trained parameters to define a kernel for reranking parse trees . in experiments , we use a neural network based statistical parser as the probabilistic model , and use the resulting kernel with the voted perceptron algorithm to rerank the top 20 parses from the probabilistic model . this method achieves a significant improvement over the accuracy of the probabilistic model .

automatic estimation of word significance oriented for speech-based information retrieval
automatic estimation of word significance oriented for speech-based information retrieval ( ir ) is addressed . since the significance of words differs in ir , automatic speech recognition ( asr ) performance has been evaluated based on weighted word error rate ( wwer ) , which gives a weight on errors from the viewpoint of ir , instead of word error rate ( wer ) , which treats all words uniformly . a decoding strategy that minimizes wwer based on a minimum bayes-risk framework has been shown , and the reduction of errors on both asr and ir has been reported . in this paper , we propose an automatic estimation method for word significance ( weights ) based on its influence on ir . specifically , weights are estimated so that evaluation measures of asr and ir are equivalent . we apply the proposed method to a speech-based information retrieval system , which is a typical ir system , and show that the method works well .

cluster-based query expansion for statistical question answering
document retrieval is a critical component of question answering ( qa ) , yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for qa . this paper introduces a new , cluster-based query expansion method that learns queries known to be successful when applied to similar questions . we show that cluster-based expansion improves the retrieval performance of a statistical question answering system when used in addition to existing query expansion methods . this paper presents experiments with several feature selection methods used individually and in combination . we show that documents retrieved using the cluster-based approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors .

regularized least-squares classification for word sense
the paper describes rlsc-lin and rlsccomb systems which participated in the senseval-3 english lexical sample task . these systems are based on regularized least-squares classification ( rlsc ) learning method . we describe the reasons of choosing this method , how we applied it to word sense disambiguation , what results we obtained on senseval1 , senseval-2 and senseval-3 data and discuss some possible improvements .

towards a description of symbolic maps
symbolic resources for text synthesis and text analysis are typically created and stored separately . in our case , we have a kpmlresource ( nigel ) and a ccg for english . in this paper , we argue that reversing efficient resources such as ours can not in general be achieved . for this reason , we propose a symbolic map that can be converted automatically into both synthesis- and analysis-oriented resources . we show that completeness of description can only be achieved by such a map while efficiency concerns can only be tackled by the directed rules of task-oriented resources not because of the current state of the art , but because reversing task-oriented symbolic resources is impossible in principle .

improving reordering performance using higher order and structural
recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order . this model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the traveling salesman problem ( tsp ) . however , for efficiently solving the tsp , the model is restricted to pairwise features which examine only a pair of words and their neighborhood . in this work , we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the tsp model using higher order and structural features which help in capturing longer range dependencies . in addition to using a more informative set of source side features , we also capture target side features indirectly by using the translation score assigned to a reordering . our experiments , involving urdu-english , show that the proposed approach outperforms a state-of-theart pbsmt system which uses the tsp model for reordering by 1.3 bleu points , and a publicly available state-of-the-art mt system , hiero , by 3 bleu points .

a simple bayesian modelling approach to event extraction from twitter
with the proliferation of social media sites , social streams have proven to contain the most up-to-date information on current events . therefore , it is crucial to extract events from the social streams such as tweets . however , it is not straightforward to adapt the existing event extraction systems since texts in social media are fragmented and noisy . in this paper we propose a simple and yet effective bayesian model , called latent event model ( lem ) , to extract structured representation of events from social media . lem is fully unsupervised and does not require annotated data for training . we evaluate lem on a twitter corpus . experimental results show that the proposed model achieves 83 % in f-measure , and outperforms the state-of-the-art baseline by over 7 % .

arabic text to arabic sign language translation system for the deaf and
this paper describes a machine translation system that offers many deaf and hearingimpaired people the chance to access published information in arabic by translating text into their first language , arabic sign language ( arsl ) . the system was created under the close guidance of a team that included three deaf native signers and one arsl interpreter . we discuss problems inherent in the design and development of such translation systems and review previous arsl machine translation systems , which all too often demonstrate a lack of collaboration between engineers and the deaf community . we describe and explain in detail both the adapted translation approach chosen for the proposed system and the arsl corpus that we collected for this purpose . the corpus has 203 signed sentences ( with 710 distinct signs ) with content restricted to the domain of instructional language as typically used in deaf education . evaluation shows that the system produces translated sign sentences outputs with an average word error rate of 46.7 % and an average position error rate of 29.4 % using leave-oneout cross validation . the most frequent source of errors is missing signs in the corpus ; this could be addressed in future by collecting more corpus material .

annotation of regular polysemy and underspecification hector martnez alonso , bolette sandford pedersen
we present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in english , danish and spanish . this article describes the annotation process , the results in terms of inter-encoder agreement , and the sense distributions obtained with two methods : majority voting with a theory-compliant backoff strategy , and mace , an unsupervised system to choose the most likely sense from all the annotations .

combine person name and person identity recognition and document clustering for chinese person name disambiguation
this paper presents the hitsz_cityu system in the cips-sighan bakeoff 2010 task 3 , chinese person name disambiguation . this system incorporates person name string recognition , person identity string recognition and an agglomerative hierarchical clustering for grouping the documents to each identical person . firstly , for the given name index string , three segmentors are applied to segment the sentences having the index string into chinese words , respectively . their outputs are compared and analyzed . an unsupervised clustering is applied here to help the personal name recognition . the document set is then divided into subsets according to each recognized person name string . next , the system identifies/extracts the person identity string from the sentences based on lexicon and heuristic rules . by incorporating the recognized person identity string , person name , organization name and contextual content words as features , an agglomerative hierarchical clustering is applied to group the similar documents in the document subsets to obtain the final person name disambiguation results . evaluations show that the proposed system , which incorporates extraction and clustering technique , achieves encouraging recall and good overall performance .

docrep : a lightweight and efficient document representation framework
modelling linguistic phenomena requires highly structured and complex data representations . document representation frameworks ( drfs ) provide an interface to store and retrieve multiple annotation layers over a document . researchers face a difficult choice : using a heavy-weight drf or implement a custom drf . the cost is substantial , either learning a new complex system , or continually adding features to a home-grown system that risks overrunning its original scope . we introduce docrep , a lightweight and efficient drf , and compare it against existing drfs . we discuss our design goals and implementations in c ++ , python , and java . we transform the ontonotes 5 corpus using docrep and uima , providing a quantitative comparison , as well as discussing modelling trade-offs . we conclude with qualitative feedback from researchers who have used docrep for their own projects . ultimately , we hope docrep is useful for the busy researcher who wants the benefits of a drf , but has better things to do than to write one .

feature-rich discriminative phrase rescoring for smt fei huang and bing xiang
this paper proposes a new approach to phrase rescoring for statistical machine translation ( smt ) . a set of novel features capturing the translingual equivalence between a source and a target phrase pair are introduced . these features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair . these phrase scores are used to discriminatively rescore the baseline mt systems phrase library : boost good phrase translations while prune bad ones . this approach not only significantly improves machine translation quality , but also reduces the model size by a considerable margin .

technical support dialog systems :
the goal of this paper is to give a description of the state of the art , the issues , the problems , and the solutions related to industrial dialog systems for the automation of technical support . after a general description of the evolution of the spoken dialog industry , and the challenges in the development of technical support applications , we will discuss two specific problems through a series of experimental results . the first problem is the identification of the call reason , or symptom , from loosely constrained user utterances . the second is the use of data for the experimental optimization of the voice user interface ( vui ) .

feature-rich part-of-speech tagging with a cyclic dependency network
we present a new part-of-speech tagger that demonstrates the following ideas : ( i ) explicit use of both preceding and following tag contexts via a dependency network representation , ( ii ) broad use of lexical features , including jointly conditioning on multiple consecutive words , ( iii ) effective use of priors in conditional loglinear models , and ( iv ) fine-grained modeling of unknown word features . using these ideas together , the resulting tagger gives a 97.24 % accuracy on the penn treebank wsj , an error reduction of 4.4 % on the best previous single automatically learned tagging result .

re-ranking models for spoken language understanding
spoken language understanding aims at mapping a natural language spoken sentence into a semantic representation . in the last decade two main approaches have been pursued : generative and discriminative models . the former is more robust to overfitting whereas the latter is more robust to many irrelevant features . additionally , the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task . in this paper we describe a machine learning framework where both models are used : a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and support vector machines , re-ranks such list . we tested our approach on the media corpus ( human-machine dialogs ) and on a new corpus ( human-machine and humanhuman dialogs ) produced in the european luna project . the results show a large improvement on the state-of-the-art in concept segmentation and labeling .

a hybrid approach to the induction of underlying morphology
we present a technique for refining a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure . performance on measures which consider surface-boundary accuracy and underlying morpheme consistency indicates this technique leads to improvements over baseline segmentations for english and turkish word lists .

toward opinion summarization : linking the sources
we target the problem of linking source mentions that belong to the same entity ( source coreference resolution ) , which is needed for creating opinion summaries . in this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution , apply a state-of-the-art coreference resolution approach to the transformed data , and evaluate on an available corpus of manually annotated opinions .

topicspam : a topic-model-based approach for spam detection
product reviews are now widely used by individuals and organizations for decision making ( litvin et al , 2008 ; jansen , 2010 ) . and because of the profits at stake , people have been known to try to game the system by writing fake reviews to promote target products . as a result , the task of deceptive review detection has been gaining increasing attention . in this paper , we propose a generative lda-based topic modeling approach for fake review detection . our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95 % accuracy on review spam datasets , outperforming existing baselines by a large margin .

how well does active learning actually work time-based evaluation
machine involvement has the potential to speed up language documentation . we assess this potential with timed annotation experiments that consider annotator expertise , example selection methods , and suggestions from a machine classifier . we find that better example selection and label suggestions improve efficiency , but effectiveness depends strongly on annotator expertise . our expert performed best with uncertainty selection , but gained little from suggestions . our non-expert performed best with random selection and suggestions . the results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators .

generating focused topic-specific sentiment lexicons
we present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents . we motivate the need for such lexicons in the field of media analysis , describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon , and evaluate the quality of the generated lexicons both manually and using a trec blog track test set for opinionated blog post retrieval . although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon , they maintain , or even improve , the performance of an opinion retrieval system .

tools and resources for tree adjoining grammars
this paper presents a workbench for tree adjoining grammars that we are currently developing . this workbench includes several tools and resources based on the markup language xml , used as a convenient language to format and exchange linguistic resources .

the phenogrammar of coordination
linear categorial grammar ( lincg ) is a sign-based , curryesque , relational , logical categorial grammar ( cg ) whose central architecture is based on linear logic . curryesque grammars separate the abstract combinatorics ( tectogrammar ) of linguistic expressions from their concrete , audible representations ( phenogrammar ) . most of these grammars encode linear order in string-based lambda terms , in which there is no obvious way to distinguish right from left . without some notion of directionality , grammars are unable to differentiate , say , subject and object for purposes of building functorial coordinate structures . we introduce the notion of a phenominator as a way to encode the term structure of a functor separately from its string support . this technology is then employed to analyze a range of coordination phenomena typically left unaddressed by linear logic-based curryesque frameworks .

speeding up the design of dialogue applications by using database
nowadays , most commercial and research dialogue applications for call centers are created using sophisticated and fullyfeature development platforms . surprisingly , most of them lack of some kind of acceleration strategy based on an automatic analysis of the contents or structure of the backend database . this paper describes our efforts to incorporate this kind of information which continues the work done in ( dharo et al 2006 ) . our main proposed strategies are : the generation of automatic state proposals for defining the dialogue flow network , the automatic selection of slots to be requested using mixed-initiative , the semi-automatic generation of sql statements , and the quick generation of the data model of the application and the connection with the database fields . subjective and objective evaluations demonstrate the advantages of using the accelerations and their high acceptance , both in our current proposals and in previous work .

correlations in the organization of large-scale syntactic ramon ferrer i cancho
we study the correlations in the connectivity patterns of large scale syntactic dependency networks . these networks are induced from treebanks : their vertices denote word forms which occur as nuclei of dependency trees . their edges connect pairs of vertices if at least two instance nuclei of these vertices are linked in the dependency structure of a sentence . we examine the syntactic dependency networks of seven languages . in all these cases , we consistently obtain three findings . firstly , clustering , i.e. , the probability that two vertices which are linked to a common vertex are linked on their part , is much higher than expected by chance . secondly , the mean clustering of vertices decreases with their degree this finding suggests the presence of a hierarchical network organization . thirdly , the mean degree of the nearest neighbors of a vertex x tends to decrease as the degree of x grows this finding indicates disassortative mixing in the sense that links tend to connect vertices of dissimilar degrees . our results indicate the existence of common patterns in the large scale organization of syntactic dependency networks .

word alignment in english-hindi parallel corpus using recency-vector approach : some studies
word alignment using recency-vector based approach has recently become popular . one major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small . this makes these algorithms worth-studying for languages where resources are scarce . in this work we studied the performance of two very popular recency-vector based approaches , proposed in ( fung and mckeown , 1994 ) and ( somers , 1998 ) , respectively , for word alignment in english-hindi parallel corpus . but performance of the above algorithms was not found to be satisfactory . however , subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus . the present paper discusses the new version of the algorithm and its performance in detail .

unsupervised models for coreference resolution
we present a generative model for unsupervised coreference resolution that views coreference as an em clustering process . for comparison purposes , we revisit haghighi and kleins ( 2007 ) fully-generative bayesian model for unsupervised coreference resolution , discuss its potential weaknesses and consequently propose three modifications to their model . experimental results on the ace data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model .

an iterative reinforcement approach for fine-grained
with the in-depth study of sentiment analysis research , finer-grained opinion mining , which aims to detect opinions on different review features as opposed to the whole review level , has been receiving more and more attention in the sentiment analysis research community recently . most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms , which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews , which satisfies ( 1 ) the review features are not appear explicit in the review sentences ; ( 2 ) it can be deduced by the opinion words in its context . from an information theoretic point of view , this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem . more specifically , the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information . the experimental results demonstrate that our approach outperforms the template extraction based approaches .

positive results for parsing with a bounded stack using a model-based
statistical parsing models have recently been proposed that employ a bounded stack in timeseries ( left-to-right ) recognition , using a rightcorner transform defined over training trees to minimize stack use ( schuler et al , 2008 ) . corpus results have shown that a vast majority of naturally-occurring sentences can be parsed in this way using a very small stack bound of three to four elements . this suggests that the standard cubic-time cky chart-parsing algorithm , which implicitly assumes an unbounded stack , may be wasting probability mass on trees whose complexity is beyond human recognition or generation capacity . this paper first describes a version of the rightcorner transform that is defined over entire probabilistic grammars ( cast as infinite sets of generable trees ) , in order to ensure a fair comparison between bounded-stack and unbounded pcfg parsing using a common underlying model ; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack cky parser using the original grammar .

recognizing stances in online debates
this paper presents an unsupervised opinion analysis method for debate-side classification , i.e. , recognizing which stance a person is taking in an online debate . in order to handle the complexities of this genre , we mine the web to learn associations that are indicative of opinion stances in debates . we combine this knowledge with discourse information , and formulate the debate side classification task as an integer linear programming problem . our results show that our method is substantially better than challenging baseline methods .

a weakly-supervised approach to argumentative zoning of scientific cnrs & ens , france
argumentative zoning ( az ) analysis of the argumentative structure of a scientific paper has proved useful for a number of information access tasks . current approaches to az rely on supervised machine learning ( ml ) . requiring large amounts of annotated data , these approaches are expensive to develop and port to different domains and tasks . a potential solution to this problem is to use weaklysupervised ml instead . we investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple az classes . our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier , yielding a high accuracy of 81 % when using just 10 % of the labeled data . this result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of az across different information access tasks .

a rudimentary lexicon and semantics help bootstrap phoneme abdellah fourtassi emmanuel dupoux
infants spontaneously discover the relevant phonemes of their language without any direct supervision . this acquisition is puzzling because it seems to require the availability of high levels of linguistic structures ( lexicon , semantics ) , that logically suppose the infants having a set of phonemes already . we show how this circularity can be broken by testing , in realsize language corpora , a scenario whereby infants would learn approximate representations at all levels , and then refine them in a mutually constraining way . we start with corpora of spontaneous speech that have been encoded in a varying number of detailed context-dependent allophones . we derive , in an unsupervised way , an approximate lexicon and a rudimentary semantic representation . despite the fact that all these representations are poor approximations of the ground truth , they help reorganize the fine grained categories into phoneme-like categories with a high degree of accuracy . one of the most fascinating facts about human infants is the speed at which they acquire their native language . during the first year alone , i.e. , before they are able to speak , infants achieve impressive landmarks regarding three key language components . first , they tune in on the phonemic categories of their language ( werker and tees , 1984 ) . second , they learn to segment the continuous speech stream into discrete units ( jusczyk and aslin , 1995 ) .

towards an on-demand simple portuguese wikipedia arnaldo candido junior ann copestake lucia specia sandra maria alusio
the simple english wikipedia provides a simplified version of wikipedia 's english articles for readers with special needs . however , there are fewer efforts to make information in wikipedia in other languages accessible to a large audience . this work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a simple portuguese wikipedia on demand , based on user interactions with the main portuguese wikipedia . our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words , while our system can correctly simplify 22,200 occurrences , with estimated f-measure 77.2 % .

low-dimensional embeddings of logic
many machine reading approaches , from shallow information extraction to deep semantic parsing , map natural language to symbolic representations of meaning . representations such as first-order logic capture the richness of natural language and support complex reasoning , but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference . in contrast , low-dimensional embeddings ( i.e . distributional representations ) are efficient and enable generalization , but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic . in this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic .

spatial prepositions in context : the semantics of near in the presence of distractor objects
the paper examines how peoples judgements of proximity between two objects are influenced by the presence of a third object . in an experiment participants were presented with images containing three shapes in different relative positions , and asked to rate the acceptability of a locative expression such as the circle is near the triangle as descriptions of those images . the results showed an interaction between the relative positions of objects and the linguistic roles that those objects play in the locative expression : proximity was a decreasing function of the distance between the head object in the expression and the prepositional clause object , and an increasing function the distance between the head and the third , distractor object . this finding leads us to a new account for the semantics of spatial prepositions such as near .

whats in a translation rule
we propose a theory that gives formal semantics to word-level alignments defined over parallel corpora . we use our theory to introduce a linear algorithm that can be used to derive from word-aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data .

segment choice models : feature-rich models for global distortion in statistical machine translation
this paper presents a new approach to distortion ( phrase reordering ) in phrasebased machine translation ( mt ) . distortion is modeled as a sequence of choices during translation . the approach yields trainable , probabilistic distortion models that are global : they assign a probability to each possible phrase reordering . these segment choice models ( scms ) can be trained on segment-aligned sentence pairs ; they can be applied during decoding or rescoring . the approach yields a metric called distortion perplexity ( disperp ) for comparing scms offline on test data , analogous to perplexity for language models . a decision-tree-based scm is tested on chinese-to-english translation , and outperforms a baseline distortion penalty approach at the 99 % confidence level .

memory-based dependency parsing
this paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text . using data from a small treebank of swedish , memory-based classifiers for predicting the next action of the parser are constructed . the accuracy of a classifier as such is evaluated on held-out data derived from the treebank , and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank . the evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further .

compressing trigram language models with golomb coding
trigram language models are compressed using a golomb coding method inspired by the original unix spell program . compression methods trade off space , time and accuracy ( loss ) . the proposed hashtbo method optimizes space at the expense of time and accuracy . trigram language models are normally considered memory hogs , but with hashtbo , it is possible to squeeze a trigram language model into a few megabytes or less . hashtbo made it possible to ship a trigram contextual speller in microsoft office 2007 .

syntactic and semantic structure for opinion expression detection
we demonstrate that relational features derived from dependency-syntactic and semantic role structures are useful for the task of detecting opinionated expressions in natural-language text , significantly improving over conventional models based on sequence labeling with local features . these features allow us to model the way opinionated expressions interact in a sentence over arbitrary distances . while the relational features make the prediction task more computationally expensive , we show that it can be tackled effectively by using a reranker . we evaluate a number of machine learning approaches for the reranker , and the best model results in a 10-point absolute improvement in soft recall on the mpqa corpus , while decreasing precision only slightly .

boosting-based multiway relation classification
we describe a boosting-based supervised learning approach to the multi-way classification of semantic relations between pairs of nominals task # 8 of semeval2 . participants were asked to determine which relation , from a set of nine relations plus other , exists between two nominals , and also to determine the roles of the two nominals in the relation . our participation has focused , rather than on the choice of a rich set of features , on the classification model adopted to determine the correct assignment of relation and roles .

post-hoc manipulations of vector space models
in this paper , we introduce several vector space manipulation methods that are applied to trained vector space models in a post-hoc fashion , and present an application of these techniques in semantic role labeling for finnish and english . specifically , we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments . further , we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments , within the same vector space .

an annotation scheme for citation function
we study the interplay of the discourse structure of a scientific argument with formal citations . one subproblem of this is to classify academic citations in scientific articles according to their rhetorical function , e.g. , as a rival approach , as a part of the solution , or as a flawed approach that justifies the current research . here , we introduce our annotation scheme with 12 categories , and present an agreement study .

a topic model for building fine-grained domain-specific emotion
emotion lexicons play a crucial role in sentiment analysis and opinion mining . in this paper , we propose a novel emotion-aware lda ( ealda ) model to build a domainspecific lexicon for predefined emotions that include anger , disgust , fear , joy , sadness , surprise . the model uses a minimal set of domain-independent seed words as prior knowledge to discover a domainspecific lexicon , learning a fine-grained emotion lexicon much richer and adaptive to a specific domain . by comprehensive experiments , we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon .

automatic generation of story highlights
in this paper we present a joint content selection and compression model for single-document summarization . the model operates over a phrase-based representation of the source document which we obtain by merging information from pcfg parse trees and dependency graphs . using an integer linear programming formulation , the model learns to select and combine phrases subject to length , coverage and grammar constraints . we evaluate the approach on the task of generating story highlightsa small number of brief , self-contained sentences that allow readers to quickly gather information on news stories . experimental results show that the models output is comparable to human-written highlights in terms of both grammaticality and content .

usp-each : improved frequency-based greedy attribute selection so paulo - brazil so paulo - brazil
we present a follow-up of our previous frequency-based greedy attribute selection strategy . the current version takes into account also the instructions given to the participants of tuna trials regarding the use of location information , showing an overall improvement on string-edit distance values driven by the results on the furniture domain .

predicting thread discourse structure over technical web forums
online discussion forums are a valuable means for users to resolve specific information needs , both interactively for the participants and statically for users who search/browse over historical thread data . however , the complex structure of forum threads can make it difficult for users to extract relevant information . the discourse structure of web forum threads , in the form of labelled dependency relationships between posts , has the potential to greatly improve information access over web forum archives . in this paper , we present the task of parsing user forum threads to determine the labelled dependencies between posts . three methods , including a dependency parsing approach , are proposed to jointly classify the links ( relationships ) between posts and the dialogue act ( type ) of each link . the proposed methods significantly surpass an informed baseline . we also experiment with in situ classification of evolving threads , and establish that our best methods are able to perform equivalently well over partial threads as complete threads .

coverage and inheritance in the preposition project
in the preposition project ( tpp ) , 13 prepositions have now been analyzed and considerable data made available . these prepositions , among the most common words in english , contain 211 senses . by analyzing the coverage of these senses , it is shown that tpp provides potentially greater breadth and depth than other inventories of the range of semantic roles . specific inheritance mechanisms are developed within the preposition sense inventory and shown to be viable and provide a basis for the rationalization of the range of preposition meaning . in addition , this rationalization can be used for developing a data-driven mapping of a semantic role hierarchy . based on these findings and methodology , the broad structure of a wordnet-like representation of preposition meaning , with self-contained disambiguation tests , is outlined .

part-of-speech tagging with antagonistic adversaries
supervised nlp tools and on-line services are often used on data that is very different from the manually annotated data used during development . the performance loss observed in such cross-domain applications is often attributed to covariate shifts , with out-of-vocabulary effects as an important subclass . many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features . regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts . we present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets . while previous approaches do not improve on our supervised baseline , our approach is better across the board with an average 4 % error reduction .

the hiero machine translation system :
hierarchical organization is a well known property of language , and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations . in this paper , we discuss a new hierarchical phrase-based statistical machine translation system ( chiang , 2005 ) , presenting recent extensions to the original proposal , new evaluation results in a community-wide evaluation , and a novel technique for fine-grained comparative analysis of mt systems .

multi-relational latent semantic analysis wen-tau yih christopher meek
we present multi-relational latent semantic analysis ( mrlsa ) which generalizes latent semantic analysis ( lsa ) . mrlsa provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor . similar to lsa , a lowrank approximation of the tensor is derived using a tensor decomposition . each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix . the degree of two words having a specific relation can then be measured through simple linear algebraic operations . we demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources , mrlsa achieves stateof-the-art performance on existing benchmark datasets for two relations , antonymy and is-a .

discriminative word alignment with a function word reordering model
we address the modeling , parameter estimation and search challenges that arise from the

mulling : multilevel linguistic graphs for knowledge extraction
mulling is a model for knowledge extraction ( especially lexical extraction from corpora ) , based on multilevel graphs . its aim is to allow large-scale data acquisition , by making it easy to realize automatically , and simple to configure by linguists with limited knowledge in computer programming . in mulling , each new level represents the information in a different manner ( more and more abstract ) . we also introduce several associated operators , written to be as generic as possible . they are independent of what nodes and edges represent , and of the task to achieve . consequently , they allow the description of a complex extraction process as a succession of simple graph manipulations . finally , we present an experiment of collocation extraction using mulling model .

utilizing state-of-the-art parsers to diagnose problems in treebank annotation for a less resourced language technology , ho chi minh city
the recent success of statistical parsing methods has made treebanks become important resources for building good parsers . however , constructing highquality annotated treebanks is a challenging task . we utilized two publicly available parsers , berkeley and mst parsers , for feedback on improving the quality of part-of-speech tagging for the vietnamese treebank . analysis of the treebank and parsing errors revealed how problems with the vietnamese treebank influenced the parsing results and real difficulties of vietnamese parsing that required further improvements to existing parsing technologies .

chart mining-based lexical acquisition with precision grammars
in this paper , we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars . the general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks , and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars . as an illustration of the functionality of our proposed technique , we develop a lexical acquisition model for english verb particle constructions which operates over unlexicalised features mined from a partial parsing chart . the proposed technique is shown to outperform a state-of-the-art parser over the target task , despite being based on relatively simplistic features .

talking robots with lego mindstorms
this paper shows how talking robots can be built from off-the-shelf components , based on the lego mindstorms robotics platform . we present four robots that students created as final projects in a seminar we supervised . because lego robots are so affordable , we argue that it is now feasible for any dialogue researcher to tackle the interesting challenges at the robot-dialogue interface .

automatic comma insertion for japanese text generation
this paper proposes a method for automatically inserting commas into japanese texts . in japanese sentences , commas play an important role in explicitly separating the constituents , such as words and phrases , of a sentence . the method can be used as an elemental technology for natural language generation such as speech recognition and machine translation , or in writing-support tools for non-native speakers . we categorized the usages of commas and investigated the appearance tendency of each category . in this method , the positions where commas should be inserted are decided based on a machine learning approach . we conducted a comma insertion experiment using a text corpus and confirmed the effectiveness of our method .

what and where : an empirical investigation of pointing gestures and descriptions in multimodal referring actions
pointing gestures are pervasive in human referring actions , and are often combined with spoken descriptions . combining gesture and speech naturally to refer to objects is an essential task in multimodal nlg systems . however , the way gesture and speech should be combined in a referring act remains an open question . in particular , it is not clear whether , in planning a pointing gesture in conjunction with a description , an nlg system should seek to minimise the redundancy between them , e.g . by letting the pointing gesture indicate locative information , with other , nonlocative properties of a referent included in the description . this question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism . this paper investigates this question empirically , using machine-learning techniques on a new corpus of dialogues involving multimodal references to objects . our results indicate that human pointing strategies interact with descriptive strategies . in particular , pointing gestures are strongly associated with the use of locative features in referring expressions .

the life and death of discourse entities : identifying singleton mentions
a discourse typically involves numerous entities , but few are mentioned more than once . distinguishing discourse entities that die out after just one mention ( singletons ) from those that lead longer lives ( coreferent ) would benefit nlp applications such as coreference resolution , protagonist identification , topic modeling , and discourse coherence . we build a logistic regression model for predicting the singleton/coreferent distinction , drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features . the model is effective in its own right ( 78 % accuracy ) , and incorporating it into a state-of-the-art coreference resolution system yields a significant improvement .

first joint workshop on statistical parsing of morphologically rich languages improving the parsing of french coordination through annotation
in the present study we explore various methods for improving the transition-based parsing of coordinated structures in french . features targeting syntactic parallelism in coordinated structures are used as additional features when training the statistical model , but also as an efficient means to find and correct annotation errors in training corpora . in terms of annotation , we compare four different annotations for coordinated structures , demonstrate the importance of globally unambiguous annotation for punctuation , and discuss the decision process of a transition-based parser for coordination , explaining why certain annotations consistently out-perform others . we compare the gains provided by different annotation standards , by targeted features , and by using a wider beam . our best configuration gives a 37.28 % reduction in the coordination error rate , when compared to the baseline spmrl test corpus for french after manual corrections .

collocation translation acquisition using monolingual corpora
collocation translation is important for machine translation and many other nlp tasks . unlike previous methods using bilingual parallel corpora , this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge . first , dependency triples are extracted from chinese and english corpora with dependency parsers . then , a dependency triple translation model is estimated using the em algorithm based on a dependency correspondence assumption . the generated triple translation model is used to extract collocation translations from two monolingual corpora . experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction .

but what do they mean an exploration into the range of cross-turn expectations denied by but
in this paper we hypothesise that denial of expectation ( dofe ) across turns in dialogue signalled by but can involve a range of different expectations , i.e. , not just causal expectations , as argued in the literature . we will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey . finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to dofe and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations .

joint decoding with multiple translation models
current smt systems usually decode with single translation models and can not benefit from the strengths of other models in decoding phase . we instead propose joint decoding , a method that combines multiple translation models in one decoder . our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually . therefore , one model can share translations and even derivations with other models . comparable to the state-of-the-art system combination technique , joint decoding achieves an absolute improvement of 1.5 bleu points over individual decoding .

understanding the semantic structure of noun phrase queries
determining the semantic intent of web queries not only involves identifying their semantic class , which is a primary focus of previous works , but also understanding their semantic structure . in this work , we formally define the semantic structure of noun phrase queries as comprised of intent heads and intent modifiers . we present methods that automatically identify these constituents as well as their semantic roles based on markov and semi-markov conditional random fields . we show that the use of semantic features and syntactic features significantly contribute to improving the understanding performance .

tjp : using twitter to analyze the polarity of contexts tawunrat chalothorn jeremy ellman
this paper presents our system , tjp , which participated in semeval 2013 task 2 part a : contextual polarity disambiguation . the goal of this task is to predict whether marked contexts are positive , neutral or negative . however , only the scores of positive and negative class will be used to calculate the evaluation result using f-score . we chose to work as constrained , which used only the provided training and development data without additional sentiment annotated resources . our approach considered unigram , bigram and trigram using nave bayes training model with the objective of establishing a simpleapproach baseline . our system achieved fscore 81.23 % and f-score 78.16 % in the results for sms messages and tweets respectively .

enhancing grammatical cohesion : generating transitional expressions for smt
transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text . however , in most current statistical machine translation ( smt ) systems , the outputs of compound-complex sentences still lack proper transitional expressions . as a result , the translations are often hard to read and understand . to address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compoundcomplex sentence structure ( css ) . our models include a css-based translation model , which generates new css-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding . the two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness . the experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .

what we know about the voynich manuscript
the voynich manuscript is an undeciphered document from medieval europe . we present current knowledge about the manuscripts text through a series of questions about its linguistic properties .

finding short definitions of terms on web pages
we present a system that finds short definitions of terms on web pages . it employs a maximum entropy classifier , but it is trained on automatically generated examples ; hence , it is in effect unsupervised . we use rouge-w to generate training examples from encyclopedias and web snippets , a method that outperforms an alternative centroid-based one . after training , our system can be used to find definitions of terms that are not covered by encyclopedias . the system outperforms a comparable publicly available system , as well as a previously published form of our system .

comparison of similarity models for the relation discovery task
we present results on the relation discovery task , which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods . we describe a detailed experimental design that compares various configurations of conceptual representations and similarity measures across six different subsets of the ace relation extraction data . previous work on relation discovery used a semantic space based on a term-bydocument matrix . we find that representations based on term co-occurrence perform significantly better . we also observe further improvements when reducing the dimensionality of the term co-occurrence matrix using probabilistic topic models , though these are not significant .

context-rich syntactic translation models
statistical mt has made great progress in the last few years , but current translation models are weak on re-ordering and target language fluency . syntactic approaches seek to remedy these problems . in this paper , we take the framework for acquiring multi-level syntactic translation rules of ( galley et al , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words . second , we propose probability estimates and a training procedure for weighting these rules . we contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 bleu point increase over minimal rules .

estimating class priors in domain adaptation for word sense disambiguation yee seng chan and hwee tou ng
instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) . this in turn affects the accuracy of word sense disambiguation ( wsd ) systems trained and applied on different domains . this paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations . by using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in wsd accuracy .

uom : using explicit semantic analysis for classifying sentiments
in this paper , we describe our system submitted for the sentiment analysis task at semeval 2013 ( task 2 ) . we implemented a combination of explicit semantic analysis ( esa ) with naive bayes classifier . esa represents text as a high dimensional vector of explicitly defined topics , following the distributional semantic model . this approach is novel in the sense that esa has not been used for sentiment analysis in the literature , to the best of our knowledge .

cross-lingual predicate cluster acquisition to improve bilingual event extraction by inductive learning
in this paper we present two approaches to automatically extract cross-lingual predicate clusters , based on bilingual parallel corpora and cross-lingual information extraction . we demonstrate how these clusters can be used to improve the nist automatic content extraction ( ace ) event extraction task1 . we propose a new inductive learning framework to automatically augment background data for lowconfidence events and then conduct global inference . without using any additional data or accessing the baseline algorithms this approach obtained significant improvement over a state-of-the-art bilingual ( english and chinese ) event extraction system .

part-of-speech tagging for gujarati using conditional random
this paper describes a machine learning algorithm for gujarati part of speech tagging . the machine learning part is performed using a crf model . the features given to crf are properly chosen keeping the linguistic aspect of gujarati in mind . as gujarati is currently a less privileged language in the sense of being resource poor , manually tagged data is only around 600 sentences . the tagset contains 26 different tags which is the standard indian language ( il ) tagset . both tagged ( 600 sentences ) and untagged ( 5000 sentences ) are used for learning . the algorithm has achieved an accuracy of 92 % for gujarati texts where the training corpus is of 10,000 words and the test corpus is of 5,000 words .

a simple automatic mt evaluation metric
this paper describes a simple evaluation metric for mt which attempts to overcome the well-known deficits of the standard bleu metric from a slightly different angle . it employes levenshteins edit distance for establishing alignment between the mt output and the reference translation in order to reflect the morphological properties of highly inflected languages . it also incorporates a very simple measure expressing the differences in the word order . the paper also includes evaluation on the data from the previous smt workshop for several language pairs .

relation extraction with matrix factorization and universal schemas
traditional relation extraction predicts relations within some fixed and finite target schema . machine learning approaches to this task require either manual annotation or , in the case of distant supervision , existing structured sources of the same schema . the need for existing datasets can be avoided by using a universal schema : the union of all involved schemas ( surface form predicates as in openie , and relations in the schemas of preexisting databases ) . this schema has an almost unlimited set of relations ( due to surface forms ) , and supports integration with existing structured data ( through the relation types of existing databases ) . to populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations . we show that such latent models achieve substantially higher accuracy than a traditional classification approach . more importantly , by operating simultaneously on relations observed in text and in pre-existing structured dbs such as freebase , we are able to reason about unstructured and structured data in mutually-supporting ways . by doing so our approach outperforms stateof-the-art distant supervision .

learning to identify single-snippet answers to definition questions
we present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections . our method combines and extends two previous techniques that were based mostly on manually crafted lexical patterns and wordnet hypernyms . we train a support vector machine ( svm ) on vectors comprising the verdicts or attributes of the previous techniques , and additional phrasal attributes that we acquire automatically . the svm is then used to identify and rank single 250-character snippets that contain answers to definition questions . experimental results indicate that our method clearly outperforms the techniques it builds upon .

hindi to punjabi machine translation vishal goyal gurpreet singh lehal
hindi-punjabi being closely related language pair ( goyal v. and lehal g.s. , 2008 ) , hybrid machine translation approach has been used for developing hindi to punjabi machine translation system . non-availability of lexical resources , spelling variations in the source language text , source text ambiguous words , named entity recognition and collocations are the major challenges faced while developing this syetm . the key activities involved during translation process are preprocessing , translation engine and post processing . lookup algorithms , pattern matching algorithms etc formed the basis for solving these issues . the system accuracy has been evaluated using intelligibility test , accuracy test and bleu score . the hybrid syatem is found to perform better than the constituent systems .

discovering entailment relations using textual entailment patterns fabio massimo zanzotto maria teresa pazienza , marco pennacchiotti
in this work we investigate methods to enable the detection of a specific type of textual entailment ( strict entailment ) , starting from the preliminary assumption that these relations are often clearly expressed in texts . our method is a statistical approach based on what we call textual entailment patterns , prototypical sentences hiding entailment relations among two activities . we experimented the proposed method using the entailment relations of wordnet as test case and the web as corpus where to estimate the probabilities ; obtained results will be shown .

interpretation in a cognitive architecture
the work reported in this article presents a computational model of interpretation . the model proposes a cognitive architecture for intelligent agents to reason about competing analyses during interpretation and leverages the positive reinforcement principle .

a novel approach to mapping framenet lexical units to wordnet synsets
in this paper we present a novel approach to mapping framenet lexical units to wordnet synsets in order to automatically enrich the lexical unit set of a given frame . while the mapping approaches proposed in the past mainly rely on the semantic similarity between lexical units in a frame and lemmas in a synset , we exploit the definition of the lexical entries in framenet and the wordnet glosses to find the best candidate synset ( s ) for the mapping . evaluation results are also reported and discussed .

perceptron learning for chinese word segmentation
we explored a simple , fast and effective learning algorithm , the uneven margins perceptron , for chinese word segmentation . we adopted the character-based classification framework and transformed the task into several binary classification problems . we participated the close and open tests for all the four corpora . for the open test we only used the utf-8 code knowledge for discrimination among latin characters , arabic numbers and all other characters . our system performed well on the as , cityu and msr corpora but was clearly worse than the best result on the pku corpus .

boosting precision and recall of dictionary-based protein name crest , jst ( japan science and technology corporation )
dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides id information of recognized terms unlike machine learning based approaches . however , dictionary based approaches have two serious problems : ( 1 ) a large number of false recognitions mainly caused by short names . ( 2 ) low recall due to spelling variation . in this paper , we tackle the former problem by using a machine learning method to filter out false positives . we also present an approximate string searching method to alleviate the latter problem . experimental results using the genia corpus show that the filtering using a naive bayes classifier greatly improves precision with slight loss of recall , resulting in a much better f-score .

quality estimation of english-french machine translation : a detailed study of the role of syntax
we investigate the usefulness of syntactic knowledge in estimating the quality of english-french translations . we find that dependency and constituency tree kernels perform well but the error rate can be further reduced when these are combined with hand-crafted syntactic features . both types of syntactic features provide information which is complementary to tried-and-tested nonsyntactic features . we then compare source and target syntax and find that the use of parse trees of machine translated sentences does not affect the performance of quality estimation nor does the intrinsic accuracy of the parser itself . however , the relatively flat structure of the french treebank does appear to have an adverse effect , and this is significantly improved by simple transformations of the french trees . finally , we provide further evidence of the usefulness of these transformations by applying them in a separate task parser accuracy prediction .

pos tagging of dialectal arabic : a minimally supervised approach
natural language processing technology for the dialects of arabic is still in its infancy , due to the problem of obtaining large amounts of text data for spoken arabic . in this paper we describe the development of a part-of-speech ( pos ) tagger for egyptian colloquial arabic . we adopt a minimally supervised approach that only requires raw text data from several varieties of arabic and a morphological analyzer for modern standard arabic . no dialect-specific tools are used . we present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and , by a state-ofthe-art modern standard arabic tagger applied to egyptian arabic .

understanding and quantifying creativity in lexical composition
why do certain combinations of words such as disadvantageous peace or metal to the petal appeal to our minds as interesting expressions with a sense of creativity , while other phrases such as quiet teenager , or geometrical base not as much we present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original , interesting , and at times even artistic . we first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words , then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity .

binarizing syntax trees to improve syntax-based machine translation accuracy
we show that phrase structures in penn treebank style parses are not optimal for syntaxbased machine translation . we exploit a series of binarization methods to restructure the penn treebank style trees such that syntactified phrases smaller than penn treebank constituents can be acquired and exploited in translation . we find that by employing the em algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result .

hybrid grammars for discontinuous parsing
we introduce the concept of hybrid grammars , which are extensions of synchronous grammars , obtained by coupling of lexical elements . one part of a hybrid grammar generates linear structures , another generates hierarchical structures , and together they generate discontinuous structures . this formalizes and generalizes some existing mechanisms for dealing with discontinuous phrase structures and non-projective dependency structures . moreover , it allows us to separate the degree of discontinuity from the time complexity of parsing .

boosting automatic lexical acquisition with morphological information
in this paper we investigate the impact of morphological features on the task of automatically extending a dictionary . we approach the problem as a pattern classification task and compare the performance of several models in classifying nouns that are unknown to a broad coverage dictionary . we used a boosting classifier to compare the performance of models that use different sets of features . we show how adding simple morphological features to a model greatly improves the classification performance .

discovering commonsense entailment rules implicit in sentences
reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge , including expectations about the probable results of actions and the lexical entailments for many predicates . we describe initial work to acquire such a collection of conditional ( ifthen ) knowledge by exploiting presuppositional discourse patterns ( such as ones involving but , yet , and hoping to ) and abstracting the matched material into general rules .

towards identifying unresolved discussions in student online forums
automatic tools for analyzing student online discussions are highly desirable for providing better assistance and promoting discussion participation . this paper presents an approach for identifying student discussions with unresolved issues or unanswered questions . in order to handle highly incoherent data , we perform several data processing steps . we then apply a two-phase classification algorithm . first , we classify speech acts of individual messages to identify the roles that the messages play , such as question , issue raising , and answers . we then use the resulting speech acts as features for classifying discussion threads with unanswered questions or unresolved issues . we performed a preliminary analysis of the classifiers and the system shows an average f score of 0.76 in discussion thread classification .

word segmentation for urdu ocr system and emerging sciences
this paper presents a technique for word segmentation for the urdu ocr system . word segmentation or word tokenization is a preliminary task for urdu language processing . several techniques are available for word segmentation in other languages . a methodology is proposed for word segmentation in this paper which determines the boundaries of words given a sequence of ligatures , based on collocation of ligatures and words in the corpus . using this technique , word identification rate of 96.10 % is achieved , using trigram probabilities normalized over the number of ligatures and words in the sequence .

markov random topic fields
most approaches to topic modeling assume an independence between documents that is frequently violated . we present an topic model that makes use of one or more user-specified graphs describing relationships between documents . these graph are encoded in the form of a markov random field over topics and serve to encourage related documents to have similar topic structures . experiments on show upwards of a 10 % improvement in modeling performance .

learning from errors : using vector-based compositional semantics for
in this paper , we address the problem of how to use semantics to improve syntactic parsing , by using a hybrid reranking method : a k-best list generated by a symbolic parser is reranked based on parsecorrectness scores given by a compositional , connectionist classifier . this classifier uses a recursive neural network to construct vector representations for phrases in a candidate parse tree in order to classify it as syntactically correct or not . tested on the wsj23 , our method achieved a statistically significant improvement of 0.20 % on f-score ( 2 % error reduction ) and 0.95 % on exact match , compared with the state-ofthe-art berkeley parser . this result shows that vector-based compositional semantics can be usefully applied in syntactic parsing , and demonstrates the benefits of combining the symbolic and connectionist approaches .

an acg analysis of the g-tag generation process
this paper presents an encoding of generation-tag ( g-tag ) within abstract categorial grammars ( acg ) . we show how the key notions of g-tag have a natural interpretation in acg , allowing us to use its reversibility property for text generation . it also offers solutions to several limitations of g-tag .

adaptive recursive neural network for target-dependent twitter sentiment classification
we propose adaptive recursive neural network ( adarnn ) for target-dependent twitter sentiment classification . adarnn adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them . it consists of more than one composition functions , and we model the adaptive sentiment propagations as distributions over these composition functions . the experimental studies illustrate that adarnn improves the baseline methods . furthermore , we introduce a manually annotated dataset for target-dependent twitter sentiment analysis .

word alignment with synonym regularization
we present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model . synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language . we design a generative model for word alignment that uses synonym information as a regularization term . the experimental results show that our proposed method significantly improves word alignment quality .

, pedro bispo santos and torsten zesch
dkpro keyphrases is a keyphrase extraction framework based on uima . it offers a wide range of state-of-the-art keyphrase experiments approaches . at the same time , it is a workbench for developing new extraction approaches and evaluating their impact . dkpro keyphrases is publicly available under an open-source license .

relative clauses in hindi and arabic : a paninian dependency grammar analysis
we present a comparative analysis of relative clauses in hindi and arabic in the tradition of the paninian grammar framework ( bharati et al , 1996b ) which leads to deriving a common logical form for equivalent sentences . parallels are drawn between the hindi co-relative construction and resumptive pronouns in arabic . the analysis arises from the development of lexicalised dependency grammars for hindi and arabic that have application for machine translation .

lattice parsing to integrate speech recognition and rule-based machine
in this paper , we present a novel approach to integrate speech recognition and rulebased machine translation by lattice parsing . the presented approach is hybrid in two senses . first , it combines structural and statistical methods for language modeling task . second , it employs a chart parser which utilizes manually created syntax rules in addition to scores obtained after statistical processing during speech recognition . the employed chart parser is a unification-based active chart parser . it can parse word graphs by using a mixed strategy instead of being bottom-up or top-down only . the results are reported based on word error rate on the nist hub-1 word-lattices . the presented approach is implemented and compared with other syntactic language modeling techniques .

anja belz eric kow
the tuna-reg09 challenge was one of the shared-task evaluation competitions at generation challenges 2009. tunareg09 used data from the tuna corpus of paired representations of entities and human-authored referring expressions . the shared task was to create systems that generate referring expressions for entities given representations of sets of entities and their properties . four teams submitted six systems to tunareg09 . we evaluated the six systems and two sets of human-authored referring expressions using several automatic intrinsic measures , a human-assessed intrinsic evaluation and a human task performance experiment . this report describes the tunareg task and the evaluation methods used , and presents the evaluation results .

semi-automatic construction of korean-chinese verb patterns based on translation equivalency
this paper addresses a new method of constructing korean-chinese verb patterns from existing patterns . a verb pattern is a subcategorization frame of a predicate extended by translation information . korean-chinese verb patterns are invaluable linguistic resources that only used for korean-chinese transfer but also for korean parsing . usually a verb pattern has been either hand-coded by expert lexicographers or extracted automatically from bilingual corpus . in the first case , the dependence on the linguistic intuition of lexicographers may lead to the incompleteness and the inconsistency of a dictionary . in the second case , extracted patterns can be domain-dependent . in this paper , we present a method to construct koreanchinese verb patterns semiautomatically from existing koreanchinese verb patterns that are manually written by lexicographers .

smooth bilingual n-gram translation upc - talp
we address the problem of smoothing translation probabilities in a bilingual n-grambased statistical machine translation system . it is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation . a neural network is used to perform the projection and the probability estimation . smoothing probabilities is most important for tasks with a limited amount of training material . we consider here the btec task of the 2006 iwslt evaluation . improvements in all official automatic measures are reported when translating from italian to english . using a continuous space model for the translation model and the target language model , an improvement of 1.5 bleu on the test data is observed .

the problem of ontology alignment on the web : a first report davide fossati and gabriele ghidoni and barbara di eugenio and isabel cruz and huiyong xiao and rajen subba
this paper presents a general architecture and four algorithms that use natural language processing for automatic ontology matching . the proposed approach is purely instance based , i.e. , only the instance documents associated with the nodes of ontologies are taken into account . the four algorithms have been evaluated using real world test data , taken from the google and looksmart online directories . the results show that nlp techniques applied to instance documents help the system achieve higher performance .

applying the semantics of negation to smt through n-best list re-ranking
although the performance of smt systems has improved over a range of different linguistic phenomena , negation has not yet received adequate treatment . previous works have considered the problem of translating negative data as one of data sparsity ( wetzel and bond ( 2012 ) ) or of structural differences between source and target language with respect to the placement of negation ( collins et al . ( 2005 ) ) . this work starts instead from the questions ofwhat is meant by negation and what makes a good translation of negation . these questions have led us to explore the use of semantics of negation in smt specifically , identifying core semantic elements of negation ( cue , event and scope ) in a source-side dependency parse and reranking hypotheses on the n-best list produced after decoding according to the extent to which an hypothesis realises these elements . the method shows considerable improvement over the baseline as measured by bleu scores and stanfords entailmentbased mt evaluation metric ( pad et al . ( 2009 ) ) .

retrieving meaning-equivalent sentences for example-based rough translation mitsuo shimohata eiichiro sumita atr spoken language translation
example-based machine translation ( ebmt ) is a promising translation method for speechto-speech translation because of its robustness . it retrieves example sentences similar to the input and adjusts their translations to obtain the output . however , it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different . this paper proposes a method for retrieving meaning-equivalent sentences to overcome these two problems . a meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information . the translations of meaning-equivalent sentences correspond to rough translations . the retrieval is based on content words , modality , and tense .

improving automatic speech recognition for lectures through transformation-based rules learned from minimal data
we demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9 % . our method is distinguished from earlier related work by its robustness to small amounts of training data , and its resulting efficiency , in spite of its use of true word error rate computations as a rule scoring function .

strategies for contiguous multiword expression analysis and paris diderot univ
in this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( mwe ) recognition , testing them on the dependency version of french treebank ( abeille and barrier , 2004 ) , as instantiated in the spmrl shared task ( seddah et al , 2013 ) . our work focuses on using an alternative representation of syntactically regular mwes , which captures their syntactic internal structure . we obtain a system with comparable performance to that of previous works on this dataset , but which predicts both syntactic dependencies and the internal structure of mwes . this can be useful for capturing the various degrees of semantic compositionality of mwes .

shallow semantic parsing using support vector machines
in this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of gildea and jurafsky ( 2002 ) , surdeanu et al ( 2003 ) and others . our algorithm is based on support vector machines which we show give an improvement in performance over earlier classifiers . we show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the aquaint corpus .

a general-purpose rule extractor for scfg-based machine translation
we present a rule extractor for scfg-based mt that generalizes many of the contraints present in existing scfg extraction algorithms . our methods increased rule coverage comes from allowing multiple alignments , virtual nodes , and multiple tree decompositions in the extraction process . at decoding time , we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set , while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains .

head-driven parsing for word lattices
we present the first application of the head-driven statistical parsing model of collins ( 1999 ) as a simultaneous language model and parser for largevocabulary speech recognition . the model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities . the parser uses structural and lexical dependencies not considered by ngram models , conditioning recognition on more linguistically-grounded relationships . experiments on the wall street journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .

transfer learning based cross-lingual knowledge extraction for wikipedia
wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing . however , infobox information is very incomplete and imbalanced among the wikipedias in different languages . it is a promising but challenging problem to utilize the rich structured knowledge from a source language wikipedia to help complete the missing infoboxes for a target language . in this paper , we formulate the problem of cross-lingual knowledge extraction from multilingual wikipedia sources , and present a novel framework , called wikicike , to solve this problem . an instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors . our experimental results demonstrate that wikicike outperforms the monolingual knowledge extraction method and the translation-based method .

on some pitfalls in automatic evaluation and significance testing for mt
we investigate some pitfalls regarding the discriminatory power of mt evaluation metrics and the accuracy of statistical significance tests . in a discriminative reranking experiment for phrase-based smt we show that the nist metric is more sensitive than bleu or f-score despite their incorporation of aspects of fluency or meaning adequacy into mt evaluation . in an experimental comparison of two statistical significance tests we show that p-values are estimated more conservatively by approximate randomization than by bootstrap tests , thus increasing the likelihood of type-i error for the latter . we point out a pitfall of randomly assessing significance in multiple pairwise comparisons , and conclude with a recommendation to combine nist with approximate randomization , at more stringent rejection levels than is currently standard .

of treebank and bracketed corpora training
an approach to automatic detection of syllable boundaries is presented . we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training . we investigate the effect of the training corpus size on the performance of our system . the evaluation shows that a hand-written grammar performs better on finding syllable boundaries than does a treebank grammar .

an approach to take multi-word expressions
this research discusses preliminary efforts to expand the coverage of the propbank lexicon to multi-word and idiomatic expressions , such as take one for the team . given overwhelming numbers of such expressions , an efficient way for increasing coverage is needed . this research discusses an approach to adding multiword expressions to the propbank lexicon in an effective yet semantically rich fashion . the pilot discussed here uses double annotation of take multi-word expressions , where annotations provide information on the best strategy for adding the multi-word expression to the lexicon . this work represents an important step for enriching the semantic information included in the propbank corpus , which is a valuable and comprehensive resource for the field of natural language processing .

other-initiated self-repairs in estonian information dialogues : solving communication problems in cooperation
the paper gives an overview of repair sequences used in estonian spoken information dialogues . 62 calls for information , travel bureaus , shops or outpatients departments are analysed . several repair types are considered . our further aim is to develop a dialogue system which can interact with the user in estonian following the norms and rules of humanhuman communication

spatial descriptions as referring expressions in the maptask domain
we discuss work-in-progress on a hybrid approach to the generation of spatial descriptions , using the maps of the map task dialogue corpus as domain models . we treat spatial descriptions as referring expressions that distinguish particular points on the maps from all other points ( potential distractors ) . our approach is based on rule-based overgeneration of spatial descriptions combined with ranking which currently is based on explicit goodness criteria but will ultimately be corpus-based . ranking for content determination tasks such as referring expression generation raises a number of deep and vexing questions about the role of corpora in nlg , the kind of knowledge they can provide and how it is used .

feature selection for fluency ranking
fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical , but also fluent . it has been shown that feature-based models , such as maximum entropy models , work well for this task . since maximum entropy models allow for incorporation of arbitrary real-valued features , it is often attractive to create very general feature templates , that create a huge number of features . to select the most discriminative features , feature selection can be applied . in this paper we compare three feature selection methods : frequency-based selection , a generalization of maximum entropy feature selection for ranking tasks with realvalued features , and a new selection method based on feature value correlation . we show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection , and that models with a few hundred well-picked features are competitive to models with no feature selection applied . in the experiments described in this paper , we compressed a model of approximately 490.000 features to 1.000 features .

automatic treebank-based acquisition of arabic lfg dependency
a number of papers have reported on methods for the automatic acquisition of large-scale , probabilistic lfg-based grammatical resources from treebanks for english ( cahill and al. , 2002 ) , ( cahill and al. , 2004 ) , german ( cahill and al. , 2003 ) , chinese ( burke , 2004 ) , ( guo and al. , 2007 ) , spanish ( odonovan , 2004 ) , ( chrupala and van genabith , 2006 ) and french ( schluter and van genabith , 2008 ) . here , we extend the lfg grammar acquisition approach to arabic and the penn arabic treebank ( atb ) ( maamouri and bies , 2004 ) , adapting and extending the methodology of ( cahill and al. , 2004 ) originally developed for english . arabic is challenging because of its morphological richness and syntactic complexity . currently 98 % of atb trees ( without frag and x ) produce a covering and connected f-structure . we conduct a qualitative evaluation of our annotation against a gold standard and achieve an f-score of 95 % .

elixirfm implementation of functional arabic morphology
functional arabic morphology is a formulation of the arabic inflectional system seeking the working interface between morphology and syntax . elixirfm is its high-level implementation that reuses and extends the functional morphology library for haskell . inflection and derivation are modeled in terms of paradigms , grammatical categories , lexemes and word classes . the computation of analysis or generation is conceptually distinguished from the general-purpose linguistic model . the lexicon of elixirfm is designed with respect to abstraction , yet is no more complicated than printed dictionaries . it is derived from the open-source buckwalter lexicon and is enhanced with information sourcing from the syntactic annotations of the prague arabic dependency treebank .

on statistical parsing of french with supervised and semi-supervised strategies
this paper reports results on grammatical induction for french . we investigate how to best train a parser on the french treebank ( abeill et al , 2003 ) , viewing the task as a trade-off between generalizability and interpretability . we compare , for french , a supervised lexicalized parsing algorithm with a semi-supervised unlexicalized algorithm ( petrov et al , 2006 ) along the lines of ( crabb and candito , 2008 ) . we report the best results known to us on french statistical parsing , that we obtained with the semi-supervised learning algorithm . the reported experiments can give insights for the task of grammatical learning for a morphologically-rich language , with a relatively limited amount of training data , annotated with a rather flat structure .

computational complexity of statistical machine translation
in this paper we study a set of problems that are of considerable importance to statistical machine translation ( smt ) but which have not been addressed satisfactorily by the smt research community . over the last decade , a variety of smt algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of smt . our work aims at providing useful insights into the the computational complexity of those problems . we prove that while ibm models 1-2 are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard . since it is unlikely that there exists a polynomial time solution for any of these hard problems ( unless p = np and p # p = p ) , our results highlight and justify the need for developing polynomial time approximations for these computations . we also discuss some practical ways of dealing with complexity .

classifying arguments by scheme vanessa wei feng
argumentation schemes are structures or templates for various kinds of arguments . given the text of an argument with premises and conclusion identified , we classify it as an instance of one of five common schemes , using features specific to each scheme . we achieve accuracies of 6391 % in one-against-others classification and 8094 % in pairwise classification ( baseline = 50 % in both cases ) .

converting mikrokosmos frames into description logics
mikrokosmos contains an ontology plus a number of lexicons in different languages that were originally developed for machine translation . the underlying representation formalism for these resources is an ad-hoc frame-based language which makes it difficult to inter-operate mikrokosmos with state-ofthe-art knowledge-based systems . in this paper we propose a translation from the frame-based representation of mikrokosmos into description logics . this translation allows us to automatically transform mikrokosmos sources into owl and thus provide a powerful ontology in the formalism of the semantic web . furthermore , the reasoning mechanisms of description logics may also support knowledge acquisition and maintenance as well as its application in natural language processing systems .

data driven language transfer hypotheses
language transfer , the preferential second language behavior caused by similarities to the speakers native language , requires considerable expertise to be detected by humans alone . our goal in this work is to replace expert intervention by data-driven methods wherever possible . we define a computational methodology that produces a concise list of lexicalized syntactic patterns that are controlled for redundancy and ranked by relevancy to language transfer . we demonstrate the ability of our methodology to detect hundreds of such candidate patterns from currently available data sources , and validate the quality of the proposed patterns through classification experiments .

a study on convolution kernels for shallow semantic parsing
in this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments . their main property is the ability to process structured representations . support vector machines ( svms ) , using a combination of such kernels and the flat feature kernel , classify propbank predicate arguments with accuracy higher than the current argument classification stateof-the-art . additionally , experiments on framenet data have shown that svms are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement .

is question answering better than information retrieval towards a task-based evaluation framework for question series
this paper introduces a novel evaluation framework for question series and employs it to explore the effectiveness of qa and ir systems at addressing users information needs . the framework is based on the notion of recall curves , which characterize the amount of relevant information contained within a fixed-length text segment . although it is widely assumed that qa technology provides more efficient access to information than ir systems , our experiments show that a simple ir baseline is quite competitive . these results help us better understand the role of nlp technology in qa systems and suggest directions for future research .

hyena-live : fine-grained online entity type classification from
recent research has shown progress in achieving high-quality , very fine-grained type classification in hierarchical taxonomies . within such a multi-level type hierarchy with several hundreds of types at different levels , many entities naturally belong to multiple types . in order to achieve high-precision in type classification , current approaches are either limited to certain domains or require time consuming multistage computations . as a consequence , existing systems are incapable of performing ad-hoc type classification on arbitrary input texts . in this demo , we present a novel webbased tool that is able to perform domain independent entity type classification under real time conditions . thanks to its efficient implementation and compacted feature representation , the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations . our system offers an online interface where natural-language text can be inserted , which returns semantic type labels for entity mentions . further more , the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy .

a hybrid text classication approach for analysis of student essays
we present carmeltc , a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions , which builds upon work presented in ( rose et al , 2002a ) . carmeltc learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a naive bayes classification of that text . we explore the tradeoffs between symbolic and bag of words approaches . our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses . our evaluation demonstrates that the hybrid carmeltc approach outperforms two bag of words approaches , namely lsa and a naive bayes , as well as a purely symbolic approach .

the god model alfio massimiliano gliozzo
god ( general ontology discovery ) is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts . operationally , it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents . our approach relies on two basic assumptions : ( i ) paradigmatic relations can be established only among terms in the same semantic domain an ( ii ) they can be inferred from texts by analyzing the subject-verb-object patterns where two domain specific terms co-occur . a qualitative analysis of the system output shows that god provide true , informative and meaningful relations in a very efficient way .

a description of tunable machine translation evaluation systems in
this paper is to describe our machine translation evaluation systems used for participation in the wmt13 shared metrics task . in the metrics task , we submitted two automatic mt evaluation systems nlepor_baseline and lepor_v3.1 . nlepor_baseline is an n-gram based language independent mt evaluation metric employing the factors of modified sentence length penalty , position difference penalty , n-gram precision and n-gram recall . nlepor_baseline measures the similarity of the system output translations and the reference translations only on word sequences . lepor_v3.1 is a new version of lepor metric using the mathematical harmonic mean to group the factors and employing some linguistic features , such as the part-of-speech information . the evaluation results of wmt13 show lepor_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using pearson correlation criterion on english-to-other ( fr , de , es , cs , ru ) language pairs .

towards a road map on human language technology : natural language processing editors : andreas eisele , dorothea ziegler-eisele
this document summarizes contributions and discussions from two workshops that took place in november 2000 and july 2001. it presents some visions of nlp-related applications that may become reality within ten years from now . it investigates the technological requirements that must be met in order to make these visions realistic and sketches milestones that may help to measure our progress towards these goals .

computing translation units and quantifying parallelism in parallel dependency treebanks
the linguistic quality of a parallel treebank depends crucially on the parallelism between the source and target language annotations . we propose a linguistic notion of translation units and a quantitative measure of parallelism for parallel dependency treebanks , and demonstrate how the proposed translation units and parallelism measure can be used to compute transfer rules , spot annotation errors , and compare different annotation schemes with respect to each other . the proposal is evaluated on the 100,000 word copenhagen danish-english dependency treebank .

how creative is your writing a linguistic creativity measure from computer science and cognitive psychology perspectives
we demonstrate that subjective creativity in sentence-writing can in part be predicted using computable quantities studied in computer science and cognitive psychology . we introduce a task in which a writer is asked to compose a sentence given a keyword . the sentence is then assigned a subjective creativity score by human judges . we build a linear regression model which , given the keyword and the sentence , predicts the creativity score . the model employs features on statistical language models from a large corpus , psychological word norms , and wordnet .

a lexicon for emotion analysis from crowd-annotated news trento - italy trento - italy
while many lexica annotated with words polarity are available for sentiment analysis , very few tackle the harder task of emotion analysis and are usually quite limited in coverage . in this paper , we present a novel approach for extracting in a totally automated way a highcoverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores , called depechemood . our approach exploits in an original way crowd-sourced affective annotation implicitly provided by readers of news articles from rappler.com . by providing new state-of-the-art performances in unsupervised settings for regression and classification tasks , even using a nave approach , our experiments show the beneficial impact of harvesting social media data for affective lexicon building .

a probabilistic rasch analysis of question answering evaluations integrated knowledge systems the mitre corporation the mitre corporation
the field of psychometrics routinely grapples with the question of what it means to measure the inherent ability of an organism to perform a given task , and for the last forty years , the field has increasingly relied on probabilistic methods such as the rasch model for test construction and the analysis of test results . because the underlying issues of measuring ability apply to human language technologies as well , such probabilistic methods can be advantageously applied to the evaluation of those technologies . to test this claim , rasch measurement was applied to the results of 67 systems participating in the question answering track of the 2002 text retrieval conference ( trec ) competition . satisfactory model fit was obtained , and the paper illustrates the theoretical and practical strengths of rasch scaling for evaluating systems as well as questions . most important , simulations indicate that a test invariant metric can be defined by carrying forward 20 to 50 equating questions , thus placing the yearly results on a common scale .

eacl - expansion of abbreviations in clinical text and maria kvist
in the medical domain , especially in clinical texts , non-standard abbreviations are prevalent , which impairs readability for patients . to ease the understanding of the physicians notes , abbreviations need to be identified and expanded to their original forms . we present a distributional semantic approach to find candidates of the original form of the abbreviation , and combine this with levenshtein distance to choose the correct candidate among the semantically related words . we apply the method to radiology reports and medical journal texts , and compare the results to general swedish . the results show that the correct expansion of the abbreviation can be found in 40 % of the cases , an improvement by 24 percentage points compared to the baseline ( 0.16 ) , and an increase by 22 percentage points compared to using word space models alone ( 0.18 ) .

semi-supervised learning for relation extraction
this paper proposes a semi-supervised learning method for relation extraction . given a small amount of labeled data and a large amount of unlabeled data , it first bootstraps a moderate number of weighted support vectors via svm through a co-training procedure with random feature projection and then applies a label propagation ( lp ) algorithm via the bootstrapped support vectors . evaluation on the ace rdc 2003 corpus shows that our method outperforms the normal lp algorithm via all the available labeled data without svm bootstrapping . moreover , our method can largely reduce the computational burden . this suggests that our proposed method can integrate the advantages of both svm bootstrapping and label propagation .

computational metaphor identification univ of california , irvine univ of california , irvine univ of california , irvine
most computational approaches to metaphor have focused on discerning between metaphorical and literal text . recent work on computational metaphor identification ( cmi ) instead seeks to identify overarching conceptual metaphors by mapping selectional preferences between source and target corpora . this paper explores using semantic role labeling ( srl ) in cmi . its goals are two-fold : first , to demonstrate that semantic roles can effectively be used to identify conceptual metaphors , and second , to compare srl to the current use of typed dependency parsing in cmi . the results show that srl can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies , but also that srl introduces its own set of complications . the paper concludes by suggesting future directions , both for evaluating the use of srl in cmi , and for fostering critical and creative thinking about metaphors .

human evaluation of a german surface realisation ranker
in this paper we present a human-based evaluation of surface realisation alternatives . we examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models ( language model , loglinear model ) , as well as the naturalness of the strings chosen by the log-linear model . we also investigate to what extent preceding context has an effect on choice . we show that native speakers do accept quite some variation in word order , but there are also clearly factors that make certain realisation alternatives more natural .

smt helps bitext dependency parsing
we propose a method to improve the accuracy of parsing bilingual texts ( bitexts ) with the help of statistical machine translation ( smt ) systems . previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain . instead , our approach uses an auto-generated bilingual treebank to produce bilingual constraints . however , because the auto-generated bilingual treebank contains errors , the bilingual constraints are noisy . to overcome this problem , we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results . the experimental results show that our new parsers significantly outperform state-of-theart baselines . moreover , our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline . especially notable is that our approach can be used in a purely monolingual setting with the help of smt .

a hybrid approach to skeleton-based translation
in this paper we explicitly consider sentence skeleton information for machine translation ( mt ) . the basic idea is that we translate the key elements of the input sentence using a skeleton translation model , and then cover the remain segments using a full translation model . we apply our approach to a state-of-the-art phrase-based system and demonstrate very promising bleu improvements and ter reductions on the nist chinese-english mt evaluation data .

linguastream : an integrated environment for computational linguistics experimentation
by presenting the linguastream platform , we introduce different methodological principles and analysis models , which make it possible to build hybrid experimental nlp systems by articulating corpus processing tasks .

the non-randomness problem in word frequency distribution modeling
frequency distribution models tuned to words and other linguistic events can predict the number of distinct types and their frequency distribution in samples of arbitrary sizes . we conduct , for the first time , a rigorous evaluation of these models based on cross-validation and separation of training and test data . our experiments reveal that the prediction accuracy of the models is marred by serious overfitting problems , due to violations of the random sampling assumption in corpus data . we then propose a simple pre-processing method to alleviate such non-randomness problems . further evaluation confirms the effectiveness of the method , which compares favourably to more complex correction techniques .

exploiting the human computational effort dedicated to message reply formatting for training discursive email segmenters nicolas hernandez soufian salim
in the context of multi-domain and multimodal online asynchronous discussion analysis , we propose an innovative strategy for manual annotation of dialog act ( da ) segments . the process aims at supporting the analysis of messages in terms of da . our objective is to train a sequence labelling system to detect the segment boundaries . the originality of the proposed approach is to avoid manually annotating the training data and instead exploit the human computational efforts dedicated to message reply formatting when the writer replies to a message by inserting his response just after the quoted text appropriate to his intervention . we describe the approach , propose a new electronic mail corpus and report the evaluation of segmentation models we built .

a discriminative matching approach to word alignment
we present a discriminative , largemargin approach to feature-based matching for word alignment . in this framework , pairs of word tokens receive a matching score , which is based on features of that pair , including measures of association between the words , distortion between their positions , similarity of the orthographic form , and so on . even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus , we achieve aer performance close to ibm model 4 , in much less time . including model 4 predictions as features , we achieve a relative aer reduction of 22 % in over intersected model 4 alignments .

pos-tagger for english-vietnamese bilingual corpus
corpus-based natural language processing ( nlp ) tasks for such popular languages as english , french , etc . have been well studied with satisfactory achievements . in contrast , corpus-based nlp tasks for unpopular languages ( e.g . vietnamese ) are at a deadlock due to absence of annotated training data for these languages . furthermore , hand-annotation of even reasonably well-determined features such as part-ofspeech ( pos ) tags has proved to be labor intensive and costly . in this paper , we suggest a solution to partially overcome the annotated resource shortage in vietnamese by building a pos-tagger for an automatically word-aligned english-vietnamese parallel corpus ( named evc ) . this pos-tagger made use of the transformation-based learning ( or tbl ) method to bootstrap the pos-annotation results of the english pos-tagger by exploiting the pos-information of the corresponding vietnamese words via their wordalignments in evc . then , we directly project posannotations from english side to vietnamese via available word alignments . this pos-annotated vietnamese corpus will be manually corrected to become an annotated training data for vietnamese nlp tasks such as pos-tagger , phrase-chunker , parser , word-sense disambiguator , etc .

exploiting named entity taggers in a second language
in this work we present a method for named entity recognition ( ner ) . our method does not rely on complex linguistic resources , and apart from a hand coded system , we do not use any languagedependent tools . the only information we use is automatically extracted from the documents , without human intervention . moreover , the method performs well even without the use of the hand coded system . the experimental results are very encouraging . our approach even outperformed the hand coded system on ner in spanish , and it achieved high accuracies in portuguese .

word alignment and cross-lingual resource acquisition
annotated corpora are valuable resources for developing natural language processing applications . this work focuses on acquiring annotated data for multilingual processing applications . we present an annotation environment that supports a web-based user-interface for acquiring word alignments between english and chinese as well as a visualization tool for researchers to explore the annotated data .

discriminative lexicon adaptation for improved character accuracy a new direction in chinese language modeling
while oov is always a problem for most languages in asr , in the chinese case the problem can be avoided by utilizing character n-grams and moderate performances can be obtained . however , character ngram has its own limitation and proper addition of new words can increase the asr performance . here we propose a discriminative lexicon adaptation approach for improved character accuracy , which not only adds new words but also deletes some words from the current lexicon . different from other lexicon adaptation approaches , we consider the acoustic features and make our lexicon adaptation criterion consistent with that in the decoding process . the proposed approach not only improves the asr character accuracy but also significantly enhances the performance of a characterbased spoken document retrieval system .

global learning of typed entailment rules
extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference . in this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates . we model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs . we apply the algorithm over a large data set of extracted predicate instances , from which a resource of typed entailment rules has been recently released ( schoenmackers et al , 2010 ) . our results show that using global transitivity information substantially improves performance over this resource and several baselines , and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs .

realization of discourse relations by other means : alternative
studies of discourse relations have not , in the past , attempted to characterize what serves as evidence for them , beyond lists of frozen expressions , or markers , drawn from a few well-defined syntactic classes . in this paper , we describe how the lexicalized discourse relation annotations of the penn discourse treebank ( pdtb ) led to the discovery of a wide range of additional expressions , annotated as altlex ( alternative lexicalizations ) in the pdtb 2.0. further analysis of altlex annotation suggests that the set of markers is openended , and drawn from a wider variety of syntactic types than currently assumed . as a first attempt towards automatically identifying discourse relation markers , we propose the use of syntactic paraphrase methods .

online large-margin training of dependency parsers ryan mcdonald koby crammer fernando pereira
we present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training ( crammer and singer , 2003 ; crammer et al , 2003 ) on top of efficient parsing techniques for dependency trees ( eisner , 1996 ) . the trained parsers achieve a competitive dependency accuracy for both english and czech with no language specific enhancements .

training a naive bayes classifier via the em algorithm with a class crest , jst ( japan science and technology corporation )
combining a naive bayes classifier with the em algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction . however , the use of unlabeled data via the basic em algorithm often causes disastrous performance degradation instead of improving classification performance , resulting in poor classification performance on average . in this study , we introduce a class distribution constraint into the iteration process of the em algorithm . this constraint keeps the class distribution of unlabeled data consistent with the class distribution estimated from labeled data , preventing the em algorithm from converging into an undesirable state . experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small .

modeling and predicting quality in spoken human-computer interaction
in this work we describe the modeling and prediction of interaction quality ( iq ) in spoken dialogue systems ( sds ) using support vector machines . the model can be employed to estimate the quality of the ongoing interaction at arbitrary points in a spoken humancomputer interaction . we show that the use of 52 completely automatic features characterizing the system-user exchange significantly outperforms state-of-the-art approaches . the model is evaluated on publically available data from the cmu lets go bus information system . it reaches a performance of 61.6 % unweighted average recall when discriminating between 5 classes ( good to very poor ) . it can be further shown that incorporating knowledge about the users emotional state does hardly improve the performance .

the effect of wording on message propagation : topic- and author-controlled natural experiments on twitter
consider a person trying to spread an important message on a social network . he/she can spend hours trying to craft the message . does it actually matter while there has been extensive prior work looking into predicting popularity of socialmedia content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic . to control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording . given such pairs , we ask : which version attracts more retweets this turns out to be a more difficult task than predicting popular topics . still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on noncontrolled data .

jointly learning to parse and perceive : connecting natural language to the physical world
this paper introduces logical semantics with perception ( lsp ) , a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment . for example , given an image , lsp can map the statement blue mug on the table to the set of image segments showing blue mugs on tables . lsp learns physical representations for both categorical ( blue , mug ) and relational ( on ) language , and also learns to compose these representations to produce the referents of entire statements . we further introduce a weakly supervised training procedure that estimates lsps parameters using annotated referents for entire statements , without annotated referents for individual words or the parse structure of the statement . we perform experiments on two applications : scene understanding and geographical question answering . we find that lsp outperforms existing , less expressive models that can not represent relational language . we further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort .

unsupervised methods of topical text segmentation for polish
this paper describes a study on performance of existing unsupervised algorithms of text documents topical segmentation when applied to polish plain text documents . for performance measurement five existing topical segmentation algorithms were selected , three different polish test collections were created and seven approaches to text preprocessing were implemented . based on quantitative results ( pk and windowdiff metrics ) use of specific algorithm was recommended and impact of pre-processing strategies was assessed . thanks to use of standardized metrics and application of previously described methodology for test collection development , comparative results for polish and english were also obtained .

combination of statistical word alignments based on multiple preprocessing schemes
we present an approach to using multiple preprocessing schemes to improve statistical word alignments . we show a relative reduction of alignment error rate of about 38 % .

generation under uncertainty
we invite the research community to consider challenges for nlg which arise from uncertainty . nlg systems should be able to adapt to their audience and the generation environment in general , but often the important features for adaptation are not known precisely . we explore generation challenges which could employ simulated environments to study nlgwhich is adaptive under uncertainty , and suggest possible metrics for such tasks . it would be particularly interesting to explore how different planning approaches to nlg perform in challenges involving uncertainty in the generation environment .

investigating the relationship between word segmentation performance and retrieval performance in chinese ir
it is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in chinese information retrieval . in this paper we show that , for chinese , the relationship between segmentation and retrieval performance is in fact nonmonotonic ; that is , at around 70 % word segmentation accuracy an over-segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance . we demonstrate this effect by presenting an empirical investigation of information retrieval on chinese trec data , using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44 % to 95 % . it appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters , while they are broken up by less accurate ( but reasonable ) segmenters , to a surprising advantage . this suggests that words themselves might be too broad a notion to conveniently capture the general semantic meaning of chinese text .

improved word alignment using a symmetric lexicon model
word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing . we improve the well-known ibm alignment models , as well as the hidden-markov alignment model using a symmetric lexicon model . this symmetrization takes not only the standard translation direction from source to target into account , but also the inverse translation direction from target to source . we present a theoretically sound derivation of these techniques . in addition to the symmetrization , we introduce a smoothed lexicon model . the standard lexicon model is based on full-form words only . we propose a lexicon smoothing method that takes the word base forms explicitly into account . therefore , it is especially useful for highly inflected languages such as german . we evaluate these methods on the germanenglish verbmobil task and the frenchenglish canadian hansards task . we show statistically significant improvements of the alignment quality compared to the best system reported so far .

a task-based comparison of information extraction pattern models
several recent approaches to information extraction ( ie ) have used dependency trees as the basis for an extraction pattern representation . these approaches have used a variety of pattern models ( schemes which define the parts of the dependency tree which can be used to form extraction patterns ) . previous comparisons of these pattern models are limited by the fact that they have used indirect tasks to evaluate each model . this limitation is addressed here in an experiment which compares four pattern models using an unsupervised learning algorithm and a standard ie scenario . it is found that there is a wide variation between the models performance and suggests that one model is the most useful for ie .

an error analysis tool for natural language processing and applied
in this paper we present a simple to use web based error analysis tool to help computational linguists , researchers building language applications , and non-technical personnel managing development of language tools to analyze the predictions made by their machine learning models . the only expectation is that the users of the tool convert their data into an intuitive xml format . once the xml is ready , several error analysis functionalities that promote principled feature engineering are a click away .

infrastructure for standardization of asian language resources
as an area of great linguistic and cultural diversity , asian language resources have received much less attention than their western counterparts . creating a common standard for asian language resources that is compatible with an international standard has at least three strong advantages : to increase the competitive edge of asian countries , to bring asian countries to closer to their western counterparts , and to bring more cohesion among asian countries . to achieve this goal , we have launched a two year project to create a common standard for asian language resources . the project is comprised of four research items , ( 1 ) building a description framework of lexical entries , ( 2 ) building sample lexicons , ( 3 ) building an upperlayer ontology and ( 4 ) evaluating the proposed framework through an application . this paper outlines the project in terms of its aim and approach .

consentcanvas : automatic texturing for improved readability
we present consentcanvas , a system which structures and texturizes end-user license agreement ( eula ) documents to be more readable . the system aims to help users better understand the terms under which they are providing their informed consent . consentcanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet . unlike similar usable security projects which employ summarization techniques , our system preserves the contents of the source document , minimizing the cognitive and legal burden for both the end user and the licensor . our system does not require a corpus for training .

incremental ltag parsing
we present a very efficient statistical incremental parser for ltag-spinal , a variant of ltag . the parser supports the full adjoining operation , dynamic predicate coordination , and non-projective dependencies , with a formalism of provably stronger generative capacity as compared to cfg . using gold standard pos tags as input , on section 23 of the ptb , the parser achieves an f-score of 89.3 % for syntactic dependency defined on ltag derivation trees , which are deeper than the dependencies extracted from ptb alone with head rules ( for example , in magermans style ) .

prototype machine translation system from text-to-indian sign
this paper presents a prototype text-toindian sign language ( isl ) translation system . the system will help dissemination of information to the deaf people in india . the current system takes english sentence as input , performs syntactic analysis , and generates the corresponding isl structure . since isl does not have any written form , the output is represented in terms of prerecorded video streams . the system uses lexical functional grammar ( lfg ) formalism for representing isl syntax .

discotk : using discourse structure for machine translation evaluation shafiq joty francisco guzm an llus m
we present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference . we experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory , and we combine the kernel scores for each of them into a single score . finally , we add other metrics from the asiya mt evaluation toolkit , and we tune the weights of the combination on actual human judgments . experiments on the wmt12 and wmt13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved , both at the segment and at the system level .

projecting parameters for multilingual word sense disambiguation
we report in this paper a way of doing word sense disambiguation ( wsd ) that has its origin in multilingual mt and that is cognizant of the fact that parallel corpora , wordnets and sense annotated corpora are scarce resources . with respect to these resources , languages show different levels of readiness ; however a more resource fortunate language can help a less resource fortunate language . our wsd method can be applied to a language even when no sense tagged corpora for that language is available . this is achieved by projecting wordnet and corpus parameters from another language to the language in question . the approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of senses remains more or less invariant across languages . the effectiveness of our approach is verified by doing parameter projection and then running two different wsd algorithms . the accuracy values of approximately 75 % ( f1-score ) for three languages in two different domains establish the fact that within a domain it is possible to circumvent the problem of scarcity of resources by projecting parameters like sense distributions , corpus-co-occurrences , conceptual distance , etc . from one language to another .

ollie : on-line learning for information extraction
this paper reports work aimed at developing an open , distributed learning environment , ollie , where researchers can experiment with different machine learning ( ml ) methods for information extraction . once the required level of performance is reached , the ml algorithms can be used to speed up the manual annotation process . ollie uses a browser client while data storage and ml training is performed on servers . the different ml algorithms use a unified programming interface ; the integration of new ones is straightforward .

finding variants of out-of-vocabulary words in arabic
transliteration of a word into another language often leads to multiple spellings . unless an information retrieval system recognises different forms of transliterated words , a significant number of documents will be missed when users specify only one spelling variant . using two different datasets , we evaluate several approaches to finding variants of foreign words in arabic , and show that the longest common subsequence ( lcs ) technique is the best overall .

an ontology-based view on prepositional senses
this paper describes ongoing work , aimed at producing a lexicon of prepositions , i.e . relations denoted by prepositions , to be used for information retrieval purposes . the work is ontology based , which for this project means that the ontological types of the arguments of the preposition are considered , rather than the word forms . thus , sense distinctions are made based on ontological constraints on the arguments .

shared task : statistical machine translation between european languages
the acl-2005 workshop on parallel texts hosted a shared task on building statistical machine translation systems for four european language pairs : frenchenglish , germanenglish , spanishenglish , and finnishenglish . eleven groups participated in the event . this paper describes the goals , the task definition and resources , as well as results and some analysis . statistical machine translation is currently the dominant paradigm in machine translation research . annual competitions are held for chineseenglish and arabicenglish by nist , which creates a forum to present and compare novel ideas and leads to steady progress in the field . one of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent . building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text . it is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on . we effectively demonstrated this with our shared task . for instance , seven teams built finnishenglish machine translation systems , a language pair that was certainly not of their immediate concern before .

learning field compatibilities to extract database records from unstructured text
named-entity recognition systems extract entities such as people , organizations , and locations from unstructured text . rather than extract these mentions in isolation , this paper presents a record extraction system that assembles mentions into records ( i.e . database tuples ) . we construct a probabilistic model of the compatibility between field values , then employ graph partitioning algorithms to cluster fields into cohesive records . we also investigate compatibility functions over sets of fields , rather than simply pairs of fields , to examine how higher representational power can impact performance . we apply our techniques to the task of extracting contact records from faculty and student homepages , demonstrating a 53 % error reduction over baseline approaches .

a joint statistical model for simultaneous word spacing and spelling error correction for korean
this paper presents noisy-channel based korean preprocessor system , which corrects word spacing and typographical errors . the proposed algorithm corrects both errors simultaneously . using eojeol transition pattern dictionary and statistical data such as eumjeol n-gram and jaso transition probabilities , the algorithm minimizes the usage of huge word dictionaries .

constraints in language processing : do grammars count
one of the central assumptions of optimality theory is the hypothesis of strict domination among constraints . a few studies have suggested that this hypothesis is too strong and should be abandoned in favor of a weaker cumulativity hypothesis . if this suggestion is correct , we should be able to find evidence for cumulativity in the comprehension of gapping sentences , which lack explicit syntactic clues in the form of the presence of a finite verb . on the basis of a comparison between several computational models of constraint evaluation , we conclude that the comprehension of gapping sentences does not yield compelling evidence against the strict domination hypothesis .

the parallel grammar project
we report on the parallel grammar ( pargram ) project which uses the xle parser and grammar development platform for six languages : english , french , german , japanese , norwegian , and urdu.1

automatic diacritization of arabic for acoustic modeling in
automatic recognition of arabic dialectal speech is a challenging task because arabic dialects are essentially spoken varieties . only few dialectal resources are available to date ; moreover , most available acoustic data collections are transcribed without diacritics . such a transcription omits essential pronunciation information about a word , such as short vowels . in this paper we investigate various procedures that enable us to use such training data by automatically inserting the missing diacritics into the transcription . these procedures use acoustic information in combination with different levels of morphological and contextual constraints . we evaluate their performance against manually diacritized transcriptions . in addition , we demonstrate the effect of their accuracy on the recognition performance of acoustic models trained on automatically diacritized training data .

livetree : an integrated workbench for discourse processing
in this paper , we introduce livetree , a core component of lidas , the linguistic discourse analysis system for automatic discourse parsing with the unified linguistic discourse model ( u-ldm ) ( x et al 2004 ) . livetree is an integrated workbench for supervised and unsupervised creation , storage and manipulation of the discourse structure of text documents under the u-ldm . the livetree environment provides tools for manual and automatic u-ldm segmentation and discourse parsing . document management , grammar testing , manipulation of discourse structures and creation and editing of discourse relations are also supported .

detecting complex predicates in hindi using pos projection across parallel corpora achla m raina
complex predicates or cps are multiword complexes functioning as single verbal units . cps are particularly pervasive in hindi and other indoaryan languages , but an usage account driven by corpus-based identification of these constructs has not been possible since single-language systems based on rules and statistical approaches require reliable tools ( pos taggers , parsers , etc . ) that are unavailable for hindi . this paper highlights the development of first such database based on the simple idea of projecting pos tags across an english-hindi parallel corpus . the cp types considered include adjective-verb ( av ) , noun-verb ( nv ) , adverb-verb ( adv-v ) , and verb-verb ( vv ) composites . cps are hypothesized where a verb in english is projected onto a multi-word sequence in hindi . while this process misses some cps , those that are detected appear to be more reliable ( 83 % precision , 46 % recall ) . the resulting database lists usage instances of 1439 cps in 4400 sentences .

desparately seeking cebuano
this paper describes an effort to rapidly develop language resources and component technology to support searching cebuano news stories using english queries . results from the first 60 hours of the exercise are presented .

a corpus-based approach to topic in danish dialog
we report on an investigation of the pragmatic category of topic in danish dialog and its correlation to surface features of nps . using a corpus of 444 utterances , we trained a decision tree system on 16 features . the system achieved nearhuman performance with success rates of 8489 % and f1-scores of 0.630.72 in 10fold cross validation tests ( human performance : 89 % and 0.78 ) . the most important features turned out to be preverbal position , definiteness , pronominalisation , and non-subordination . we discovered that nps in epistemic matrix clauses ( e.g . i think . . . ) were seldom topics and we suspect that this holds for other interpersonal matrix clauses as well .

metaphor detection through term relevance spoken language systems
most computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selectional preferences . both require extensive knowledge of the mappings/preferences in question , as well as sufficient data for all involved conceptual domains . creating these resources is expensive and often limits the scope of these systems . we propose a statistical approach to metaphor detection that utilizes the rarity of novel metaphors , marking words that do not match a texts typical vocabulary as metaphor candidates . no knowledge of semantic concepts or the metaphors source domain is required . we analyze the performance of this approach as a stand-alone classifier and as a feature in a machine learning model , reporting improvements in f 1 measure over a random baseline of 58 % and 68 % , respectively . we also observe that , as a feature , it appears to be particularly useful when data is sparse , while its effect diminishes as the amount of training data increases .

exploiting semantic relation , context , and association features chikara hashimoto kentaro torisawa julien kloetzer motoki sano istvan varga jong-hoon oh yutaka kidawara
we propose a supervised method of extracting event causalities like conduct slash-and-burn agricultureexacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features . experiments show that our method outperforms baselines that are based on state-of-the-art methods . we also propose methods of generating future scenarios like conduct slash-and-burn agricultureexacerbate desertificationincrease asian dust ( from china ) asthma gets worse . experiments show that we can generate 50,000 scenarios with 68 % precision . we also generated a scenario deforestation continuesglobal warming worsenssea temperatures risevibrio parahaemolyticus fouls ( water ) , which is written in no document in our input web corpus crawled in 2007. but the vibrio risk due to global warming was observed in baker-austin et al ( 2013 ) . thus , we predicted the future event sequence in a sense .

urdu localization project : lexicon , mt and tts ( ulp )
pakistan has a population of 140 million speaking more than 56 different languages . urdu is the lingua franca of these people , as many speak urdu as a second language , also the national language of pakistan . being a developing population , pakistani people need access to information . most of the information over the ict infrastructure is only available in english and only 5-10 % of these people are familiar with english . therefore , government of pakistan has embarked on a project which will generate software to automatically translate the information available in english to urdu . the project will also be able to convert urdu text to speech to extend this information to the illiterate population as well . this paper overviews the overall architecture of the project and provides briefs on the three components of this project , namely urdu lexicon , english to urdu machine translation system and urdu text to speech system .

a cross-corpus study of unsupervised subjectivity identification based on calibrated
in this study we investigate using an unsupervised generative learning method for subjectivity detection in text across different domains . we create an initial training set using simple lexicon information , and then evaluate a calibrated em ( expectation-maximization ) method to learn from unannotated data . we evaluate this unsupervised learning approach on three different domains : movie data , news resource , and meeting dialogues . we also perform a thorough analysis to examine impacting factors on unsupervised learning , such as the size and self-labeling accuracy of the initial training set . our experiments and analysis show inherent differences across domains and performance gain from calibration in em .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

paving the way to a large-scale pseudosense-annotated dataset
in this paper we propose a new approach to the generation of pseudowords , i.e. , artificial words which model real polysemous words . our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets : semantic awareness and coverage . we evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts .

interactive exploration of asynchronous conversations : applying a
exploring an online conversation can be very difficult for a user , especially when it becomes a long complex thread . we follow a human-centered design approach to tightly integrate text mining methods with interactive visualization techniques to support the users in fulfilling their information needs . the resulting visual text analytic system provides multifaceted exploration of asynchronous conversations . we discuss a number of open challenges and possible directions for further improvement including the integration of interactive human feedback in the text mining loop , applying more advanced text analysis methods with visualization techniques , and evaluating the system with real users .

effect of word complexity on l2 vocabulary learning
research has shown that a number of factors , such as maturational constraints , previous language background , and attention , can have an effect on l2 acquisition . one related issue that remains to be explored is what factors make an individual word more easily learned . in this study we propose that word complexity , on both the phonetic and semantic levels , affect l2 vocabulary learning . two studies showed that words with simple grapheme-to-phoneme ratios were easier to learn than more phonetically complex words , and that words with two or fewer word senses were easier to learn that those with three or more .

how to change a persons mind : understanding the difference between the effects and consequences of speech acts
this paper discusses a planner of the semantics of utterances , whose essential design is an epistemic theorem prover . the planner was designed for the purpose of planning communicative actions , whose effects are famously unknowable and unobservable by the doer/speaker , and depend on the beliefs of and inferences made by the recipient/hearer . the fully implemented model can achieve goals that do not match action effects , but that are rather entailed by them , which it does by reasoning about how to act : state-space planning is interwoven with theorem proving in such a way that a theorem prover uses the effects of actions as hypotheses . the planner is able to model problematic conversational situations , including felicitous and infelicitous instances of bluffing , lying , sarcasm , and stating the obvious .

improving pronoun resolution by incorporating coreferential information of candidates
coreferential information of a candidate , such as the properties of its antecedents , is important for pronoun resolution because it reflects the salience of the candidate in the local discourse . such information , however , is usually ignored in previous learning-based systems . in this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution . preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates . we further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module . the experimental results show that our model still achieves better performance than the baseline .

recognition of sentiment sequences in online discussions
currently 19 % -28 % of internet users participate in online health discussions . in this work , we study sentiments expressed on online medical forums . as well as considering the predominant sentiments expressed in individual posts , we analyze sequences of sentiments in online discussions . individual posts are classified into one of the five categories encouragement , gratitude , confusion , facts , and endorsement . 1438 messages from 130 threads were annotated manually by two annotators with a strong inter-annotator agreement ( fleiss kappa = 0.737 and 0.763 for posts in sequence and separate posts respectively ) . the annotated posts were used to analyse sentiments in consecutive posts . in automated sentiment classification , we applied healthaffect , a domain-specific lexicon of affective words .

licensing complex prepositions via lexical constraints
in this paper , we will investigate a cross-linguistic phenomenon referred to as complex prepositions ( cps ) , which is a frequent type of multiword expressions ( mwes ) in many languages . based on empirical data , we will point out the problems of the traditional treatment of cps as complex lexical categories , and , thus , propose an analysis using the formal paradigm of the hpsg in the tradition of ( pollard and sag , 1994 ) . our objective is to provide an approach to cps which ( 1 ) convincingly explains empirical data , ( 2 ) is consistent with the underlying formal framework and does not require any extensions or modifications of the existing description apparatus , ( 3 ) is computationally tractable .

opinion mining and topic categorization with novel term weighting
in this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and internet . we compare the novel term weighting technique with existing approaches such as tf-idf and confweight . the performance on the data from the text-mining campaigns deft07 and deft08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster . the proposed text preprocessing method can be applied in large-scale information retrieval and data mining problems and it can be easily transported to different domains and different languages since it does not require any domain-related or linguistic information .

inducing lexico-structural transfer rules from parsed bi-texts
this paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching , statistical cooccurrence and error-driven filtering . we present initial evaluation results and discuss future directions .

improving unsupervised dependency parsing with richer contexts and smoothing
unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts . traditionally , the unsupervised models have been kept simple due to tractability and data sparsity concerns . in this paper , we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing . our model produces state-of-theart results on the task of unsupervised grammar induction , improving over the best previous work by almost 10 percentage points .

identification of relevant terms to support the construction of domain + roberto basili
though the utility of domain ontologies is now widely acknowledged in the it ( information technology ) community , several barriers must be overcome before ontologies become practical and useful tools . one important achievement would be to reduce the cost of identifying and manually entering several thousand-concept descriptions . this paper describes a text mining technique to aid an ontology engineer to identify the important concepts in a domain ontology .

investigation in statistical language-independent approaches for opinion detection in english , chinese and japanese olena zubaryeva jacques savoy
in this paper we present a new statistical approach to opinion detection and its evaluation on the english , chinese and japanese corpora . besides , the proposed method is compared with three baselines , namely nave bayes classifier , a language model and an approach based on significant collocations . these models being language independent are improved with the use of language-dependent technique on the example of the english corpus . we show that our method almost always gives better performance compared to the considered baselines .

re-estimation of lexical parameters for treebank pcfgs
we present procedures which pool lexical information estimated from unlabeled data via the inside-outside algorithm , with lexical information from a treebank pcfg . the procedures produce substantial improvements ( up to 31.6 % error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed penn treebank-trained pcfg . even with relatively small quantities of unlabeled training data , the re-estimated models show promising improvements in labeled bracketing f-scores on wall street journal parsing , and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs .

learning a lexicon for broad-coverage semantic parsing
while there has been significant recent work on learning semantic parsers for specific task/ domains , the results dont transfer from one domain to another domains . we describe a project to learn a broad-coverage semantic lexicon for domain independent semantic parsing . the technique involves several bootstrapping steps starting from a semantic parser based on a modest-sized hand-built semantic lexicon . we demonstrate that the approach shows promise in building a semantic lexicon on the scale of wordnet , with more coverage and detail that currently available in widely-used resources such as verbnet . we view having such a lexicon as a necessary prerequisite for any attempt at attaining broad-coverage semantic parsing in any domain . the approach we described applies to all word classes , but in this paper we focus here on verbs , which are the most critical phenomena facing semantic parsing .

extracting gene regulation networks using linear-chain conditional random fields and rules slavko zitnik marinka zitnik blaz zupan marko bajec
published literature in molecular genetics may collectively provide much information on gene regulation networks . dedicated computational approaches are required to sip through large volumes of text and infer gene interactions . we propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules . also , we introduce a new skip-mention data representation to enable distant relation extraction using first-order models . to account for a variety of relation types , multiple models are inferred . the system was applied to the bionlp 2013 gene regulation network shared task . our approach was ranked first of five , with a slot error rate of 0.73 .

the unified annotation of syntax and discourse in the copenhagen dependency treebanks matthias buch-kromann irn korzen
we propose a unified model of syntax and discourse in which text structure is viewed as a tree structure augmented with anaphoric relations and other secondary relations . we describe how the model accounts for discourse connectives and the syntax-discourse-semantics interface . our model is dependency-based , ie , words are the basic building blocks in our analyses . the analyses have been applied cross-linguistically in the copenhagen dependency treebanks , a set of parallel treebanks for danish , english , german , italian , and spanish which are currently being annotated with respect to discourse , anaphora , syntax , morphology , and translational equivalence .

a flexemic tagset for polish
the article notes certain weaknesses of current efforts aiming at the standardization of pos tagsets for morphologically rich languages and argues that , in order to achieve clear mappings between tagsets , it is necessary to have clear and formal rules of delimiting poss and grammatical categories within any given tagset . an attempt at constructing such a tagset for polish is presented .

detecting change and emergence for multiword expressions
this work looks at a temporal aspect of multiword expressions ( mwes ) , namely that the behaviour of a given n-gram and its status as a mwe change over time . we propose a model in which context words have particular probabilities given a usage choice for an n-gram , and those usage choices have time dependent probabilities , and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice . for a range of mwe usages of recent coinage , we evaluate whether the technique is able to detect the emerging usage .

smatch : an evaluation metric for semantic feature structures
the evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation . however , there is no widely-used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study .

gender attribution : tracing stylometric evidence beyond topic and genre
sociolinguistic theories ( e.g. , lakoff ( 1973 ) ) postulate that womens language styles differ from that of men . in this paper , we explore statistical techniques that can learn to identify the gender of authors in modern english text , such as web blogs and scientific papers . although recent work has shown the efficacy of statistical approaches to gender attribution , we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier . our work is the first that consciously avoids gender bias in topics , thereby providing stronger evidence to gender-specific styles in language beyond topic . in addition , our comparative study provides new insights into robustness of various stylometric techniques across topic and genre .

jointly modeling aspects and opinions with a maxent-lda hybrid
discovering and summarizing opinions from online reviews is an important and challenging task . a commonly-adopted framework generates structured review summaries with aspects and opinions . recently topic models have been used to identify meaningful review aspects , but existing topic models do not identify aspect-specific opinion words . in this paper , we propose a maxent-lda hybrid model to jointly discover both aspects and aspect-specific opinion words . we show that with a relatively small amount of training data , our model can effectively identify aspect and opinion words simultaneously . we also demonstrate the domain adaptability of our model .

multimodal dialog description language for rapid system development
in this paper , we explain a rapid development method of multimodal dialogue sys-tem using miml ( multimodal interaction markup language ) , which defines dialogue patterns between human and various types of interactive agents . the feature of this language is three-layered description of agent-based interactive systems which separates task level description , interaction description and device dependent realization . miml has advantages in high-level interaction description , modality extensibility and compatibility with standardized technologies .

in-depth exploitation of noun and verb semantics
recognition of causality is important to achieve natural language discourse understanding . previous approaches rely on shallow linguistic features . in this work , we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs . particularly , we acquire and employ three novel types of knowledge : ( 1 ) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies , ( 2 ) data-driven semantic classes of verbal events with the least tendency to encode causality , and ( 3 ) tendencies of verb frames to encode causality . using these knowledge sources , we achieve around 15 % improvement in fscore over a supervised classifier trained using linguistic features .

predicting the fluency of text with shallow structural features : case studies of machine translation and human-written text
sentence fluency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that define it . we report the results of an initial study into the predictive power of surface syntactic statistics for the task ; we use fluency assessments done for the purpose of evaluating machine translation . we find that these features are weakly but significantly correlated with fluency . machine and human translations can be distinguished with accuracy over 80 % . the performance of pairwise comparison of fluency is also very highover 90 % for a multi-layer perceptron classifier . we also test the hypothesis that the learned models capture general fluency properties applicable to human-written text . the results do not support this hypothesis : prediction accuracy on the new data is only 57 % . this finding suggests that developing a dedicated , task-independent corpus of fluency judgments will be beneficial for further investigations of the problem .

analysis and robust extraction of changing named entities masatoshi tsuchiya shoko endo seiichi nakagawa
this paper focuses on the change of named entities over time and its influence on the performance of the named entity tagger . first , we analyze japanese named entities which appear in mainichi newspaper articles published in 1995 , 1996 , 1997 , 1998 and 2005. this analysis reveals that the number of named entity types and the number of named entity tokens are almost steady over time and that 70 80 % of named entity types in a certain year occur in the articles published either in its succeeding year or in its preceding year . these facts lead that 20 30 % of named entity types are replaced with new ones every year . the experiment against these texts shows that our proposing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training works robustly although the traditional supervised method is fragile against the change of name entity distribution .

extracting product features and opinions from reviews
consumers are often forced to wade through many on-line reviews in order to make an informed product choice . this paper introduces opine , an unsupervised informationextraction system which mines reviews in order to build a model of important product features , their evaluation by reviewers , and their relative quality across products . compared to previous work , opine achieves 22 % higher precision ( with only 3 % lower recall ) on the feature extraction task . opines novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity .

a unified framework for scope learning via simplified shallow semantic parsing
this paper approaches the scope learning problem via simplified shallow semantic parsing . this is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue . evaluation on the bioscope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments . it also shows that our parsing approach significantly outperforms the state-of-the-art chunking ones . although our parsing approach is only evaluated on negation and speculation scope learning here , it is portable to other kinds of scope learning .

understanding information graphics : a discourse-level problem
information graphics that appear in newspapers and magazines generally have a message that the viewer is intended to recognize . this paper argues that understanding such information graphics is a discourse-level problem . in particular , it requires assimilating information from multiple knowledge sources to recognize the intended message of the graphic , just as recognizing intention in text does . moreover , when an article is composed of text and graphics , the intended message of the information graphic ( its discourse intention ) must be integrated into the discourse structure of the surrounding text and contributes to the overall discourse intention of the article . this paper describes how we extend plan-based techniques that have been used for understanding traditional discourse to the understanding of information graphics . this work is part of a project to develop an interactive natural language system that provides sight-impaired users with access to information graphics .

towards a game-theoretic approach to content determination
this paper argues for a game-theoretic approach to content determination that uses text-type specific strategies in order to determine the optimal content for various user types . by means of content determination for the description of numerical data the benefits of a game-theoretic treatment of content determination are outlined .

towards robust unsupervised personal name disambiguation
the increasing use of large open-domain document sources is exacerbating the problem of ambiguity in named entities . this paper explores the use of a range of syntactic and semantic features in unsupervised clustering of documents that result from ad hoc queries containing names . from these experiments , we find that the use of robust syntactic and semantic features can significantly improve the state of the art for disambiguation performance for personal names for both chinese and english .

semantic role assignment for event nominalisations by leveraging verbal data
this paper presents a novel approach to the task of semantic role labelling for event nominalisations , which make up a considerable fraction of predicates in running text , but are underrepresented in terms of training data and difficult to model . we propose to address this situation by data expansion . we construct a model for nominal role labelling solely from verbal training data . the best quality results from salvaging grammatical features where applicable , and generalising over lexical heads otherwise .

event-based hyperspace analogue to language for query expansion
bag-of-words approaches to information retrieval ( ir ) are effective but assume independence between words . the hyperspace analogue to language ( hal ) is a cognitively motivated and validated semantic space model that captures statistical dependencies between words by considering their co-occurrences in a surrounding window of text . hal has been successfully applied to query expansion in ir , but has several limitations , including high processing cost and use of distributional statistics that do not exploit syntax . in this paper , we pursue two methods for incorporating syntactic-semantic information from textual events into hal . we build the hal space directly from events to investigate whether processing costs can be reduced through more careful definition of word co-occurrence , and improve the quality of the pseudo-relevance feedback by applying event information as a constraint during hal construction . both methods significantly improve performance results in comparison with original hal , and interpolation of hal and relevance model expansion outperforms either method alone .

referring expression generation through attribute-based heuristics
in this paper , we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents . despite a wide variation in the way subjects refer across a set of ten stimuli , we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a significant degree . this leads us to propose an alternative way of thinking of referring expression generation , where each attribute in a description is provided by a separate heuristic .

nonconvex global optimization for latent-variable models
many models in nlp involve latent variables , such as unknown parses , tags , or alignments . finding the optimal model parameters is then usually a difficult nonconvex optimization problem . the usual practice is to settle for local optimization methods such as em or gradient ascent . we explore how one might instead search for a global optimum in parameter space , using branch-and-bound . our method would eventually find the global maximum ( up to a user-specified ) if run for long enough , but at any point can return a suboptimal solution together with an upper bound on the global maximum . as an illustrative case , we study a generative model for dependency parsing . we search for the maximum-likelihood model parameters and corpus parse , subject to posterior constraints . we show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints . we use the reformulation linearization technique to produce convex relaxations during branch-and-bound . although these techniques do not yet provide a practical solution to our instance of this np-hard problem , they sometimes find better solutions than viterbi em with random restarts , in the same time .

automatic identification of rhetorical roles using conditional random fields for legal document summarization
in this paper , we propose a machine learning approach to rhetorical role identification from legal documents . in our approach , we annotate roles in sample documents with the help of legal experts and take them as training data . conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets . the understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage . other important new findings in this work include that the training of a model for one sub-domain can be extended to another sub-domains with very limited augmentation of feature sets . moreover , we can significantly improve extraction-based summarization results by modifying the ranking of sentences with the importance of specific roles .

extracting salient keywords from instructional videos using joint text ,
this paper presents a multi-modal featurebased system for extracting salient keywords from transcripts of instructional videos . specifically , we propose to extract domain-specific keywords for videos by integrating various cues from linguistic and statistical knowledge , as well as derived sound classes and characteristic visual content types . the acquisition of such salient keywords will facilitate video indexing and browsing , and significantly improve the quality of current video search engines . experiments on four government instructional videos show that 82 % of the salient keywords appear in the top 50 % of the highly ranked keywords . in addition , the audiovisual cues improve precision and recall by 1.1 % and 1.5 % respectively .

improving pairwise coreference models through feature space hierarchy learning
this paper proposes a new method for significantly improving the performance of pairwise coreference models . given a set of indicators , our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models . in effect , our approach finds an optimal feature space ( derived from a base feature set and indicator set ) for discriminating coreferential mention pairs . although our approach explores a very large space of possible feature spaces , it remains tractable by exploiting the structure of the hierarchies built from the indicators . our experiments on the conll-2012 shared task english datasets ( gold mentions ) indicate that our method is robust relative to different clustering strategies and evaluation metrics , showing large and consistent improvements over a single pairwise model using the same base features . our best system obtains a competitive 67.2 of average f1 over muc , b3 , and ceaf which , despite its simplicity , places it above the mean score of other systems on these datasets .

information extraction for question answering : improving recall through syntactic patterns
we investigate the impact of the precision/recall trade-off of information extraction on the performance of an offline corpus-based question answering ( qa ) system . one of our findings is that , because of the robust final answer selection mechanism of the qa system , recall is more important . we show that the recall of the extraction component can be improved using syntactic parsing instead of more common surface text patterns , substantially increasing the number of factoid questions answered by the qa system .

an interactive machine translation system with online learning
state-of-the-art machine translation ( mt ) systems are still far from being perfect . an alternative is the so-called interactive machine translation ( imt ) framework , where the knowledge of a human translator is combined with the mt system . we present a statistical imt system able to learn from user feedback by means of the application of online learning techniques . these techniques allow the mt system to update the parameters of the underlying models in real time . according to empirical results , our system outperforms the results of conventional imt systems . to the best of our knowledge , this online learning capability has never been provided by previous imt systems . our imt system is implemented in c++ , javascript , and actionscript ; and is publicly available on the web .

extracting social power relationships from natural language
sociolinguists have long argued that social context influences language use in all manner of ways , resulting in lects 1 . this paper explores a text classification problem we will call lect modeling , an example of what has been termed computational sociolinguistics . in particular , we use machine learning techniques to identify social power relationships between members of a social network , based purely on the content of their interpersonal communication . we rely on statistical methods , as opposed to language-specific engineering , to extract features which represent vocabulary and grammar usage indicative of social power lect . we then apply support vector machines to model the social power lects representing superior-subordinate communication in the enron email corpus . our results validate the treatment of lect modeling as a text classification problem albeit a hard one and constitute a case for future research in computational sociolinguistics .

efcient parsing of highly ambiguous context-free grammars with bit vectors
an efficient bit-vector-based cky-style parser for context-free parsing is presented . the parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences . the parser uses bit-vector operations to parallelise the basic parsing operations . the parser is particularly useful when all analyses are needed rather than just the most probable one .

augmenting wordnet-based inference with argument mapping
wordnet is a useful resource for lexical inference in applications . inference over predicates , however , often requires a change in argument positions , which is not specified in wordnet . we propose a novel framework for augmenting wordnet-based inferences over predicates with corresponding argument mappings . we further present a concrete implementation of this framework , which yields substantial improvement to wordnet-based inference .

model adaptation via model interpolation and boosting for web search ranking
this paper explores two classes of model adaptation methods for web search ranking : model interpolation and error-driven learning approaches based on a boosting algorithm . the results show that model interpolation , though simple , achieves the best results on all the open test sets where the test data is very different from the training data . the tree-based boosting algorithm achieves the best performance on most of the closed test sets where the test data and the training data are similar , but its performance drops significantly on the open test sets due to the instability of trees . several methods are explored to improve the robustness of the algorithm , with limited success .

partial parse selection for robust deep processing
this paper presents an approach to partial parse selection for robust deep processing . the work is based on a bottom-up chart parser for hpsg parsing . following the definition of partial parses in ( kasper et al , 1999 ) , different partial parse selection methods are presented and evaluated on the basis of multiple metrics , from both the syntactic and semantic viewpoints . the application of the partial parsing in spontaneous speech texts processing shows promising competence of the method .

and speech processing , and speech processing , and speech processing ,
sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint . the majority of current approaches , however , attempt to detect the overall polarity of a sentence , paragraph , or text span , irrespective of the entities mentioned ( e.g. , laptops ) and their aspects ( e.g. , battery , screen ) . semeval2014 task 4 aimed to foster research in the field of aspect-based sentiment analysis , where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect . the task provided datasets containing manually annotated reviews of restaurants and laptops , as well as a common evaluation procedure . it attracted 163 submissions from 32 teams .

personalized recommendation of user comments via factor models
in recent years , the amount of user-generated opinionated texts ( e.g. , reviews , user comments ) continues to grow at a rapid speed : featured news stories on a major event easily attract thousands of user comments on a popular online news service . how to consume subjective information of this volume becomes an interesting and important research question . in contrast to previous work on review analysis that tried to filter or summarize information for a generic average user , we explore a different direction of enabling personalized recommendation of such information . for each user , our task is to rank the comments associated with a given article according to personalized user preference ( i.e. , whether the user is likely to like or dislike the comment ) . to this end , we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way . our full model significantly outperforms strong baselines as well as related models that have been considered in previous work .

automated whole sentence grammar correction using a noisy channel
automated grammar correction techniques have seen improvement over the years , but there is still much room for increased performance . current correction techniques mainly focus on identifying and correcting a specific type of error , such as verb form misuse or preposition misuse , which restricts the corrections to a limited scope . we introduce a novel technique , based on a noisy channel model , which can utilize the whole sentence context to determine proper corrections . we show how to use the em algorithm to learn the parameters of the noise model , using only a data set of erroneous sentences , given the proper language model . this frees us from the burden of acquiring a large corpora of corrected sentences . we also present a cheap and efficient way to provide automated evaluation results for grammar corrections by using bleu and meteor , in contrast to the commonly used manual evaluations .

semantic coherence scoring using an ontology iryna gurevych rainer malaka robert porzel hans-peter zorn
in this paper we present ontoscore , a system for scoring sets of concepts on the basis of an ontology . we apply our system to the task of scoring alternative speech recognition hypotheses ( srh ) in terms of their semantic coherence . we conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses . an evaluation of our system against the annotated data shows that , it successfully classifies 73.2 % in a german corpus of 2.284 srhs as either coherent or incoherent ( given a baseline of 54.55 % ) .

resolving paraphrases to support modeling language perception in an intelligent agent
when interacting with humans , intelligent agents must be able not only to understand natural language inputs but also to remember them and link their content with the contents of their memory of event and object instances . as inputs can come in a variety of forms , linking to memory can be successful only when paraphrasing relations are established between the meaning of new input and the content of the agents memory . this paper discusses a variety of types of paraphrases relevant to this task and describes the way we implement this capability in a virtual patient application .

softmax-margin crfs : training log-linear models with cost functions
we describe a method of incorporating taskspecific cost functions into standard conditional log-likelihood ( cll ) training of linear structured prediction models . recently introduced in the speech recognition community , we describe the method generally for structured models , highlight connections to cll and max-margin learning for structured prediction ( taskar et al , 2003 ) , and show that the method optimizes a bound on risk . the approach is simple , efficient , and easy to implement , requiring very little change to an existing cll implementation . we present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition .

wikipedia as sense inventory to improve diversity in web search results
is it possible to use sense inventories to improve web search results diversity for one word queries to answer this question , we focus on two broad-coverage lexical resources of a different nature : wordnet , as a de-facto standard used in word sense disambiguation experiments ; and wikipedia , as a large coverage , updated encyclopaedic resource which may have a better coverage of relevant senses in web pages . our results indicate that ( i ) wikipedia has a much better coverage of search results , ( ii ) the distribution of senses in search results can be estimated using the internal graph structure of the wikipedia and the relative number of visits received by each sense in wikipedia , and ( iii ) associating web pages to wikipedia senses with simple and efficient algorithms , we can produce modified rankings that cover 70 % more wikipedia senses than the original search engine rankings .

linguistic steganography using automatically generated paraphrases
this paper describes a method for checking the acceptability of paraphrases in context . we use the google n-gram data and a ccg parser to certify the paraphrasing grammaticality and fluency . we collect a corpus of human judgements to evaluate our system . the ultimate goal of our work is to integrate text paraphrasing into a linguistic steganography system , by using paraphrases to hide information in a cover text . we propose automatically generated paraphrases as a new and useful source of transformations for linguistic steganography , and show that our method for checking paraphrases is effective at maintaining a high level of imperceptibility , which is crucial for effective steganography .

approximate factoring for a search
we present a novel method for creating a estimates for structured search problems . in our approach , we project a complex model onto multiple simpler models for which exact inference is efficient . we use an optimization framework to estimate parameters for these projections in a way which bounds the true costs . similar to klein and manning ( 2003 ) , we then combine completion estimates from the simpler models to guide search in the original complex model . we apply our approach to bitext parsing and lexicalized parsing , demonstrating its effectiveness in these domains .

language technology from a european perspective
this paper describes the cooperation of four european universities aiming at attracting more students to european master studies in language and communication technologies . the cooperation has been formally approved within the framework of the new european program erasmus mundus as a specific support action in 2004. the consortium also aims at creating a sound basis for a joint master program in the field of language technology and computer science .

a quantitative model of word order and movement in english , dutch and german complement constructions cognitive psychology unit abstract we present a quantitative model of word
order and movement constraints that enables a simple and uniform treatment of a seemingly heterogeneous collection of linear order phenomena in english , dutch and german complement constructions ( wh-extraction , clause union , extraposition , verb clustering , particle movement , etc . ) . underlying the scheme are central assumptions of the psycholinguistically motivated performance grammar ( pg ) . here we describe this formalism in declarative terms based on typed feature unification . pg allows a homogenous treatment of both the within- and between-language variations of the ordering phenomena under discussion , which reduce to different settings of a small number of quantitative parameters .

domain adaptation of maximum entropy language models
we investigate a recently proposed bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition , given a large corpus of written language data and a small corpus of speech transcripts . experiments show that the method consistently outperforms linear interpolation which is typically used in such cases .

investigation of question classifier in question answering
in this paper , we investigate how an accurate question classifier contributes to a question answering system . we first present a maximum entropy ( me ) based question classifier which makes use of head word features and their wordnet hypernyms . we show that our question classifier can achieve the state of the art performance in the standard uiuc question dataset . we then investigate quantitatively the contribution of this question classifier to a feature driven question answering system . with our accurate question classifier and some standard question answer features , our question answering system performs close to the state of the art using trec corpus .

overview of the infectious diseases ( id ) task of bionlp shated task 2011
this paper presents the preparation , resources , results and analysis of the infectious diseases ( id ) information extraction task , a main task of the bionlp shared task 2011. the id task represents an application and extension of the bionlp09 shared task event extraction approach to full papers on infectious diseases . seven teams submitted final results to the task , with the highest-performing system achieving 56 % f-score in the full task , comparable to state-of-the-art performance in the established bionlp09 task . the results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

multilingual conceptual access to lexicon based on shared orthography : an ontology-driven study of chinese and japanese
in this paper we propose a model for conceptual access to multilingual lexicon based on shared orthography . our proposal relies crucially on two facts : that both chinese and japanese conventionally use chinese orthography in their respective writing systems , and that the chinese orthography is anchored on a system of radical parts which encodes basic concepts . each orthographic unit , called hanzi and kanji respectively , contains a radical which indicates the broad semantic class of the meaning of that unit . our study utilizes the homomorphism between the chinese hanzi and japanese kanji systems to ide1ntify bilingual word correspondences . we use bilingual dictionaries , including wordnet , to verify semantic relation between the crosslingual pairs . these bilingual pairs are then mapped to an ontology constructed based on relations to the relation between the meaning of each character and thebasic concept of their radical parts . the conceptual structure of the radical ontology is proposed as a model for simultaneous conceptual access to both languages . a study based on words containing characters composed of the ( mouth ) radical is given to illustrate the proposal and the actual model .

direct parsing of discontinuous constituents in german
discontinuities occur especially frequently in languages with a relatively free word order , such as german . generally , due to the longdistance dependencies they induce , they lie beyond the expressivity of probabilistic cfg , i.e. , they can not be directly reconstructed by a pcfg parser . in this paper , we use a parser for probabilistic linear context-free rewriting systems ( plcfrs ) , a formalism with high expressivity , to directly parse the german negra and tiger treebanks . in both treebanks , discontinuities are annotated with crossing branches . based on an evaluation using different metrics , we show that an output quality can be achieved which is comparable to the output quality of pcfg-based systems .

a bottom up approach to persian stemming amir azim sharifloo
stemmers have many applications in natural language processing and some fields such as information retrieval . many algorithms have been proposed for stemming . in this paper , we propose a new algorithm for persian language . our algorithm is a bottom up algorithm that is capable to reorganize without changing the implementation . our experiments show that the proposed algorithm has a suitable result in stemming and flexibility .

integration of the thesaurus for the social sciences ( thesoz ) in an information extraction system
we present current work dealing with the integration of a multilingual thesaurus for social sciences in a nlp framework for supporting knowledge-driven information extraction in the field of social sciences . we describe the various steps that lead to a running ie system : lexicalization of the labels of the thesaurus and semi-automatic generation of domain specific ie grammars , with their subsequent implementation in a finite state engine . finally , we outline the actual field of application of the ie system : analysis of social media for recognition of relevant topics in the context of elections .

controlling animated agents in natural language
this paper presents a prototype dialogue system , k3 , in which a user can instruct agents through speech input to manipulate various objects in a 3-d virtual world . in this paper , we focus on two distinctive features of the k3 system : plan-based anaphora resolution and handling vagueness in spatial expressions . after an overview of the system architecture , each of these features is described . we also look at the future research agenda of this system .

efficacy of beam thresholding , unification filtering and hybrid parsing in probabilistic hpsg parsing
we investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic hpsg parsing using the penn treebank . we first tested the beam thresholding and iterative parsing developed for pcfg parsing with an hpsg . next , we tested three techniques originally developed for deep parsing : quick check , large constituent inhibition , and hybrid parsing with a cfg chunk parser . the contributions of the large constituent inhibition and global thresholding were not significant , while the quick check and chunk parser greatly contributed to total parsing performance . the precision , recall and average parsing time for the penn treebank ( section 23 ) were 87.85 % , 86.85 % , and 360 ms , respectively .

the exploitation of spatial information in narrative discourse
we present the results of several machine learning tasks that exploit explicit spatial language to classify rhetorical relations and the spatial information of narrative events . three corpora are annotated with figure and ground ( granularity ) relationships , mereotopologically classified verbs and prepositions , and frames of reference . for rhetorical relations , nave bayesian models achieve 84.90 % and 57.87 % accuracy in classifying narration and background / elaboration relations respectively ( 16 % and 23 % above baseline ) . for the spatial information of narrative events , k* models achieve 55.68 % average accuracy ( 12 % above baseline ) for all spatial information types . this result is boosted to 71.85 % ( 28 % above baseline ) when inertial spatial reference and text sequence information are considered . overall , spatial information is shown to be central to narrative discourse structure and prediction tasks .

integrating sentence- and word-level error identification for disfluency correction
while speaking spontaneously , speakers often make errors such as self-correction or false starts which interfere with the successful application of natural language processing techniques like summarization and machine translation to this data . there is active work on reconstructing this errorful data into a clean and fluent transcript by identifying and removing these simple errors . previous research has approximated the potential benefit of conducting word-level reconstruction of simple errors only on those sentences known to have errors . in this work , we explore new approaches for automatically identifying speaker construction errors on the utterance level , and quantify the impact that this initial step has on word- and sentence-level reconstruction accuracy .

sentence level discourse parsing using syntactic and lexical information
we introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees . the models use syntactic and lexical features . a discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8 % over a state-ofthe-art decision-based discourse parser . a set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance .

opinion identification in spanish texts aiala ros dina wonsever
we present our work on the identification of opinions and its components : the source , the topic and the message . we describe a rule-based system for which we achieved a recall of 74 % and a precision of 94 % . experimentation with machine-learning techniques for the same task is currently underway .

cross-lingual projected expectation regularization for weakly supervised learning
we consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages . past approaches project labels across bitext and use them as features or gold labels for training . we propose a new method that projects model expectations rather than labels , which facilities transfer of model uncertainty across language boundaries . we encode expectations as constraints and train a discriminative crf model using generalized expectation criteria ( mann and mccallum , 2010 ) . evaluated on standard chinese-english and german-english ner datasets , our method demonstrates f1 scores of 64 % and 60 % when no labeled data is used . attaining the same accuracy with supervised crfs requires 12k and 1.5k labeled sentences . furthermore , when combined with labeled examples , our method yields significant improvements over state-of-the-art supervised methods , achieving best reported numbers to date on chinese ontonotes and german conll-03 datasets .

a hybrid approach to the development of dialogue systems directed by semantics
in this work we present an approach to the development of the basurde 1 dialogue system , which answers telephone queries about railway timetables in spanish . we will focus on the understanding and dialogue components which are modeled under a stochastic framework . the preliminary results from semantic and dialogue interpretations of user dialogue turns are also included in this work .

evaluating automatically generated user-focused multi-document summaries for geo-referenced images
this paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images . the automatic captioning procedure requires summarizing multiple web documents that contain information related to images location . we use summa ( saggion and gaizauskas , 2005 ) to generate generic and query-based multi-document summaries and evaluate them using rouge evaluation metrics ( lin , 2004 ) relative to human generated summaries . results show that , even though query-based summaries perform better than generic ones , they are still not selecting the information that human participants do . in particular , the areas of interest that human summaries display ( history , travel information , etc . ) are not contained in the query-based summaries . for our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile .

computational linguistics for helping requirements elicitation : a dream about automated software development
requirements elicitation is one of the first processes of software development and it is intended to be hand-made by means of analyst-stakeholder interviews . as a naturallanguage-based activity , requirements elicitation can take advantages of computational linguistics techniques , in order to achieve better results looking for automation in this field . in this paper we survey some of the work related to software development automation , guided by computational linguistics techniques , and performed by the computational language research group from the universidad nacional de colombia . we aim the definition of future trans-national effort to be made in this research line .

can you repeat that using word repetition to improve spoken term detection
we aim to improve spoken term detection performance by incorporating contextual information beyond traditional ngram language models . instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents . we show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document . we leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits . we then develop a principled approach to select interpolation weights using only the asr training data . using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the babel program .

combining independent syntactic and semantic annotation schemes
we present mais , a uima-based environment for combining information from various annotated resources . each resource contains one mode of linguistic annotation and remains independent from the other resources . interactions between annotations are defined based on use cases .

two-step data selection and rbmt-style synthetic rules
this paper presents the machine translation systems submitted by the abumatran project to the wmt 2014 translation task . the language pair concerned is englishfrench with a focus on french as the target language . the french to english translation direction is also considered , based on the word alignment computed in the other direction . large language and translation models are built using all the datasets provided by the shared task organisers , as well as the monolingual data from ldc . to build the translation models , we apply a two-step data selection method based on bilingual crossentropy difference and vocabulary saturation , considering each parallel corpus individually . synthetic translation rules are extracted from the development sets and used to train another translation model . we then interpolate the translation models , minimising the perplexity on the development sets , to obtain our final smt system . our submission for the english to french translation task was ranked second amongst nine teams and a total of twenty submissions .

linking syntactic and semantic arguments in a dependency-based formalism
we propose a formal characterization of variation in the syntactic realization of semantic arguments , using hierarchies of syntactic relations and thematic roles , and a mechanism of lexical inheritance to obtain valency frames from individual linking types . we embed the formalization in the new lexicalized , dependency-based grammar formalism of topological dependency grammar ( tdg ) ( duchier and debusmann , 2001 ) . we account for arguments that can be alternatively realized as a np or a pp , and model thematic role alternations . we also treat auxiliary constructions , where the correspondance between syntactic and semantic argumenthood is indirect.1

selecting sentences for answering complex questions
complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented , informative multi-document summarization . in this paper , we have experimented with one empirical and two unsupervised statistical machine learning techniques : kmeans and expectation maximization ( em ) , for computing relative importance of the sentences . however , the performance of these approaches depends entirely on the feature set used and the weighting of these features . we extracted different kinds of features ( i.e . lexical , lexical semantic , cosine similarity , basic element , tree kernel based syntactic and shallow-semantic ) for each of the document sentences in order to measure its importance and relevancy to the user query . we used a local search technique to learn the weights of the features . for all our methods of generating summaries , we have shown the effects of syntactic and shallow-semantic features over the bag of words ( bow ) features .

deriving adjectival scales from continuous space word representations
continuous space word representations extracted from neural network language models have been used effectively for natural language processing , but until recently it was not clear whether the spatial relationships of such representations were interpretable . mikolov et al ( 2013 ) show that these representations do capture syntactic and semantic regularities . here , we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales ( e.g. , okay < good < excellent ) . we evaluate the scales on the indirect answers to yes/no questions corpus ( de marneffe et al , 2010 ) . we obtain 72.8 % accuracy , which outperforms previous results ( 60 % ) on this corpus and highlights the quality of the scales extracted , providing further support that the continuous space word representations are meaningful .

event extraction from trimmed dependency graphs
we describe the approach to event extraction which the julielab team from fsu jena ( germany ) pursued to solve task 1 in the bionlp09 shared task on event extraction . we incorporate manually curated dictionaries and machine learning methodologies to sort out associated event triggers and arguments on trimmed dependency graph structures . trimming combines pruning irrelevant lexical material from a dependency graph and decorating particularly relevant lexical material from that graph with more abstract conceptual class information . given that methodological framework , the julielab team scored on 2nd rank among 24 competing teams , with 45.8 % precision , 47.5 % recall and 46.7 % f1-score on all 3,182 events .

the frobenius anatomy of relative pronouns
this paper develops a compositional vector-based semantics of relative pronouns within a categorical framework . frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns , including passing information between the relative clause and the modified noun phrase , as well as copying , combining , and discarding parts of the relative clause . we develop two instantiations of the abstract semantics , one based on a truth-theoretic approach and one based on corpus statistics .

creating and exploiting a resource of parallel parses information structure incremental specification in context
this paper describes the creation of a resource of german sentences with multiple automatically created alternative syntactic analyses ( parses ) for the same text , and how qualitative and quantitative investigations of this resource can be performed using annis , a tool for corpus querying and visualization . using the example of pp attachment , we show how parsing can benefit from the use of such a resource .

improving japanese zero pronoun resolution by global word sense disambiguation
this paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system . the word sense disambiguation is applied to verbs and nouns . we consider that case frames define verb senses and semantic features in a thesaurus define noun senses , respectively , and perform sense disambiguation by selecting them based on case analysis . in addition , according to the one sense per discourse heuristic , the word sense disambiguation results are cached and applied globally to the subsequent words . we integrated this global word sense disambiguation into our zero pronoun resolution system , and conducted experiments of zero pronoun resolution on two different domain corpora . both of the experimental results indicated the effectiveness of our approach .

detecting forum authority claims in online discussions
this paper explores the problem of detecting sentence-level forum authority claims in online discussions . using a maximum entropy model , we explore a variety of strategies for extracting lexical features in a sparse training scenario , comparing knowledge- and datadriven methods ( and combinations ) . the augmentation of lexical features with parse context is also investigated . we find that certain markup features perform remarkably well alone , but are outperformed by data-driven selection of lexical features augmented with parse context .

multiword expressions as dependency subgraphs
we propose to model multiword expressions as dependency subgraphs , and realize this idea in the grammar formalism of extensible dependency grammar ( xdg ) . we extend xdg to lexicalize dependency subgraphs , and show how to compile them into simple lexical entries , amenable to parsing and generation with the existing xdg constraint solver .

ksc-pal : a peer learning agent that encourages students to take the cynthia kersey and barbara di eugenio
we present an innovative application of discourse processing concepts to educational technology . in our corpus analysis of peer learning dialogues , we found that initiative and initiative shifts are indicative of learning , and of learning-conducive episodes . we are incorporating this finding in ksc-pal , the peer learning agent we have been developing . ksc-pal will promote learning by encouraging shifts in task initiative .

chunk-based verb reordering in vso sentences for arabic-english statistical machine translation
in arabic-to-english phrase-based statistical machine translation , a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in vso sentences , where the verb is anticipated with respect to the english word order . in this paper , we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the arabic side of a word-aligned parallel corpus . this method is applied to preprocess the training data , and to collect statistics about verb movements . from this analysis , specific verb reordering lattices are then built on the test sentences before decoding them . the application of our reordering methods on the training and test sets results in consistent bleu score improvements on the nist-mt 2009 arabicenglish benchmark .

inference rules and their application to recognizing textual entailment
in this paper , we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment . for this purpose , we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a hand-crafted lexical resource . following this , we derive a dependency-based structure representation from texts , which aims to provide a proper base for the inference rule application . the evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements .

cascaded regular grammars over xml documents clark programme & bultreebank project
the basic mechanism of clark for linguistic processing of text corpora is the cascade regular grammar processor . the main challenge to the grammars in question is how to apply them on xml encoding of the linguistic information . the system o ers a solution using an xpath language for constructing the input word to the grammar and an xml encoding of the categories of the recognized words .

towards model driven architectures for human language technologies alessandro di bari advanced studies of trento povo di trento povo di trento advanced studies of trento povo di trento
developing multi-purpose human language technologies ( hlt ) pipelines and integrating them into the large scale software environments is a complex software engineering task . one needs to orchestrate a variety of new and legacy natural language processing components , language models , linguistic and encyclopedic knowledge resources . this requires working with a variety of different apis , data formats and knowledge models . in this paper , we propose to employ the model driven development ( mdd ) approach to software engineering , which provides rich structural and behavioral modeling capabilities and solid software support for model transformation and code generation . these benefits help to increase development productivity and quality of hlt assets . we show how mdd techniques and tools facilitate working with different data formats , adapting to new languages and domains , managing uima type systems , and accessing the external knowledge bases .

error mining in parsing results projet
we introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems . we applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain . we were thus able to identify missing and erroneous information in these resources .

discovering the discriminative views : measuring term weights for division of electrical and computer engineering
this paper describes an approach to utilizing term weights for sentiment analysis tasks and shows how various term weighting schemes improve the performance of sentiment analysis systems . previously , sentiment analysis was mostly studied under data-driven and lexicon-based frameworks . such work generally exploits textual features for fact-based analysis tasks or lexical indicators from a sentiment lexicon . we propose to model term weighting into a sentiment analysis system utilizing collection statistics , contextual and topicrelated characteristics as well as opinionrelated properties . experiments carried out on various datasets show that our approach effectively improves previous methods .

construction of structurally annotated spoken dialogue corpus
this paper describes the structural annotation of a spoken dialogue corpus . by statistically dealing with the corpus , the automatic acquisition of dialoguestructural rules is achieved . the dialogue structure is expressed as a binary tree and 789 dialogues consisting of 8150 utterances in the ciair speech corpus are annotated . to evaluate the scalability of the corpus for creating dialogue-structural rules , a dialogue parsing experiment was conducted .

nus-pt : exploiting parallel texts for word sense disambiguation in the english all-words tasks yee seng chan and hwee tou ng and zhi zhong
we participated in the semeval-2007 coarse-grained english all-words task and fine-grained english all-words task . we used a supervised learning approach with svm as the learning algorithm . the knowledge sources used include local collocations , parts-of-speech , and surrounding words . we gathered training examples from english-chinese parallel corpora , semcor , and dso corpus . while the fine-grained sense inventory of wordnet was used to train our system employed for the fine-grained english all-words task , our system employed for the coarse-grained english all-words task was trained with the coarse-grained sense inventory released by the task organizers . our scores ( for both recall and precision ) are 0.825 and 0.587 for the coarse-grained english all-words task and fine-grained english all-words task respectively . these scores put our systems in the first place for the coarse-grained english all-words task1 and the second place for the fine-grained english all-words task .

fbk-tr : applying svm with multiple linguistic features for cross-level semantic similarity
recently , the task of measuring semantic similarity between given texts has drawn much attention from the natural language processing community . especially , the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts , e.g paragraph-sentence , sentence-phrase , phrase-word , etc . in this paper , we , the fbk-tr team , describe our system participating in task 3 `` cross-level semantic similarity '' , at semeval 2014. we also report the results obtained by our system , compared to the baseline and other participating systems in this task .

umcc_dlsi- ( sa ) : using a ranking algorithm and informal features to solve sentiment analysis in twitter antonio fernndez orqun ,
in this paper , we describe the development and performance of the supervised system umcc_dlsi- ( sa ) . this system uses corpora where phrases are annotated as positive , negative , objective , and neutral , to achieve new sentiment resources involving word dictionaries with their associated polarity . as a result , new sentiment inventories are obtained and applied in conjunction with detected informal patterns , to tackle the challenges posted in task 2b of the semeval2013 competition . assessing the effectiveness of our application in sentiment classification , we obtained a 69 % f-measure for neutral and an average of 43 % f-measure for positive and negative using tweets and sms messages .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

for chinese relation extraction
chinese is an ancient hieroglyphic . it is inattentive to structure . therefore , segmenting and parsing chinese are more difficult and less accurate . in this paper , we propose an omniword feature and a soft constraint method for chinese relation extraction . the omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation . in order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency . both omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by chinese word segmentation and parsing . we test these methods on the ace 2005 rdc chinese corpus . the results show a significant improvement in chinese relation extraction , outperforming other methods in f-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .

clustering voices in the waste land
t.s . eliots modernist poem the waste land is often interpreted as collection of voices which appear multiple times throughout the text . here , we investigate whether we can automatically cluster existing segmentations of the text into coherent , expert-identified characters . we show that clustering the waste land is a fairly difficult task , though we can do much better than random baselines , particularly if we begin with a good initial segmentation .

an hmm-based approach to automatic phrasing for mandarin textto-speech synthesis
automatic phrasing is essential to mandarin textto-speech synthesis . we select word format as target linguistic feature and propose an hmmbased approach to this issue . then we define four states of prosodic positions for each word when employing a discrete hidden markov model . the approach achieves high accuracy of roughly 82 % , which is very close to that from manual labeling . our experimental results also demonstrate that this approach has advantages over those part-ofspeech-based ones .

word ordering with phrase-based grammars
we describe an approach to word ordering using modelling techniques from statistical machine translation . the system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent , grammatical sentences . we describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases . against the best previous results reported on this task , obtained using syntax driven models , we report huge quality improvements , with bleu score gains of 20+ which we confirm with human fluency judgements . our system incorporates dependency language models , large n-gram language models , and minimum bayes risk decoding .

chinese word segmentation using various dictionaries
most of the chinese word segmentation systems utilizes monolingual dictionary and are used for monolingual processing . for the tasks of machine translation ( mt ) and cross-language information retrieval ( clir ) , another translation dictionary may be used to transfer the words of documents from the source languages to target languages . the inconsistencies resulting from the two types of dictionaries ( segmentation dictionary and transfer dictionary ) may produce some problems for mt and clir . this paper shows the effectiveness of the external resources ( bilingual dictionary and word list ) for chinese word segmentations .

effect of using regression on class confidence scores in sentiment analysis of twitter data , ali mert ertugrul
in this study , we aim to test our hypothesis that confidence scores of sentiment values of tweets aid in classification of sentiment . we used several feature sets consisting of lexical features , emoticons , features based on sentiment scores and combination of lexical and sentiment features . since our dataset includes confidence scores of real numbers in [ 0-1 ] range , we employ regression analysis on each class of sentiments . we determine the class label of a tweet by looking at the maximum of the confidence scores assigned to it by these regressors . we test the results against classification results obtained by converting the confidence scores into discrete labels . thus , the strength of sentiment is ignored . our expectation was that taking the strength of sentiment into consideration would improve the classification results . contrary to our expectations , our results indicate that using classification on discrete class labels and ignoring sentiment strength perform similar to using regression on continuous confidence scores .

sawdust : a semi-automated wizard dialogue utterance selection tool for domain-independent large-domain dialogue
we present a tool that allows human wizards to select appropriate response utterances for a given dialogue context from a set of utterances observed in a dialogue corpus . such a tool can be used in wizard-of-oz studies and for collecting data which can be used for training and/or evaluating automatic dialogue models . we also propose to incorporate such automatic dialogue models back into the tool as an aid in selecting utterances from a large dialogue corpus . the tool allows a user to rank candidate utterances for selection according to these automatic models .

taming structured perceptrons on wild feature vectors
structured perceptrons are attractive due to their simplicity and speed , and have been used successfully for tuning the weights of binary features in a machine translation system . in attempting to apply them to tuning the weights of real-valued features with highly skewed distributions , we found that they did not work well . this paper describes a modification to the update step and compares the performance of the resulting algorithm to standard minimum error-rate training ( mert ) . in addition , preliminary results for combining mert or structured-perceptron tuning of the log-linear feature weights with coordinate ascent of other translation system parameters are presented .

evaluation and extension of maximum entropy models with inequality constraints
a maximum entropy ( me ) model is usually estimated so that it conforms to equality constraints on feature expectations . however , the equality constraint is inappropriate for sparse and therefore unreliable features . this study explores an me model with box-type inequality constraints , where the equality can be violated to reflect this unreliability . we evaluate the inequality me model using text categorization datasets . we also propose an extension of the inequality me model , which results in a natural integration with the gaussian map estimation . experimental results demonstrate the advantage of the inequality models and the proposed extension .

automated error detection in digitized cultural heritage documents
the work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain . a hybrid method is proposed , combining statistical classification algorithms and linguistic knowledge to automatize post-ocr error detection and correction . the current paper deals with the integration of linguistic modules and their impact on error detection .

easily identifiable discourse relations
we present a corpus study of local discourse relations based on the penn discourse tree bank , a large manually annotated corpus of explicitly or implicitly realized relations . we show that while there is a large degree of ambiguity in temporal explicit discourse connectives , overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type . we achieve 93.09 % accuracy in classifying the explicit relations and 74.74 % accuracy overall . in addition , we show that some pairs of relations occur together in text more often than expected by chance . this finding suggests that global sequence classification of the relations in text can lead to better results , especially for implicit relations .

creating a finite-state parser with application semantics
parsli is a finite-state ( fs ) parser which can be tailored to the lexicon , syntax , and semantics of a particular application using a hand-editable declarative lexicon . the lexicon is defined in terms of a lexicalized tree adjoining grammar , which is subsequently mapped to a fs representation . this approach gives the application designer better and easier control over the natural language understanding component than using an off-the-shelf parser . we present results using parsli on an application that creates 3d-images from typed input .

ecnu : a combination method and multiple features for aspect extraction and sentiment polarity classification
this paper reports our submissions to the four subtasks of aspect based sentiment analysis ( absa ) task ( i.e. , task 4 ) in semeval 2014 including aspect term extraction and aspect sentiment polarity classification ( aspect-level tasks ) , aspect category detection and aspect category sentiment polarity classification ( categorylevel tasks ) . for aspect term extraction , we present three methods , i.e. , noun phrase ( np ) extraction , named entity recognition ( ner ) and a combination of np and ner method . for aspect sentiment classification , we extracted several features , i.e. , topic features , sentiment lexicon features , and adopted a maximum entropy classifier . our submissions rank above average .

a noisy-channel model for document compression
we present a document compression system that uses a hierarchical noisy-channel model of text production . our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input . the system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent , grammatical document compressions of arbitrary length . the system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text . our results support the claim that discourse knowledge plays an important role in document summarization .

hybrid multilingual parsing with hpsg for srl
in this paper we present our syntactic and semantic dependency parsing system submitted to both the closed and open challenges of the conll 2009 shared task . the system extends the system of zhang , wang , & uszkoreit ( 2008 ) in the multilingual direction , and achieves 76.49 average macro f1 score on the closed joint task . substantial improvements to the open srl task have been observed that are attributed to the hpsg parses with handcrafted grammars .

online word games for semantic data collection
obtaining labeled data is a significant obstacle for many nlp tasks . recently , online games have been proposed as a new way of obtaining labeled data ; games attract users by being fun to play . in this paper , we consider the application of this idea to collecting semantic relations between words , such as hypernym/hyponym relationships . we built three online games , inspired by the real-life games of scattergoriestm and tabootm . as of june 2008 , players have entered nearly 800,000 data instances , in two categories . the first type of data consists of category/answer pairs ( types of vehicle , car ) , while the second is essentially free association data ( submarine , underwater ) . we analyze both types of data in detail and discuss potential uses of the data . we show that we can extract from our data set a significant number of new hypernym/hyponym pairs not already found in wordnet .

insertion operator for bayesian tree substitution grammars
we propose a model that incorporates an insertion operator in bayesian tree substitution grammars ( btsg ) . tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than btsg . the experimental parsing results show that our model outperforms a standard pcfg and btsg for a small dataset . for a large dataset , our model obtains comparable results to btsg , making the number of grammar rules much smaller than with btsg .

a psychologically plausible and computationally effective approach to
computational learning of natural language is often attempted without using the knowledge available from other research areas such as psychology and linguistics . this can lead to systems that solve problems that are neither theoretically or practically useful . in this paper we present a system cll which aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible . this theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax . cll has then been applied to a corpus of declarative sentences from the penn treebank ( marcus et al , 1993 ; marcus et al , 1994 ) on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems , which are significantly more supervised and are applied to somewhat simpler problems .

corpus effects on the evaluation of automated transliteration systems sarvnaz karimi andrew turpin falk scholer
most current machine transliteration systems employ a corpus of known sourcetarget word pairs to train their system , and typically evaluate their systems on a similar corpus . in this paper we explore the performance of transliteration systems on corpora that are varied in a controlled way . in particular , we control the number , and prior language knowledge of human transliterators used to construct the corpora , and the origin of the source words that make up the corpora . we find that the word accuracy of automated transliteration systems can vary by up to 30 % ( in absolute terms ) depending on the corpus on which they are run . we conclude that at least four human transliterators should be used to construct corpora for evaluating automated transliteration systems ; and that although absolute word accuracy metrics may not translate across corpora , the relative rankings of system performance remains stable across differing corpora .

handling information access dialogue through qa technologies a novel challenge for open-domain question answering
a novel challenge for evaluating open-domain question answering technologies is proposed . in this challenge , question answering systems are supposed to be used interactively to answer a series of related questions , whereas in the conventional setting , systems answer isolated questions one by one . such an interaction occurs in the case of gathering information for a report on a specific topic , or when browsing information of interest to the user . in this paper , first , we explain the design of the challenge . we then discuss its reality and show how the capabilities measured by the challenge are useful and important in practical situations , and that the difficulty of the challenge is proper for evaluating the current state of open-domain question answering technologies .

determining compositionality of word expressions using word space models
this research focuses on determining semantic compositionality of word expressions using word space models ( wsms ) . we discuss previous works employing wsms and present differences in the proposed approaches which include types of wsms , corpora , preprocessing techniques , methods for determining compositionality , and evaluation testbeds . we also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components . the vectors were obtained by latent semantic analysis ( lsa ) applied to the ukwac corpus . our results outperform those of all the participants in the distributional semantics and compositionality ( disco ) 2011 shared task .

ra-sr : using a ranking algorithm to automatically building resources for subjectivity analysis over annotated corpora antonio fernndez orqun , andrs
in this paper we propose a method that uses corpora where phrases are annotated as positive , negative , objective and neutral , to achieve new sentiment resources involving words dictionaries with their associated polarity . our method was created to build sentiment words inventories based on sentisemantic evidences obtained after exploring text with annotated sentiment polarity information . through this process a graph-based algorithm is used to obtain auto-balanced values that characterize sentiment polarities well used on sentiment analysis tasks . to assessment effectiveness of the obtained resource , sentiment classification was made , achieving objective instances over 80 % .

knowledge-based question answering as machine translation
a typical knowledge-based question answering ( kb-qa ) system faces two challenges : one is to transform natural language questions into their meaning representations ( mrs ) ; the other is to retrieve answers from knowledge bases ( kbs ) using generated mrs. unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework . we translate questions to answers based on cyk parsing . answers as translations of the span covered by each cyk cell are obtained by a question translation method , which first generates formal triple queries as mrs for the span based on question patterns and relation expressions , and then retrieves answers from a given kb based on triple queries generated . a linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs . compared to a kb-qa system using a state-of-the-art semantic parser , our method achieves better results .

which system differences matter
we investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems . the join evaluation and differences identification ( jedi ) , finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question . jedi provides evidence on the usefulness of a recent method , `1/`p-regularized regression ( obozinski et al , 2007 ) . we evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information .

hybrid approach for coreference resolution
this paper describes our participation in the conll-2011 shared task for closed task . the approach used combines refined salience measure based pronominal resolution and crfs for non-pronominal resolution . in this work we also use machine learning based approach for identifying non-anaphoric pronouns .

using collocation segmentation to augment the phrase table
this paper describes the 2010 phrase-based statistical machine translation system developed at the talp research center of the upc 1 in cooperation with bmic 2 and vmu 3 . in phrase-based smt , the phrase table is the main tool in translation . it is created extracting phrases from an aligned parallel corpus and then computing translation model scores with them . performing a collocation segmentation over the source and target corpus before the alignment causes that different and larger phrases are extracted from the same original documents . we performed this segmentation and used the union of this phrase set with the phrase set extracted from the nonsegmented corpus to compute the phrase table . we present the configurations considered and also report results obtained with internal and official test sets .

what to do when lexicalization fails : parsing german with suffix analysis
in this paper , we present an unlexicalized parser for german which employs smoothing and suffix analysis to achieve a labelled bracket f-score of 76.2 , higher than previously reported results on the negra corpus . in addition to the high accuracy of the model , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .

samplerank training for phrase-based machine translation
statistical machine translation systems are normally optimised for a chosen gain function ( metric ) by using mert to find the best model weights . this algorithm suffers from stability problems and can not scale beyond 20-30 features . we present an alternative algorithm for discriminative training of phrasebasedmt systems , samplerank , which scales to hundreds of features , equals or beats mert on both small and medium sized systems , and permits the use of sentence or document level features . samplerank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function .

varro : an algorithm and toolkit for regular structure discovery in centrum voor computerlingustiek , ku leuven
the varro toolkit is a system for identifying and counting a major class of regularity in treebanks and annotated natural language data in the form of treestructures : frequently recurring unordered subtrees . this software has been designed for use in linguistics to be maximally applicable to actually existing treebanks and other stores of tree-structurable natural language data . it minimizes memory use so that moderately large treebanks are tractable on commonly available computer hardware . this article introduces condensed canonically ordered trees as a data structure for efficiently discovering frequently recurring unordered subtrees .

chinese-english organization name translation based on correlative expansion
this paper presents an approach to translating chinese organization names into english based on correlative expansion . firstly , some candidate translations are generated by using statistical translation method . and several correlative named entities for the input are retrieved from a correlative named entity list . secondly , three kinds of expansion methods are used to generate some expanded queries . finally , these queries are submitted to a search engine , and the refined translation results are mined and re-ranked by using the returned web pages . experimental results show that this approach outperforms the compared system in overall translation accuracy .

command-line utilities for managing and exploring annotated corpora
users of annotated corpora frequently perform basic operations such as inspecting the available annotations , filtering documents , formatting data , and aggregating basic statistics over a corpus . while these may be easily performed over flat text files with stream-processing unix tools , similar tools for structured annotation require custom design . dawborn and curran ( 2014 ) have developed a declarative description and storage for structured annotation , on top of which we have built generic command-line utilities . we describe the most useful utilities some for quick data exploration , others for high-level corpus management with reference to comparable unix utilities . we suggest that such tools are universally valuable for working with structured corpora ; in turn , their utility promotes common storage and distribution formats for annotated text .

analyzing argumentative discourse units in online interactions
argument mining of online interactions is in its infancy . one reason is the lack of annotated corpora in this genre . to make progress , we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation . we propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential .

translation acquisition using synonym sets
we propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora . the motivation is that , given a certain query term , it is often possible for a user to specify one or more synonyms . using the resulting set of query terms has the advantage that we can overcome the problem that a single query terms context vector does not always reliably represent a terms meaning due to the context vectors sparsity . our proposed method uses a weighted average of the synonyms context vectors , that is derived by inferring the mean vector of the von mises-fisher distribution . we evaluate our method , using the synsets from the cross-lingually aligned japanese and english wordnet . the experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors .

an enhanced model for chinese word segmentation and part-of-speech tagging
this paper will present an enhanced probabilistic model for chinese word segmentation and part-of-speech ( pos ) tagging . the model introduces the information of chinese word length as one of its features to reach a more accurate result . and in addition , the model also achieves the integration of segmentation and pos tagging . after presenting the model , this paper will give a brief discussion on how to solve the problems in statistics and how to further integrate chinese named entity recognition into the model . finally , some figures of experiments and comparisons will be reported , which shows that the accuracy of word segmentation is 97.09 % , and the accuracy of pos tagging is 98.77 % .

recipes for building voice search uis for automotive
in this paper we describe a set of techniques we found suitable for building multi-modal search applications for automotive environments . as these applications often search across different topical domains , such as maps , weather or wikipedia , we discuss the problem of switching focus between different domains . also , we propose techniques useful for minimizing the response time of the search system in mobile environment . we evaluate some of the proposed techniques by means of usability tests with 10 novice test subjects who drove a simulated lane change test on a driving simulator . we report results describing the induced driving distraction and user acceptance .

using lexical expansion to learn inference rules from sparse data
automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words . in this scheme , prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability . to improve the learning of such rules in an unsupervised way , we propose to lexically expand sparse argument word vectors with semantically similar words . our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines .

using idiolects and sociolects to improve word prediction
in this paper the word prediction system soothsayer 1 is described . this system predicts what a user is going to write as he is keying it in . the main innovation of soothsayer is that it not only uses idiolects , the language of one individual person , as its source of knowledge , but also sociolects , the language of the social circle around that person . we use twitter for data collection and experimentation . the idiolect models are based on individual twitter feeds , the sociolect models are based on the tweets of a particular person and the tweets of the people he often communicates with . the idea behind this is that people who often communicate start to talk alike ; therefore the language of the friends of person x can be helpful in trying to predict what person x is going to say . this approach achieved the best results . for a number of users , more than 50 % of the keystrokes could have been saved if they had used soothsayer .

augmenting ensemble classification for word sense disambiguation with a kernel pca model
the hkust word sense disambiguation systems benefit from a new nonlinear kernel principal component analysis ( kpca ) based disambiguation technique . we discuss and analyze results from the senseval-3 english , chinese , and multilingual lexical sample data sets . among an ensemble of four different kinds of voted models , the kpca-based model , along with the maximum entropy model , outperforms the boosting model and nave bayes model . interestingly , while the kpcabased model typically achieves close or better accuracy than the maximum entropy model , nevertheless a comparison of predicted classifications shows that it has a significantly different bias . this characteristic makes it an excellent voter , as confirmed by results showing that removing the kpca-based model from the ensemble generally degrades performance .

a conversational in-car dialog system
in this demonstration we present a conversational dialog system for automobile drivers . the system provides a voicebased interface to playing music , finding restaurants , and navigating while driving . the design of the system as well as the new technologies developed will be presented . our evaluation showed that the system is promising , achieving high task completion rate and good user satisfation .

assessing the effectiveness of conversational features for dialogue segmentation in medical team meetings and in the ami corpus
this paper presents a comparison of two similar dialogue analysis tasks : segmenting real-life medical team meetings into patient case discussions , and segmenting scenario-based meetings into topics . in contrast to other methods which use transcribed content and prosodic features ( such as pitch , loudness etc ) , the method used in this comparison employs only the duration of the prosodic units themselves as the basis for dialogue representation . a concept of vocalisation horizon ( vh ) allows us to treat segmentation as a classification task where each instance to be classified is represented by the duration of a talk spurt , pause or speech overlap event in the dialogue . we report on the results this method yielded in segmentation of medical meetings , and on the implications of the results of further experiments on a larger corpus , the augmented multiparty meeting corpus , to our ongoing efforts to support data collection and information retrieval in medical team meetings .

proposal for multi-word expression annotation in running text
we present a proposal for the annotation of multi-word expressions in a 1m corpus of contemporary portuguese . our aim is to create a resource that allows us to study multi-word expressions ( mwes ) in their context . the corpus will be a valuable additional resource next to the already existing mwe lexicon that was based on a much larger corpus of 50m words . in this paper we discuss the problematic cases for annotation and proposed solutions , focusing on the variational properties of mwes .

cross-dataset clustering : revealing corresponding themes across multiple corpora
we present a method for identifying corresponding themes across several corpora that are focused on related , but distinct , domains . this task is approached through simultaneous clustering of keyword sets extracted from the analyzed corpora . our algorithm extends the informationbottleneck soft clustering method for a suitable setting consisting of several datasets . experimentation with topical corpora reveals similar aspects of three distinct religions . the evaluation is by way of comparison to clusters constructed manually by an expert .

factorizing complex models : a case study in mention
as natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features . one such example , explored in this article , is the mention detection and recognition task in the automatic content extraction project , with the goal of identifying named , nominal or pronominal references to real-world entitiesmentions and labeling them with three types of information : entity type , entity subtype and mention type . in this article , we investigate three methods of assigning these related tags and compare them on several data sets . a system based on the methods presented in this article participated and ranked very competitively in the ace04 evaluation .

can chinese phonemes improve machine transliteration : a comparative study of english-to-chinese transliteration models
inspired by the success of english grapheme-to-phoneme research in speech synthesis , many researchers have proposed phoneme-based english-to-chinese transliteration models . however , such approaches have severely suffered from the errors in chinese phoneme-to-grapheme conversion . to address this issue , we propose a new english-to-chinese transliteration model and make systematic comparisons with the conventional models . our proposed model relies on the joint use of chinese phonemes and their corresponding english graphemes and phonemes . experiments showed that chinese phonemes in our proposed model can contribute to the performance improvement in english-to-chinese transliteration .

taxonomy learning using word sense induction
taxonomies are an important resource for a variety of natural language processing ( nlp ) applications . despite this , the current stateof-the-art methods in taxonomy learning have disregarded word polysemy , in effect , developing taxonomies that conflate word senses . in this paper , we present an unsupervised method that builds a taxonomy of senses learned automatically from an unlabelled corpus . our evaluation on two wordnet-derived taxonomies shows that the learned taxonomies capture a higher number of correct taxonomic relations compared to those produced by traditional distributional similarity approaches that merge senses by grouping the features of each word into a single vector .

celi : edits and generic text pair classification
this paper presents celis participation in the semeval the joint student response analysis and 8th recognizing textual entailment challenge ( task7 ) and cross-lingual textual entailment for content synchronization task ( task 8 ) .

joint processing and discriminative training for sittichai jiampojamarn colin cherry grzegorz kondrak
we present a discriminative structureprediction model for the letter-to-phoneme task , a crucial step in text-to-speech processing . our method encompasses three tasks that have been previously handled separately : input segmentation , phoneme prediction , and sequence modeling . the key idea is online discriminative training , which updates parameters according to a comparison of the current system output to the desired output , allowing us to train all of our components together . by folding the three steps of a pipeline approach into a unified dynamic programming framework , we are able to achieve substantial performance gains . our results surpass the current state-of-the-art on six publicly available data sets representing four different languages .

ambiguity-aware ensemble training for semi-supervised
this paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training . instead of only using 1best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data . with a conditional random field based probabilistic dependency parser , our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings . this framework offers two promising advantages . 1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees . during training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves . 2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser . experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .

emergence of community structures in vowel inventories : an analysis based on complex networks
in this work , we attempt to capture patterns of co-occurrence across vowel systems and at the same time figure out the nature of the force leading to the emergence of such patterns . for this purpose we define a weighted network where the vowels are the nodes and an edge between two nodes ( read vowels ) signify their co-occurrence likelihood over the vowel inventories . through this network we identify communities of vowels , which essentially reflect their patterns of co-occurrence across languages . we observe that in the assortative vowel communities the constituent nodes ( read vowels ) are largely uncorrelated in terms of their features indicating that they are formed based on the principle of maximal perceptual contrast . however , in the rest of the communities , strong correlations are reflected among the constituent vowels with respect to their features indicating that it is the principle of feature economy that binds them together .

an experiment setup for collecting data for adaptive output planning in a multimodal dialogue system
we describe a wizard-of-oz experiment setup for the collection of multimodal interaction data for a music player application . this setup was developed and used to collect experimental data as part of a project aimed at building a flexible multimodal dialogue system which provides an interface to an mp3 player , combining speech and screen input and output . besides the usual goal of woz data collection to get realistic examples of the behavior and expectations of the users , an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development . the wizards responses were therefore not constrained by a script . one of the challenges we had to address was to allow the wizards to produce varied screen output a in real time . our setup includes a preliminary screen output planning module , which prepares several versions of possible screen output . the wizards were free to speak , and/or to select a screen output .

summarising legal texts : sentential tense and argumentative roles claire grover , ben hachey , & chris korycinski
we report on the sum project which applies automatic summarisation techniques to the legal domain . we pursue a methodology based on teufel and moens ( 2002 ) where sentences are classified according to their argumentative role . we describe some experiments with judgments of the house of lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles . we use state-of-the-art nlp techniques to perform the linguistic annotation using xml-based tools and a combination of rulebased and statistical methods . we focus here on the predictive capacity of tense and aspect features for a classifier .

integration of multiple bilingually-learned segmentation schemes into statistical machine translation
this paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation ( smt ) approaches . the method can be applied to any language pair where the source language is unsegmented and the target language segmentation is known . first , an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an smt system trained on the resegmented bitext . in the second step , multiple segmentation schemes are integrated into a single smt system by characterizing the source language side and merging identical translation pairs of differently segmented smt models . experimental results translating five asian languages into english revealed that the method of integrating multiple segmentation schemes outperforms smt models trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools .

modeling and analysis of elliptic coordination by dynamic exploitation of derivation forests in ltag parsing
in this paper , we introduce a generic approach to elliptic coordination modeling through the parsing of ltag grammars . we show that erased lexical items can be replaced during parsing by informations gathered in the other member of the coordinate structure and used as a guide at the derivation level . moreover , we show how this approach can be indeed implemented as a light extension of the ltag formalism throuh a so-called fusion operation and by the use of tree schemata during parsing in order to obtain a dependency graph .

uoy : graphs of unambiguous vertices for word sense induction and disambiguation
this paper presents an unsupervised graph-based method for automatic word sense induction and disambiguation . the innovative part of our method is the assignment of either a word or a word pair to each vertex of the constructed graph . word senses are induced by clustering the constructed graph . in the disambiguation stage , each induced cluster is scored according to the number of its vertices found in the context of the target word . our system participated in semeval-2010 word sense induction and disambiguation task .

relaxcor participation in conll shared task on coreference resolution
this paper describes the participation of relaxcor in the conll-2011 shared task : modeling unrestricted coreference in ontonotes . relaxcor is a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling . the approach combines the strengths of groupwise classifiers and chain formation methods in one global method .

automatically generating annotator rationales to improve sentiment classification ainur yessenalina yejin choi claire cardie
one of the central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document . previous research has shown that enriching the sentiment labels with human annotators rationales can produce substantial improvements in categorization performance . we explore methods to automatically generate annotator rationales for document-level sentiment classification . rather unexpectedly , we find the automatically generated rationales just as helpful as human rationales .

a topic model for word sense disambiguation
we develop latent dirichlet alocation with wordnet ( ldawn ) , an unsupervised probabilistic topic model that includes word sense as a hidden variable . we develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word . using the wordnet hierarchy , we embed the construction of abney and light ( 1999 ) in the topic model and show that automatically learned domains improve wsd accuracy compared to alternative contexts .

fasil email summarisation system
email summarisation presents a unique set of requirements that are different from general text summarisation . this work describes the implementation of an email summarisation system for use in a voice-based virtual personal assistant developed for the eu fasil project . evaluation results from the first integrated version of the project are presented .

semi-markov phrase-based monolingual alignment
we introduce a novel discriminative model for phrase-based monolingual alignment using a semi-markov crf . our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets ( rte and paraphrase ) , while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment . additional experiments highlight the potential benefit of our alignment model to rte , paraphrase identification and question answering , where even a naive application of our models alignment score approaches the state of the art .

what pushes their buttons predicting comment polarity from the content of political blog posts
political blogs as a form of social media allow for an uniquely interactive form of political discourse . this is especially evident in focused blogs with a strong ideological identity . we investigate techniques to identify topics within the context of the community , which when discussed in a blog post evoke a discernible positive or negative collective opinion from readers who respond to posts in comments . this is done by using computational methods to assign sentiment polarity to blog comments and learning community specific models that summarize issues tackled by blogs and predict the polarity based on the topics discussed in a blog post .

heuristic search in a cognitive model of human parsing
we present a cognitive process model of human sentence comprehension based on generalized left-corner parsing . a search heuristic based upon previouslyparsed corpora derives garden path effects , garden path paradoxes , and the local coherence effect .

identifying semantic roles using combinatory categorial grammar
we present a system for automatically identifying propbank-style semantic roles based on the output of a statistical parser for combinatory categorial grammar . this system performs at least as well as a system based on a traditional treebank parser , and outperforms it on core argument roles .

conceptual coherence in the generation of referring expressions
one of the challenges in the automatic generation of referring expressions is to identify a set of domain entities coherently , that is , from the same conceptual perspective . we describe and evaluate an algorithm that generates a conceptually coherent description of a target set . the design of the algorithm is motivated by the results of psycholinguistic experiments .

the cmu-ark german-english translation system
this paper describes the german-english translation system developed by the ark research group at carnegie mellon university for the sixth workshop on machine translation ( wmt11 ) . we present the results of several modeling and training improvements to our core hierarchical phrase-based translation system , including : feature engineering to improve modeling of the derivation structure of translations ; better handing of oovs ; and using development set translations into other languages to create additional pseudoreferences for training .

unsupervised word sense induction using distributional statistics
word sense induction is an unsupervised task to find and characterize different senses of polysemous words . this work investigates two unsupervised approaches that focus on using distributional word statistics to cluster the contextual information of the target words using two different algorithms involving latent dirichlet allocation and spectral clustering . using a large corpus for achieving this task , we quantitatively analyze our clusters on the semeval-2010 dataset and also perform a qualitative analysis of our induced senses . our results indicate that our methods successfully characterized the senses of the target words and were also able to find unconventional senses for those words .

target-centric features for translation quality estimation
we describe the dcu-mixed and dcusvr submissions to the wmt-14 quality estimation task 1.1 , predicting sentencelevel perceived post-editing effort . feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations , which are included in task 1.1 of this years wmt quality estimation shared task . we experiment with features of the quest framework , features of our past work , and three novel feature sets . despite these efforts , our two systems perform poorly in the competition . follow up experiments indicate that the poor performance is due to improperly optimised parameters .

selecting an ontology for biomedical text mining
text mining for biomedicine requires a significant amount of domain knowledge . much of this information is contained in biomedical ontologies . developers of text mining applications often look for appropriate ontologies that can be integrated into their systems , rather than develop new ontologies from scratch . however , there is often a lack of documentation of the qualities of the ontologies . a number of methodologies for evaluating ontologies have been developed , but it is difficult for users by using these methods to select an ontology . in this paper , we propose a framework for selecting the most appropriate ontology for a particular text mining application . the framework comprises three components , each of which considers different aspects of requirements of text mining applications on ontologies . we also present an experiment based on the framework choosing an ontology for a gene normalization system .

kui : an ubiquitous tool for collective intelligence development and kergrit robkop communications technology ( nict ) , japan
collective intelligence is the capability for a group of people to collaborate in order to achieve goals in a complex context than its individual member . this common concept increases topic of interest in many sciences including computer science where computers are bring about as group support elements . this paper presents a new platform , called knowledge unifying initiator ( kui ) for knowledge development which enables connection and collaboration among individual intelligence in order to accomplish a complex mission . kui is a platform to unify the various thoughts following the process of thinking , i.e. , initiating the topic of interest , collecting the opinions to the selected topics , localizing the opinions through the translation or customization and posting for public hearing to conceptualize the knowledge . the process of thinking is done under the selectional preference simulated by voting mechanism in case that many alternatives occur . by measuring the history of participation of each member , kui adaptively manages the reliability of each members opinion and vote according to the estimated expertscore .

measuring the compositionality of collocations via word co-occurrence vectors : shared task system description
a description of a system for measuring the compositionality of collocations within the framework of the shared task of the distributional semantics and compositionality workshop ( disco 2011 ) is presented . the system exploits the intuition that a highly compositional collocation would tend to have a considerable semantic overlap with its constituents ( headword and modifier ) whereas a collocation with low compositionality would share little semantic content with its constituents . this intuition is operationalised via three configurations that exploit cosine similarity measures to detect the semantic overlap between the collocation and its constituents . the system performs competitively in the task .

category competition drives contrast maintenance within an exemplar-based
the evolution of competing lexical categories is simulated within a model in which lexical outputs are organized as sequences of articulatory gestures . when exemplar-based categories compete for assignment and storage of incoming exemplars in a production/storage loop , contrast between categories spontaneously emerges and remains stable , driven by the differences in storage consistency between more contrastive and less contrastive variants . further , when lexical outputs are biased toward use of previously produced gestures , the set of exemplars in the lexicon evolve to be derived from a small set of contrastive units used in combination , despite the absence of direct selection for contrast at the sub-lexical level .

multi-class animacy classification with semantic features
animacy is the semantic property of nouns denoting whether an entity can act , or is perceived as acting , of its own will . this property is marked grammatically in various languages , albeit rarely in english . it has recently been highlighted as a relevant property for nlp applications such as parsing and anaphora resolution . in order for animacy to be used in conjunction with other semantic features for such applications , appropriate data is necessary . however , the few corpora which do contain animacy annotation , rarely contain much other semantic information . the addition of such an annotation layer to a corpus already containing deep semantic annotation should therefore be of particular interest . the work presented in this paper contains three main contributions . firstly , we improve upon the state of the art in multiclass animacy classification . secondly , we use this classifier to contribute to the annotation of an openly available corpus containing deep semantic annotation . finally , we provide source code , as well as trained models and scripts needed to reproduce the results presented in this paper , or aid in annotation of other texts .

verb phrase ellipsis detection using automatically parsed text leif arda nielsen
this paper describes a verb phrase ellipsis ( vpe ) detection system , built for robustness , accuracy and domain independence . the system is corpus-based , and uses a variety of machine learning techniques on free text that has been automatically parsed using two different parsers . tested on a mixed corpus comprising a range of genres , the system achieves a 72 % f1-score . it is designed as the first stage of a complete vpe resolution system that is input free text , detects vpes , and proceeds to find the antecedents and resolve them .

exact decoding of phrase-based translation models through lagrangian relaxation
this paper describes an algorithm for exact decoding of phrase-based translation models , based on lagrangian relaxation . the method recovers exact solutions , with certificates of optimality , on over 99 % of test examples . the method is much more efficient than approaches based on linear programming ( lp ) or integer linear programming ( ilp ) solvers : these methods are not feasible for anything other than short sentences . we compare our method to moses ( koehn et al , 2007 ) , and give precise estimates of the number and magnitude of search errors that moses makes .

machine learning for coreference resolution : from local classification to global ranking
in this paper , we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems . we propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions . our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets .

a corpus of italian web texts
pais ` a is a creative commons licensed , large web corpus of contemporary italian . we describe the design , harvesting , and processing steps involved in its creation .

evaluation metrics for the lexical substitution
we identify some problems of the evaluation metrics used for the english lexical substitution task of semeval-2007 , and propose alternative metrics that avoid these problems , which we hope will better guide the future development of lexical substitution systems .

understanding the value of features for coreference resolution
in recent years there has been substantial work on the important problem of coreference resolution , most of which has concentrated on the development of new models and algorithmic techniques . these works often show that complex models improve over a weak pairwise baseline . however , less attention has been given to the importance of selecting strong features to support learning a coreference model . this paper describes a rather simple pairwise classification model for coreference resolution , developed with a well-designed set of features . we show that this produces a state-of-the-art system that outperforms systems built with complex models . we suggest that our system can be used as a baseline for the development of more complex models which may have less impact when a more robust set of features is used . the paper also presents an ablation study and discusses the relative contributions of various features .

modelling early language acquisition skills : towards a general statistical learning mechanism
this paper reports the on-going research of a thesis project investigating a computational model of early language acquisition . the model discovers word-like units from crossmodal input data and builds continuously evolving internal representations within a cognitive model of memory . current cognitive theories suggest that young infants employ general statistical mechanisms that exploit the statistical regularities within their environment to acquire language skills . the discovery of lexical units is modelled on this behaviour as the system detects repeating patterns from the speech signal and associates them to discrete abstract semantic tags . in its current state , the algorithm is a novel approach for segmenting speech directly from the acoustic signal in an unsupervised manner , therefore liberating it from a pre-defined lexicon . by the end of the project , it is planned to have an architecture that is capable of acquiring language and communicative skills in an online manner , and carry out robust speech recognition . preliminary results already show that this method is capable of segmenting and building accurate internal representations of important lexical units as emergent properties from crossmodal data .

data-driven language independent word segmentation using
this paper presents a data-driven language independent word segmentation system that has been trained for chinese corpus at the second chinese word segmentation bakeoff . the system consists of a base segmentation algorithm and the refining procedures for the undecided character sequences . it does not use any lexicon and the base segmentation is simply done by character bigram and hmm-model is applied for the remaining character sequences . as a final step , high-frequency character trigram modifies the error-prone parts of the text.t1t

a graph-based approach to named entity categorization in wikipedia using conditional random fields
this paper presents a method for categorizing named entities in wikipedia . in wikipedia , an anchor text is glossed in a linked html text . we formalize named entity categorization as a task of categorizing anchor texts with linked html texts which glosses a named entity . using this representation , we introduce a graph structure in which anchor texts are regarded as nodes . in order to incorporate html structure on the graph , three types of cliques are defined based on the html tree structure . we propose a method with conditional random fields ( crfs ) to categorize the nodes on the graph . since the defined graph may include cycles , the exact inference of crfs is computationally expensive . we introduce an approximate inference method using treebased reparameterization ( trp ) to reduce computational cost . in experiments , our proposed model obtained significant improvements compare to baseline models that use support vector machines .

word alignment via submodular maximization over matroids
we cast the word alignment problem as maximizing a submodular function under matroid constraints . our framework is able to express complex interactions between alignment components while remaining computationally efficient , thanks to the power and generality of submodular functions . we show that submodularity naturally arises when modeling word fertility . experiments on the english-french hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches .

generation under space constraints joan giralt duran
reasoning about how much to generate when space is limited is a challenge for generation systems . this paper presents two algorithms that exploit the discourse structure to decide which content to drop when there are space restrictions , in the context of producing documents from pre-authored text fragments . we analyse the effectiveness of both algorithms and show that the second is near optimal .

the wild thing ! kenneth church bo thiesson
suppose you are on a mobile device with no keyboard ( e.g. , a cell or pda ) . how can you enter text quickly t9 graffiti this demo will show how language modeling can be used to speed up data entry , both in the mobile context , as well as the desktop . the wild thing encourages users to use wildcards ( * ) . a language model finds the k-best expansions . users quickly figure out when they can get away with wildcards . general purpose trigram language models are effective for the general case ( unrestricted text ) , but there are important special cases like searching over popular web queries , where more restricted language models are even more effective .

inducing gazetteers for named entity recognition by large-scale clustering of dependency relations communications technology ( nict ) ,
we propose using large-scale clustering of dependency relations between verbs and multiword nouns ( mns ) to construct a gazetteer for named entity recognition ( ner ) . since dependency relations capture the semantics of mns well , the mn clusters constructed by using dependency relations should serve as a good gazetteer . however , the high level of computational cost has prevented the use of clustering for constructing gazetteers . we parallelized a clustering algorithm based on expectationmaximization ( em ) and thus enabled the construction of large-scale mn clusters . we demonstrated with the irex dataset for the japanese ner that using the constructed clusters as a gazetteer ( cluster gazetteer ) is a effective way of improving the accuracy of ner . moreover , we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from wikipedia , which is also useful for ner , can further improve the accuracy in several cases .

context-dependent semantic parsing for time expressions
we present an approach for learning context-dependent semantic parsers to identify and interpret time expressions . we use a combinatory categorial grammar to construct compositional meaning representations , while considering contextual cues , such as the document creation time and the tense of the governing verb , to compute the final time values . experiments on benchmark datasets show that our approach outperforms previous stateof-the-art systems , with error reductions of 13 % to 21 % in end-to-end performance .

learning soft linear constraints with application to citation field
accurately segmenting a citation string into fields for authors , titles , etc . is a challenging task because the output typically obeys various global constraints . previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance . on the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference . we extend dual decomposition to perform prediction subject to soft constraints . moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training . this allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .

using web selectors for the disambiguation of all words
this research examines a word sense disambiguation method using selectors acquired from the web . selectors describe words which may take the place of another given word within its local context . work in using web selectors for noun sense disambiguation is generalized into the disambiguation of verbs , adverbs , and adjectives as well . additionally , this work incorporates previously ignored adverb context selectors and explores the effectiveness of each type of context selector according to its part of speech . overall results for verb , adjective , and adverb disambiguation are well above a random baseline and slightly below the most frequent sense baseline , a point which noun sense disambiguation overcomes . our experiments find that , for noun and verb sense disambiguation tasks , each type of context selector may assist target selectors in disambiguation . finally , these experiments also help to draw insights about the future direction of similar research .

tier-based strictly local constraints for phonology
beginning with goldsmith ( 1976 ) , the phonological tier has a long history in phonological theory to describe non-local phenomena . this paper defines a class of formal languages , the tier-based strictly local languages , which begin to describe such phenomena . then this class is located within the subregular hierarchy ( mcnaughton and papert , 1971 ) . it is found that these languages contain the strictly local languages , are star-free , are incomparable with other known sub-star-free classes , and have other interesting properties .

exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model
we investigate auxiliary distributions ( johnson and riezler , 2000 ) for domain adaptation of a supervised parsing system of dutch . to overcome the limited target domain training data , we exploit an original and larger out-of-domain model as auxiliary distribution . however , our empirical results exhibit that the auxiliary distribution does not help : even when very little target training data is available the incorporation of the out-of-domain model does not contribute to parsing accuracy on the target domain ; instead , better results are achieved either without adaptation or by simple model combination .

grammatical error detection and correction using a single maximum entropy model
this paper describes the system of shanghai jiao tong unvierity team in the conll-2014 shared task . error correction operations are encoded as a group of predefined labels and therefore the task is formulized as a multi-label classification task . for training , labels are obtained through a strict rule-based approach . for decoding , errors are detected and corrected according to the classification results . a single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm . our system achieved precision of 29.83 , recall of 5.16 and f 0.5 of 15.24 in the official evaluation .

an erp-based brain-computer interface for text entry using rapid serial visual presentation and language modeling
event related potentials ( erp ) corresponding to stimuli in electroencephalography ( eeg ) can be used to detect the intent of a person for brain computer interfaces ( bci ) . this paradigm is widely used to build letter-byletter text input systems using bci . nevertheless using a bci-typewriter depending only on eeg responses will not be sufficiently accurate for single-trial operation in general , and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed . hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed . in this demonstration we will present a bci system for typing that integrates a stochastic language model with erp classification to achieve speedups , via the rapid serial visual presentation ( rsvp ) paradigm .

philosophers are mortal : inferring the truth of unseen facts
large databases of facts are prevalent in many applications . such databases are accurate , but as they broaden their scope they become increasingly incomplete . in contrast to extending such a database , we present a system to query whether it contains an arbitrary fact . this work can be thought of as re-casting open domain information extraction : rather than growing a database of known facts , we smooth this data into a database in which any possible fact has membership with some confidence . we evaluate our system predicting held out facts , achieving 74.2 % accuracy and outperforming multiple baselines . we also evaluate the system as a commonsense filter for the reverb open ie system , and as a method for answer validation in a question answering task .

aggregation improves learning : experiments in natural language generation for intelligent tutoring systems
to improve the interaction between students and an intelligent tutoring system , we developed two natural language generators , that we systematically evaluated in a three way comparison that included the original system as well . we found that the generator which intuitively produces the best language does engender the most learning . specifically , it appears that functional aggregation is responsible for the improvement .

latent-descriptor clustering for unsupervised pos induction division of applied mathematics
we present a novel approach to distributionalonly , fully unsupervised , pos tagging , based on an adaptation of the em algorithm for the estimation of a gaussian mixture . in this approach , which we call latent-descriptor clustering ( ldc ) , word types are clustered using a series of progressively more informative descriptor vectors . these descriptors , which are computed from the immediate left and right context of each word in the corpus , are updated based on the previous state of the cluster assignments . the ldc algorithm is simple and intuitive . using standard evaluation criteria for unsupervised pos tagging , ldc shows a substantial improvement in performance over state-of-the-art methods , along with a several-fold reduction in computational cost .

beyond parallel data : joint word alignment and decipherment improves machine translation
inspired by previous work , where decipherment is used to improve machine translation , we propose a new idea to combine word alignment and decipherment into a single learning process . we use em to estimate the model parameters , not only to maximize the probability of parallel corpus , but also the monolingual corpus . we apply our approach to improve malagasy-english machine translation , where only a small amount of parallel data is available . in our experiments , we observe gains of 0.9 to 2.1 bleu over a strong baseline .

analytical approaches to combining mt technologies
the talk will report on recent and ongoing work dedicated to analytical methods for a systematic combination of observed strengths of translation technologies . the focus will be on different ways of exploiting existing data on mt output and performance measures for system combination and for gaining insights on strengths and weaknesses of existing technologies .

aueb : two stage sentiment analysis of social network messages
this paper describes the system submitted for the sentiment analysis in twitter task of semeval 2014 and specifically the message polarity classification subtask . we used a 2stage pipeline approach employing a linear svm classifier at each stage and several features including morphological features , pos tags based features and lexicon based features .

its time for a semantic engine
a common computational goal is to encapsulate the modeling of a target phenomenon within a unified and comprehensive engine , which addresses a broad range of the required processing tasks . this goal is followed in common modeling of the morphological and syntactic levels of natural language , where most processing tasks are encapsulated within morphological analyzers and syntactic parsers . in this talk i suggest that computational modeling of the semantic level should also focus on encapsulating the various processing tasks within a unified module ( engine ) . the input/output specification of such engine ( api ) can be based on the textual entailment paradigm , which will be described in brief and suggested as an attractive framework for applied semantic inference . the talk will illustrate an initial proposal for the engines api , designed to be embedded within the prominent language processing applications . finally , i will sketch the entailment formalism and efficient inference algorithm developed at bar-ilan university , which illustrates a principled transformational ( rather than interpretational ) approach towards developing a comprehensive semantic engine .

high oov-recall chinese word segmenter
for the competition of chinese word segmentation held in the first cips-sighna joint conference . we applied a subwordbased word segmenter using crfs and extended the segmenter with oov words recognized by accessor variety . moreover , we proposed several post-processing rules to improve the performance . our system achieved promising oov recall among all the participants .

proposel : a human-oriented prosody and pos english lexicon for
proposel is a prosody and pos english lexicon , purpose-built to integrate and leverage domain knowledge from several well-established lexical resources for machine learning and nlp applications . the lexicon of 104049 separate entries is in accessible text file format , is human and machine-readable , and is intended for open source distribution with the natural language toolkit . it is therefore supported by python software tools which transform proposel into a python dictionary or associative array of linguistic concepts mapped to compound lookup keys . users can also conduct searches on a subset of the lexicon and access entries by word class , phonetic transcription , syllable count and lexical stress pattern . proposel caters for a range of different cognitive aspects of the lexicon .

search-aware tuning for machine translation
parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as mert , mira and pro are agnostic about search , while search errors could severely degrade translation quality . we propose a searchaware framework to promote promising partial translations , preventing them from being pruned . to do so we develop two metrics to evaluate partial derivations . our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on chinese-to-english and english-to-chinese translation show up to +2.6 bleu gains over search-agnostic baselines .

natural language generation as planning under uncertainty for spoken
we present and evaluate a new model for natural language generation ( nlg ) in spoken dialogue systems , based on statistical planning , given noisy feedback from the current generation context ( e.g . a user and a surface realiser ) . we study its use in a standard nlg problem : how to present information ( in this case a set of search results ) to users , given the complex tradeoffs between utterance length , amount of information conveyed , and cognitive load . we set these trade-offs by analysing existing match data . we then train a nlg policy using reinforcement learning ( rl ) , which adapts its behaviour to noisy feedback from the current generation context . this policy is compared to several baselines derived from previous work in this area . the learned policy significantly outperforms all the prior approaches .

an intelligent procedure assistant
we will demonstrate the latest version of an ongoing project to create an intelligent procedure assistant for use by astronauts on the international space station ( iss ) . the system functionality includes spoken dialogue control of navigation , coordinated display of the procedure text , display of related pictures , alarms , and recording and playback of voice notes . the demo also exemplifies several interesting component technologies . speech recognition and language understanding have been developed using the open source regulus 2 toolkit . this implements an approach to portable grammar-based language modelling in which all models are derived from a single linguistically motivated unification grammar . domain-specific cfg language models are produced by first specialising the grammar using an automatic corpus-based method , and then compiling the resulting specialised grammars into cfg form . translation between language centered and domain centered semantic representations is carried out by alterf , another open source toolkit , which combines rule-based and corpusbased processing in a transparent way .

connective-based measuring of the inter-annotator agreement in the annotation of discourse in pdt
we present several ways of measuring the inter-annotator agreement in the ongoing annotation of semantic inter-sentential discourse relations in the prague dependency treebank ( pdt ) . two ways have been employed to overcome limitations of measuring the agreement on the exact location of the start/end points of the relations . both methods skipping one tree level in the start/end nodes , and the connective-based measure are focused on a recognition of the existence and of the type of the relations , rather than on fixing the exact positions of the start/end points of the connecting arrows .

beyond the pipeline : discrete optimization in nlp
we present a discrete optimization model based on a linear programming formulation as an alternative to the cascade of classiers implemented in many language processing systems . since nlp tasks are correlated with one another , sequential processing does not guarantee optimal solutions . we apply our model in an nlg application and show that it performs better than a pipeline-based system .

composite kernels for relation extraction
the automatic extraction of relations between entities expressed in natural language text is an important problem for ir and text understanding . in this paper we show how different kernels for parse trees can be combined to improve the relation extraction quality . on a public benchmark dataset the combination of a kernel for phrase grammar parse trees and for dependency parse trees outperforms all known tree kernel approaches alone suggesting that both types of trees contain complementary information for relation extraction .

compiling comp ling : practical weighted dynamic programming and the dyna language
weighted deduction with aggregation is a powerful theoretical formalism that encompasses many nlp algorithms . this paper proposes a declarative specification language , dyna ; gives general agenda-based algorithms for computing weights and gradients ; briefly discusses dyna-to-dyna program transformations ; and shows that a first implementation of a dyna-to-c++ compiler produces code that is efficient enough for real nlp research , though still several times slower than hand-crafted code .

effective adaptation of a hidden markov model-based named entity recognizer for biomedical domain
in this paper , we explore how to adapt a general hidden markov model-based named entity recognizer effectively to biomedical domain . we integrate various features , including simple deterministic features , morphological features , pos features and semantic trigger features , to capture various evidences especially for biomedical named entity and evaluate their contributions . we also present a simple algorithm to solve the abbreviation problem and a rule-based method to deal with the cascaded phenomena in biomedical domain . our experiments on genia v3.0 and genia v1.1 achieve the 66.1 and 62.5 f-measure respectively , which outperform the previous best published results by 8.1 f-measure when using the same training and testing data .

bilingual lexicon extraction from comparable corpora enhanced with
in this article , we present a simple and effective approach for extracting bilingual lexicon from comparable corpora enhanced with parallel corpora . we make use of structural characteristics of the documents comprising the comparable corpus to extract parallel sentences with a high degree of quality . we then use state-of-the-art techniques to build a specialized bilingual lexicon from these sentences and evaluate the contribution of this lexicon when added to the comparable corpus-based alignment technique . finally , the value of this approach is demonstrated by the improvement of translation accuracy for medical words .

co-training for cross-lingual sentiment classification
the lack of chinese sentiment corpora limits the research progress on chinese sentiment classification . however , there are many freely available english sentiment corpora on the web . this paper focuses on the problem of cross-lingual sentiment classification , which leverages an available english corpus for chinese sentiment classification by using the english corpus as training data . machine translation services are used for eliminating the language gap between the training set and test set , and english features and chinese features are considered as two independent views of the classification problem . we propose a cotraining approach to making use of unlabeled chinese data . experimental results show the effectiveness of the proposed approach , which can outperform the standard inductive classifiers and the transductive classifiers .

non-factoid japanese question answering through passage retrieval that is weighted based on types of answers
we constructed a system for answering nonfactoid japanese questions . we used various methods of passage retrieval for the system . we extracted paragraphs based on terms from an input question and output them as the preferred answers . we classified the non-factoid questions into six categories . we used a particular method for each category . for example , we increased the scores of paragraphs including the word reason for questions including the word why . we participated at ntcir-6 qac-4 , where our system obtained the most correct answers out of all the eight participating teams . the rate of accuracy was 0.77 , which indicates that our methods were effective .

learning arguments and supertypes of semantic relations using
a challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations . we propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the web . we evaluate the performance of our algorithm on multiple semantic relations expressed using verb , noun , and verb prep lexico-syntactic patterns . humanbased evaluation shows that the accuracy of the harvested information is about 90 % . we also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge .

approximating context-free by rational transduction for
existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples . this paper investigates the approximation of such transduction by means of weighted rational transduction . the advantage is increased processing speed , which benefits realtime applications involving spoken language .

a computational approach to politeness with application to social factors
we propose a computational framework for identifying linguistic aspects of politeness . our starting point is a new corpus of requests annotated for politeness , which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context . these findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory , such as indirection , deference , impersonalization and modality . our classifier achieves close to human performance and is effective across domains . we use our framework to study the relationship between politeness and social power , showing that polite wikipedia editors are more likely to achieve high status through elections , but , once elevated , they become less polite . we see a similar negative correlation between politeness and power on stack exchange , where users at the top of the reputation scale are less polite than those at the bottom . finally , we apply our classifier to a preliminary analysis of politeness variation by gender and community .

philippine language resources : trends and directions
we present the diverse research activities on philippine languages from all over the country , with focus on the center for language technologies of the college of computer studies , de la salle university , manila , where majority of the work are conducted . these projects include the formal representation of philippine languages and the processes involving these languages . language representation entails the manual and automatic development of language resources such as lexicons and corpora for various human languages including philippine languages , across various forms such as text , speech and video files . tools and applications on languages that we have worked on include morphological processes , part of speech tagging , language grammars , machine translation , sign language processing and speech systems . future directions are also presented .

improving coreference resolution by using conversational metadata
in this paper , we propose the use of metadata contained in documents to improve coreference resolution . specifically , we quantify the impact of speaker and turn information on the performance of our coreference system , and show that the metadata can be effectively encoded as features of a statistical resolution system , which leads to a statistically significant improvement in performance .

ecnucs : measuring short text semantic equivalence
this paper reports our submissions to the semantic textual similarity ( sts ) task in sem shared task 2013. we submitted three support vector regression ( svr ) systems in core task , using 6 types of similarity measures , i.e. , string similarity , number similarity , knowledge-based similarity , corpus-based similarity , syntactic dependency similarity and machine translation similarity . our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs . we also submitted two systems in typed task using string based measure and named entity based measure . our best system ranks 5 out of 15 runs .

a cognitive cost model of annotations based on eye-tracking data language & information language & information applied cognitive science applied cognitive science
we report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device . as experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics . our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and , more interestingly , indicates that fullscale context is mostly negligible with the exception of semantic high-complexity cases . we then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models . our data reveals that the cognitively founded model explains annotation costs ( expressed in annotation time ) more adequately than non-cognitive ones .

towards syntax-aware compositional distributional semantic models fabio massimo zanzotto
compositional distributional semantics models ( cdsms ) are traditionally seen as an entire different world with respect to tree kernels ( tks ) . in this paper , we show that under a suitable regime these two approaches can be regarded as the same and , thus , structural information and distributional semantics can successfully cooperate in csdms for nlp tasks . leveraging on distributed trees , we present a novel class of cdsms that encode both structure and distributional meaning : the distributed smoothed trees ( dsts ) . by using dsts to compute the similarity among sentences , we implicitly define the distributed smoothed tree kernels ( dstks ) . experiment with our dsts show that dstks approximate the corresponding smoothed tree kernels ( stks ) . thus , dsts encode both structural and distributional semantics of text fragments as stks do . experiments on rte and sts show that distributional semantics encoded in dstks increase performance over structure-only kernels .

mining name translations from comparable corpora by creating bilingual information networks
this paper describes a new task to extract and align information networks from comparable corpora . as a case study we demonstrate the effectiveness of this task on automatically mining name translation pairs . starting from a small set of seeds , we design a novel approach to acquire name translation pairs in a bootstrapping framework . the experimental results show this approach can generate highly accurate name translation pairs for persons , geopolitical and organization entities .

combining optimal clustering and hidden markov models for extractive summarization
we propose hidden markov models with unsupervised training for extractive summarization . extractive summarization selects salient sentences from documents to be included in a summary . unsupervised clustering combined with heuristics is a popular approach because no annotated data is required . however , conventional clustering methods such as k-means do not take text cohesion into consideration . probabilistic methods are more rigorous and robust , but they usually require supervised training with annotated data . our method incorporates unsupervised training with clustering , into a probabilistic framework . clustering is done by modified k-means ( mkm ) -- a method that yields more optimal clusters than the conventional k-means method . text cohesion is modeled by the transition probabilities of an hmm , and term distribution is modeled by the emission probabilities . the final decoding process tags sentences in a text with theme class labels . parameter training is carried out by the segmental k-means ( skm ) algorithm . the output of our system can be used to extract salient sentences for summaries, or used for topic detection . content - based evaluation shows that our method outperforms an existing extractive summarizer by 22.8 % in terms of relative similarity, and outperforms a baseline summarizer that selects the top n sentences as salient sentences by 46.3 % .

web-scale distributional similarity and entity set expansion
computing the pairwise semantic similarity between all words on the web is a computationally challenging task . parallelization and optimizations are necessary . we propose a highly scalable implementation based on distributional similarity , implemented in the mapreduce framework and deployed over a 200 billion word crawl of the web . the pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes . we apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size , corpus quality , seed composition and seed size . we make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from wikipedia .

a two-stage approach to retrieving answers for how-to
this paper addresses the problem of automatically retrieving answers for how-to questions , focusing on those that inquire about the procedure for achieving a specific goal . for such questions , typical information retrieval methods , based on key word matching , are better suited to detecting the content of the goal ( e.g. , installing a windows xp server ) than the general nature of the desired information ( i.e. , procedural , a series of steps for achieving this goal ) . we suggest dividing the process of retrieving answers for such questions into two stages , with each stage focusing on modeling one aspect of a how-to question . we compare the two-stage approach with two alternative approaches : a baseline approach that only uses the content of the goal to retrieve relevant documents and another approach that explores the potential of automatic query expansion . the result of the experiment shows that the two-stage approach significantly outperforms the baseline but achieves similar result with the systems using automatic query expansion techniques . we analyze the reason and also present some future work .

almut silja hildebrand
in this paper we present our entry to the wmt13 shared task : quality estimation ( qe ) for machine translation ( mt ) . we participated in the 1.1 , 1.2 and 1.3 sub-tasks with our qe system trained on features from diverse information sources like mt decoder features , n-best lists , mono- and bi-lingual corpora and giza training models . our system shows competitive results in the workshop shared task .

creating disjunctive logical forms from aligned sentences for grammar-based paraphrase generation
we present a method of creating disjunctive logical forms ( dlfs ) from aligned sentences for grammar-based paraphrase generation using the openccg broad coverage surface realizer . the method takes as input word-level alignments of two sentences that are paraphrases and projects these alignments onto the logical forms that result from automatically parsing these sentences . the projected alignments are then converted into phrasal edits for producing dlfs in both directions , where the disjunctions represent alternative choices at the level of semantic dependencies . the resulting dlfs are fed into the openccg realizer for n-best realization , using a pruning strategy that encourages lexical diversity . after merging , the approach yields an n-best list of paraphrases that contain grammatical alternatives to each original sentence , as well as paraphrases that mix and match content from the pair . a preliminary error analysis suggests that the approach could benefit from taking the word order in the original sentences into account . we conclude with a discussion of plans for future work , highlighting the methods potential use in enhancing automatic mt evaluation .

can they get along carla parra escartn
this paper reports different experiments created to study the impact of using linguistics to preprocess german compounds prior to translation in statistical machine translation ( smt ) . compounds are a known challenge both in machine translation ( mt ) and translation in general as well as in other natural language processing ( nlp ) applications . in the case of smt , german compounds are split into their constituents to decrease the number of unknown words and improve the results of evaluation measures like the bleu score . to assess to which extent it is necessary to deal with german compounds as a part of preprocessing in smt systems , we have tested different compound splitters and strategies , such as adding lists of compounds and their translations to the training set . this paper summarizes the results of our experiments and attempts to yield better translations of german nominal compounds into spanish and shows how our approach improves by up to 1.4 bleu points with respect to the baseline .

a hierarchical classifier applied to multi-way sentiment detection
this paper considers the problem of document-level multi-way sentiment detection , proposing a hierarchical classifier algorithm that accounts for the inter-class similarity of tagged sentiment-bearing texts . this type of classifier also provides a natural mechanism for reducing the feature space of the problem . our results show that this approach improves on state-of-the-art predictive performance for movie reviews with three-star and fourstar ratings , while simultaneously reducing training times and memory requirements .

using soundex codes for indexing names in asr documents
in this paper we highlight the problems that arise due to variations of spellings of names that occur in text , as a result of which links between two pieces of text where the same name is spelt differently may be missed . the problem is particularly pronounced in the case of asr text . we propose the use of approximate string matching techniques to normalize names in order to overcome the problem . we show how we could achieve an improvement if we could tag names with reasonable accuracy in asr .

software requirements : a new domain for semantic parsers
software requirements are commonly written in natural language , making them prone to ambiguity , incompleteness and inconsistency . by converting requirements to formal semantic representations , emerging problems can be detected at an early stage of the development process , thus reducing the number of ensuing errors and the development costs . in this paper , we treat the mapping from requirements to formal representations as a semantic parsing task . we describe a novel data set for this task that involves two contributions : first , we establish an ontology for formally representing requirements ; and second , we introduce an iterative annotation scheme , in which formal representations are derived through step-wise refinements .

a character-net based chinese text segmentation method
the segmentation of chinese texts is a key process in chinese information processing . the difficulties in segmentation are the process of ambiguous character string and unknown chinese words . in order to obtain the correct result , the first is identification of all possible candidates of chinese words in a text . in this paper , a data structure chinese-character-net is put forward , then , based on this character-net , a new algorithm is presented to obtain all possible candidate of chinese words in a text . this paper gives the experiment result . finally the characteristics of the algorithm are analysed .

a new perceptron algorithm for
we can not use non-local features with current major methods of sequence labeling such as crfs due to concerns about complexity . we propose a new perceptron algorithm that can use non-local features . our algorithm allows the use of all types of non-local features whose values are determined from the sequence and the labels . the weights of local and non-local features are learned together in the training process with guaranteed convergence . we present experimental results from the conll 2003 named entity recognition ( ner ) task to demonstrate the performance of the proposed algorithm .

evaluating task performance for a unidirectional controlled language medical speech translation system beth ann hockey
we present a task-level evaluation of the french to english version of medslt , a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews . our main goal was to establish task performance levels of novice users and compare them to expert users . tests were carried out on eight medical students with no previous exposure to the system , with each student using the system for a total of three sessions . by the end of the third session , all the students were able to use the system confidently , with an average task completion time of about 4 minutes .

plaser : pronunciation learning via automatic speech recognition
plaser is a multimedia tool with instant feedback designed to teach english pronunciation for high-school students of hong kong whose mother tongue is cantonese chinese . the objective is to teach correct pronunciation and not to assess a students overall pronunciation quality . major challenges related to speech recognition technology include : allowance for non-native accent , reliable and corrective feedbacks , and visualization of errors . plaser employs hidden markov models to represent position-dependent english phonemes . they are discriminatively trained using the standard american english timit corpus together with a set of timit utterances collected from good local english speakers . there are two kinds of speaking exercises : minimal-pair exercises and word exercises . in the word exercises , plaser computes a confidence-based score for each phoneme of the given word , and paints each vowel or consonant segment in the word using a novel 3-color scheme to indicate their pronunciation accuracy . plaser was used by 900 students of grade 7 and 8 over a period of 23 months . about 80 % of the students said that they preferred using plaser over traditional english classes to learn pronunciation . a pronunciation test was also conducted before and after they used plaser .

understanding seed selection in bootstrapping
bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost . in bootstrapping , unlabeled instances can be harvested from the initial labeled seed set . the selected seed set affects accuracy , but how to select a good seed set is not yet clear . thus , an iterative seeding framework is proposed for bootstrapping to reduce its labeling cost . our framework iteratively selects the unlabeled instance that has the best goodness of seed and labels the unlabeled instance in the seed set . our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem . we propose a method called expected model rotation ( emr ) that works well on not well-separated data which frequently occur as realistic data . experimental results show that emr can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets .

template-based information extraction without the templates
standard algorithms for template-based information extraction ( ie ) require predefined template schemas , and often labeled data , to learn to extract their slot fillers ( e.g. , an embassy is the target of a bombing template ) . this paper describes an approach to template-based ie that removes this requirement and performs extraction without knowing the template structure in advance . our algorithm instead learns the template structure automatically from raw text , inducing template schemas as sets of linked events ( e.g. , bombings include detonate , set off , and destroy events ) associated with semantic roles . we also solve the standard ie task , using the induced syntactic patterns to extract role fillers from specific documents . we evaluate on the muc-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure , and we extract role fillers with an f1 score of .40 , approaching the performance of algorithms that require full knowledge of the templates .

using a maximum entropy model to build segmentation lattices for mt
recent work has shown that translating segmentation lattices ( lattices that encode alternative ways of breaking the input to an mt system into words ) , rather than text in any particular segmentation , improves translation quality of languages whose orthography does not mark morpheme boundaries . however , much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices . in this work , we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding . using a model optimized for german translation , we present results showing significant improvements in translation quality in german-english , hungarian-english , and turkish-english translation over state-ofthe-art baselines .

dont until the final verb wait : reinforcement learning for simultaneous machine translation
we introduce a reinforcement learningbased approach to simultaneous machine translationproducing a translation while receiving input words between languages with drastically different word orders : from verb-final languages ( e.g. , german ) to verb-medial languages ( english ) . in traditional machine translation , a translator must wait for source material to appear before translation begins . we remove this bottleneck by predicting the final verb in advance . we use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence . we also introduce an evaluation metric to measure expeditiousness and quality . we show that our new translation model outperforms batch and monotone translation strategies .

generalized expectation criteria for bootstrapping extractors using record-text alignment
traditionally , machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce . however , in many cases , there already exists a database ( db ) with schema related to the desired output , and records related to the expected input text . we present a conditional random field ( crf ) that aligns tokens of a given db record and its realization in text . the crf model is trained using only the available db and unlabeled text with generalized expectation criteria . an annotation of the text induced from inferred alignments is used to train an information extractor . we evaluate our method on a citation extraction task in which alignments between dblp database records and citation texts are used to train an extractor . experimental results demonstrate an error reduction of 35 % over a previous state-of-the-art method that uses heuristic alignments .

summarizing emails with conversational cohesion and subjectivity
in this paper , we study the problem of summarizing email conversations . we first build a sentence quotation graph that captures the conversation structure among emails . we adopt three cohesion measures : clue words , semantic similarity and cosine similarity as the weight of the edges . second , we use two graph-based summarization approaches , generalized cluewordsummarizer and pagerank , to extract sentences as summaries . third , we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones . the empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures . moreover , subjective words can significantly improve accuracy .

statistical machine translation of euparl data by using bilingual n-grams
this work discusses translation results for the four euparl data sets which were made available for the shared task exploiting parallel texts for statistical machine translation . all results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model .

kea : sentiment analysis of phrases within short texts
sentiment analysis has become an increasingly important research topic . this paper describes our approach to building a system for the sentiment analysis in twitter task of the semeval-2014 evaluation . the goal is to classify a phrase within a short piece of text as positive , negative or neutral . in the evaluation , classifiers trained on twitter data are tested on data from other domains such as sms , blogs as well as sarcasm . the results indicate that apart from sarcasm , classifiers built for sentiment analysis of phrases from tweets can be generalized to other short text domains quite effectively . however , in crossdomain experiments , sms data is found to generalize even better than twitter data .

evaluations of nlg systems : common corpus and tasks or common
in this position paper , we argue that a common task and corpus are not the only ways to evaluate natural language generation ( nlg ) systems . it might be , in fact , too narrow a view on evaluation and thus not be the best way to evaluate these systems . the aim of a common task and corpus is to allow for a comparative evaluation of systems , looking at the systems performances . it is thus a systemoriented view of evaluation . we argue here that , if we are to take a system oriented view of evaluation , the community might be better served by enlarging the view of evaluation , defining common dimensions and metrics to evaluate systems and approaches . we also argue that end-user ( or usability ) evaluations form another important aspect of a systems evaluation and should not be forgotten .

a multimodal interface for access to content in the home
in order to effectively access the rapidly increasing range of media content available in the home , new kinds of more natural interfaces are needed . in this paper , we explore the application of multimodal interface technologies to searching and browsing a database of movies . the resulting system allows users to access movies using speech , pen , remote control , and dynamic combinations of these modalities . an experimental evaluation , with more than 40 users , is presented contrasting two variants of the system : one combining speech with traditional remote control input and a second where the user has a tablet display supporting speech and pen input .

self-training for biomedical parsing
parser self-training is the technique of taking an existing parser , parsing extra data and then creating a second parser by treating the extra data as further training data . here we apply this technique to parser adaptation . in particular , we self-train the standard charniak/johnson penn-treebank parser using unlabeled biomedical abstracts . this achieves an f -score of 84.3 % on a standard test set of biomedical abstracts from the genia corpus . this is a 20 % error reduction over the best previous result on biomedical data ( 80.2 % on the same test set ) .

ju-evora : a graph based cross-level semantic similarity analysis using discourse information
text analytics using semantic information is the latest trend of research due to its potential to represent better the texts content compared with the bag-of-words approaches . on the contrary , representation of semantics through graphs has several advantages over the traditional representation of feature vector . therefore , error tolerant graph matching techniques can be used for text comparison . nevertheless , not many methodologies exist in the literature which expresses semantic representations through graphs . the present system is designed to deal with cross level semantic similarity analysis as proposed in the semeval-2014 : semantic evaluation , international workshop on semantic evaluation , dublin , ireland .

application of localized similarity for web documents
in this paper we present a novel approach to automatic creation of anchor texts for hyperlinks in a document pointing to similar documents . methods used in this approach rank parts of a document based on the similarity to a presumably related document . ranks are then used to automatically construct the best anchor text for a link inside original document to the compared document . a number of different methods from information retrieval and natural language processing are adapted for this task . automatically constructed anchor texts are manually evaluated in terms of relatedness to linked documents and compared to baseline consisting of originally inserted anchor texts . additionally we use crowdsourcing for evaluation of original anchors and automatically constructed anchors . results show that our best adapted methods rival the precision of the baseline method .

chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems
we introduce chinese whispers , a randomized graph-clustering algorithm , which is time-linear in the number of edges . after a detailed definition of the algorithm and a discussion of its strengths and weaknesses , the performance of chinese whispers is measured on natural language processing ( nlp ) problems as diverse as language separation , acquisition of syntactic word classes and word sense disambiguation . at this , the fact is employed that the small-world property holds for many graphs in nlp .

semi-supervised semantic pattern discovery with guidance from unsupervised pattern clusters
we present a simple algorithm for clustering semantic patterns based on distributional similarity and use cluster memberships to guide semi-supervised pattern discovery . we apply this approach to the task of relation extraction . the evaluation results demonstrate that our novel bootstrapping procedure significantly outperforms a standard bootstrapping . most importantly , our algorithm can effectively prevent semantic drift and provide semi-supervised learning with a natural stopping criterion .

a measure of term representativeness based on the number of co-occurring salient words
we propose a novel measure of the representativeness ( i.e. , indicativeness or topic specificity ) of a term in a given corpus . the measure embodies the idea that the distribution of words co-occurring with a representative term should be biased according to the word distribution in the whole corpus . the bias of the word distribution in the co-occurring words is defined as the number of distinct words whose occurrences are saliently biased in the co-occurring words . the saliency of a word is defined by a threshold probability that can be automatically defined using the whole corpus . comparative evaluation clarified that the measure is clearly superior to conventional measures in finding topic-specific words in the newspaper archives of different sizes .

the semantics of markup : mapping legacy markup schemas to a common
a method for mapping linguistic descriptions in plain xml into semantically rich rdf/owl is outlined and demonstrated . starting with simonss ( 2003 ) original proof of concept of this method , we extend his semantic interpretation language ( sil ) for creating metaschemas to carry out the mapping , employ the general ontology for linguistic description ( gold ) of farrar and langendoen ( 2003 ) as the target semantic schema , and make use of serql , an rdf-aware search engine . this data migration effort is in keeping with the vision of a semantic web ; it is part of an effort to build a community of practice around semantically rich linguistic resources .

morphological analysis and disambiguation for dialectal arabic
the many differences between dialectal arabic and modern standard arabic ( msa ) pose a challenge to the majority of arabic natural language processing tools , which are designed for msa . in this paper , we retarget an existing state-of-the-art msa morphological tagger to egyptian arabic ( arz ) . our evaluation demonstrates that our arz morphology tagger outperforms its msa variant on arz input in terms of accuracy in part-of-speech tagging , diacritization , lemmatization and tokenization ; and in terms of utility for arz-toenglish statistical machine translation .

disambiguating toponyms in news eric garbin inderjeet mani
this research is aimed at the problem of disambiguating toponyms ( place names ) in terms of a classification derived by merging information from two publicly available gazetteers . to establish the difficulty of the problem , we measured the degree of ambiguity , with respect to a gazetteer , for toponyms in news . we found that 67.82 % of the toponyms found in a corpus that were ambiguous in a gazetteer lacked a local discriminator in the text . given the scarcity of humanannotated data , our method used unsupervised machine learning to develop disambiguation rules . toponyms were automatically tagged with information about them found in a gazetteer . a toponym that was ambiguous in the gazetteer was automatically disambiguated based on preference heuristics . this automatically tagged data was used to train a machine learner , which disambiguated toponyms in a human-annotated news corpus at 78.5 % accuracy .

bayesian unsupervised word segmentation with nested pitman-yor language modeling daichi mochihashi takeshi yamada naonori ueda
in this paper , we propose a new bayesian model for fully unsupervised word segmentation and an efficient blocked gibbs sampler combined with dynamic programming for inference . our model is a nested hierarchical pitman-yor language model , where pitman-yor spelling model is embedded in the word model . we confirmed that it significantly outperforms previous reported results in both phonetic transcripts and standard datasets for chinese and japanese word segmentation . our model is also considered as a way to construct an accurate word n-gram language model directly from characters of arbitrary language , without any word indications .

bidirectional decoding for statistical machine translation taro watanabe and eiichiro sumita
this paper describes the right-to-left decoding method , which translates an input string by generating in right-to-left direction . in addition , presented is the bidirectional decoding method , that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions . the experimental results on japanese and english translation showed that the right-to-left was better for englith-to-japanese translation , while the left-to-right was suitable for japanese-to-english translation . it was also observed that the bidirectional method was better for english-to-japanese translation .

adaptive transformation-based learning for improving dictionary tagging
we present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components ( headwords , pronunciations , parts of speech , translations , etc . ) using an extremely small amount of user provided training data . we use transformationbased learning ( tbl ) as a postprocessor at two points in our system to improve performance . the results using two dictionaries show that the tagging accuracy increases from 83 % and 91 % to 93 % and 94 % for individual words or tokens , and from 64 % and 83 % to 90 % and 93 % for contiguous phrases such as definitions or examples of usage .

deriunlp : a context based approach to automatic keyphrase unit for natural language processing unit for natural language processing
the deri unlp team participated in the semeval 2010 task # 5 with an unsupervised system that automatically extracts keyphrases from scientific articles . our approach does not only consider a general description of a term to select keyphrase candidates but also context information in the form of skill types . even though our system analyses only a limited set of candidates , it is still able to outperform baseline unsupervised and supervised approaches .

automatic generation of related work sections in scientific papers : an optimization approach
in this paper , we investigate a challenging task of automatic related work generation . given multiple reference papers as input , the task aims to generate a related work section for a target paper . the generated related work section can be used as a draft for the author to complete his or her final related work section . we propose our automatic related work generation system called arwg to address this task . it first exploits a plsa model to split the sentence set of the given papers into different topic-biased parts , and then applies regression models to learn the importance of the sentences . at last it employs an optimization framework to generate the related work section . our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed arwg system can generate related work sections with better quality . a user study is also performed to show arwg can achieve an improvement over generic multi-document summarization baselines .

automatic identification of bengali noun-noun compounds using random forest vivekananda gayen kamal sarkar
this paper presents a supervised machine learning approach that uses a machine learning algorithm called random forest for recognition of bengali noun-noun compounds as multiword expression ( mwe ) from bengali corpus . our proposed approach to mwe recognition has two steps : ( 1 ) extraction of candidate multi-word expressions using chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi-word expression as multi-word expression or not . a variety of association measures , syntactic and linguistic clues are used as features for identifying mwes . the proposed system is tested on a bengali corpus for identifying noun-noun compound mwes from the corpus .

influence of target reader background and text features on text readability in bangla : a computational approach
in this paper , we have studied the effect of two important factors influencing text readability in bangla : the target reader and text properties . accordingly , at first we have built a novel bangla readability dataset of 135 documents annotated by 50 readers from two different backgrounds . we have identified 20 different features that can affect the readability of bangla texts ; the features were divided in two groups , namely , classic and non-classic . preliminary correlation analysis reveals that text features have varying influence on the text hardness stated by the two groups . we have employed support vector machine ( svm ) and support vector regression ( svr ) techniques to model the reading difficulties of bangla texts . in addition to developing different models targeted towards different type of readers , separate combinations of features were tested to evaluate their comparative contributions . our study establishes that the perception of text difficulty varies largely with the background of the reader . to the best of our knowledge , no such work on text readability has been recorded earlier in bangla .

question answering on a case insensitive corpus
most question answering ( qa ) systems rely on both keyword index and named entity ( ne ) tagging . the corpus from which the qa systems attempt to retrieve answers is usually mixed case text . however , there are numerous corpora that consist of case insensitive documents , e.g . speech recognition results . this paper presents a successful approach to qa on a case insensitive corpus , whereby a preprocessing module is designed to restore the case-sensitive form . the document pool with the restored case then feeds the qa system , which remains unchanged . the case restoration preprocessing is implemented as a hidden markov model trained on a large raw corpus of case sensitive documents . it is demonstrated that this approach leads to very limited degradation in qa benchmarking ( 2.8 % ) , mainly due to the limited degradation in the underlying information extraction support .

part-of-speech tagging of northern sotho : disambiguating polysemous function words gertrud faa ulrich heid elsabe taljard danie prinsloo
a major obstacle to part-of-speech ( =pos ) tagging of northern sotho ( bantu , s 32 ) are ambiguous function words . many are highly polysemous and very frequent in texts , and their local context is not always distinctive . with certain taggers , this issue leads to comparatively poor results ( between 88 and 92 % accuracy ) , especially when sizeable tagsets ( over 100 tags ) are used . we use the rf-tagger ( schmid and laws , 2008 ) , which is particularly designed for the annotation of fine-grained tagsets ( e.g . including agreement information ) , and we restructure the 141 tags of the tagset proposed by taljard et al ( 2008 ) in a way to fit the rf tagger . this leads to over 94 % accuracy . error analysis in addition shows which types of phenomena cause trouble in the pos-tagging of northern sotho .

simple but effective feedback generation to tutor abstract problem solving
to generate natural language feedback for an intelligent tutoring system , we developed a simple planning model with a distinguishing feature : its plan operators are derived automatically , on the basis of the association rules mined from our tutorial dialog corpus . automatically mined rules are also used for realization . we evaluated 5 different versions of a system that tutors on an abstract sequence learning task . the version that uses our planning framework is significantly more effective than the other four versions . we compared this version to the human tutors we employed in our tutorial dialogs , with intriguing results .

what to do about bad language on the internet
the rise of social media has brought computational linguistics in ever-closer contact with bad language : text that defies our expectations about vocabulary , spelling , and syntax . this paper surveys the landscape of bad language , and offers a critical review of the nlp communitys response , which has largely followed two paths : normalization and domain adaptation . each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication . in addition , the paper presents a quantitative analysis of the lexical diversity of social media text , and its relationship to other corpora .

improving arabic dependency parsing with lexical and inflectional morphological features
we explore the contribution of different lexical and inflectional morphological features to dependency parsing of arabic , a morphologically rich language . we experiment with all leading pos tagsets for arabic , and introduce a few new sets . we show that training the parser using a simple regular expressive extension of an impoverished pos tagset with high prediction accuracy does better than using a highly informative pos tagset with only medium prediction accuracy , although the latter performs best on gold input . using controlled experiments , we find that definiteness ( or determiner presence ) , the so-called phifeatures ( person , number , gender ) , and undiacritzed lemma are most helpful for arabic parsing on predicted input , while case and state are most helpful on gold .

efficient deep processing of japanese
we present a broad coverage japanese grammar written in the hpsg formalism with mrs semantics . the grammar is created for use in real world applications , such that robustness and performance issues play an important role . it is connected to a pos tagging and word segmentation tool . this grammar is being developed in a multilingual context , requiring mrs structures that are easily comparable across languages .

and morphological disambiguation in one fell swoop
we present an approach to using a morphological analyzer for tokenizing and morphologically tagging ( including partof-speech tagging ) arabic words in one process . we learn classifiers for individual morphological features , as well as ways of using these classifiers to choose among entries from the output of the analyzer . we obtain accuracy rates on all tasks in the high nineties .

a comparative study on generalization of semantic roles in framenet yuichiroh matsubayashi naoaki okazaki junichi tsujii
a number of studies have presented machine-learning approaches to semantic role labeling with availability of corpora such as framenet and propbank . these corpora define the semantic roles of predicates for each frame independently . thus , it is crucial for the machine-learning approach to generalize semantic roles across different frames , and to increase the size of training instances . this paper explores several criteria for generalizing semantic roles in framenet : role hierarchy , human-understandable descriptors of roles , semantic types of filler phrases , and mappings from framenet roles to thematic roles of verbnet . we also propose feature functions that naturally combine and weight these criteria , based on the training data . the experimental result of the role classification shows 19.16 % and 7.42 % improvements in error reduction rate and macro-averaged f1 score , respectively . we also provide in-depth analyses of the proposed criteria .

the effect of linguistic devices in information presentation messages on
in this paper we examine the effect of linguistic devices on recall and comprehension in information presentation using both recall and eye-tracking data . in addition , the results were validated via an experiment using amazons mechanical turk micro-task environment .

event role extraction using domain-relevant word representations
the efficiency of information extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high . in this article , we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the muc-4 data set .

chunk parsing revisited
chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use . in this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking . experimental results with the penn treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed ( 14 msec/sentence ) . we also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .

an empirical study on multiple lvcsr model combination by machine learning takehito utsuro yasuhiro kodama tomohiro watanabe hiromitsu nishizaki seiichi nakagawa
this paper proposes to apply machine learning techniques to the task of combining outputs of multiple lvcsr models . the proposed technique has advantages over that by voting schemes such as rover , especially when the majority of participating models are not reliable . in this machine learning framework , as features of machine learning , information such as the model ids which output the hypothesized word are useful for improving the word recognition rate . experimental results show that the combination results achieve a relative word error reduction of up to 39 % against the best performing single model and that of up to 23 % against rover . we further empirically show that it performs better when lvcsr models to be combined are chosen so as to cover as many correctly recognized words as possible , rather than choosing models in descending order of their word correct rates .

a comparable corpus approach
we present a study on linguistic contrast and commonality in english scientific discourse on the basis of a monolingually comparable corpus . the focus is on selected scientific disciplines at the boundaries to computer science ( computational linguistics , bioinformatics , digital construction , microelectronics ) . the data basis is the english scientific text corpus ( scitex ) which covers a time range of roughly thirty years ( 1970/80s to early 2000s ) . in particular , we investigate the disciplinary diversification/relatedness of scientific research articles in terms of register . our results are relevant for research on multilingually comparable corpora as used in machine translation and related research , since they shed new light on the notion of comparablity .

fast and robust multilingual dependency parsing with a generative latent variable model
we use a generative history-based model to predict the most likely derivation of a dependency parse . our probabilistic model is based on incremental sigmoid belief networks , a recently proposed class of latent variable models for structure prediction . their ability to automatically induce features results in multilingual parsing which is robust enough to achieve accuracy well above the average for each individual language in the multilingual track of the conll-2007 shared task . this robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods . we also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy .

stacking for statistical machine translation
we propose the use of stacking , an ensemble learning technique , to the statistical machine translation ( smt ) models . a diverse ensemble of weak learners is created using the same smt engine ( a hierarchical phrase-based system ) by manipulating the training data and a strong model is created by combining the weak models on-the-fly . experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 bleu points over a conventionally trained smt model .

using selectional profile distance to detect verb alternations
we propose a new method for detecting verb alternations , by comparing the probability distributions over wordnet classes occurring in two potentially alternating argument positions . existing distance measures compute only the distributional distance , and do not take into account the semantic similarity between wordnet senses across the distributions . our method compares two probability distributions over wordnet by measuring the semantic distance of the component nodes , weighted by their probability . to incorporate semantic similarity , we calculate the ( dis ) similarity between two probability distributions as a weighted distance travelled from one to the other through the wordnet hierarchy . we evaluate the measure on the causative alternation , and find that overall it outperforms existing distance measures .

unsupervised synthesis of multilingual wikipedia articles
in this paper , we propose an unsupervised approach to automatically synthesize wikipedia articles in multiple languages . taking an existing high-quality version of any entry as content guideline , we extract keywords from it and use the translated keywords to query the monolingual web of the target language . candidate excerpts or sentences are selected based on an iterative ranking function and eventually synthesized into a complete article that resembles the reference version closely . 16 english and chinese articles across 5 domains are evaluated to show that our algorithm is domainindependent . both subjective evaluations by native chinese readers and rouge-l scores computed with respect to standard reference articles demonstrate that synthesized articles outperform existing chinese versions or mt texts in both content richness and readability . in practice our method can generate prototype texts for wikipedia that facilitate later human authoring .

construction of an infrastructure for providing users with suitable language resources hitomi tohyama shunsuke kozawa kiyotaka uchimoto
our research organization has been constructing a large scale database named shachi by collecting detailed meta information on language resources ( lrs ) in asia and western countries . the metadata database contains more than 2,000 compiled lrs such as corpora , dictionaries , thesauruses and lexicons , forming a large scale metadata of lrs archive . its metadata , an extended version of olac metadata set conforming to dublin core , have been collected semi-automatically . this paper explains the design and the structure of the metadata database , as well as the realization of the catalogue search tool .

dutch word sense disambiguation : optimizing the localness of context
we describe a new version of the dutch word sense disambiguation system trained and tested on a corrected version of the senseval-2 data . the system is an ensemble of word experts ; each word expert is a memory-based classifier of which the parameters are automatically determined through cross-validation on training material . the original best-performing system , which used only local context features for disambiguation , is further refined by performing additional parallel crossvalidation experiments for optimizing algorithmic parameters and the amount of local context available to each of the word experts memory-based kernels . this procedure produces an accuracy of 84.8 % on test material , improving on a baseline score of 77.2 % and the previous senseval-2 score of 84.2 % . we show that cross-validation overfits ; had the local context been held constant at two left and right neighbouring words , the system would have scored 85.0 % .

morphological features help pos tagging of unknown words across
part-of-speech tagging , like any supervised statistical nlp task , is more difficult when test sets are very different from training sets , for example when tagging across genres or language varieties . we examined the problem of pos tagging of different varieties of mandarin chinese ( prc-mainland , prchong kong , and taiwan ) . an analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging . unknown words in english tend to be proper nouns . by contrast , we found that mandarin unknown words were mostly common nouns and verbs . we showed these results are caused by the high frequency of morphological compounding in mandarin ; in this sense mandarin is more like german than english . based on this analysis , we propose a variety of new morphological unknown-word features for pos tagging , extending earlier work by others on unknown-word tagging in english and german . our features were implemented in a maximum entropy markov model . our system achieves state-of-the-art performance in mandarin tagging , including improving unknown-word tagging performance on unseen varieties in chinese treebank 5.0 from 61 % to 80 % correct .

cross language text categorization using a bilingual lexicon
with the popularity of the internet at a phenomenal rate , an ever-increasing number of documents in languages other than english are available in the internet . cross language text categorization has attracted more and more attention for the organization of these heterogeneous document collections . in this paper , we focus on how to conduct effective cross language text categorization . to this end , we propose a cross language naive bayes algorithm . the preliminary experiments on collected document collections show the effectiveness of the proposed method and verify the feasibility of achieving performance close to monolingual text categorization , using a bilingual lexicon alone . also , our algorithm is more efficient than our baselines .

unsupervised resolution of objects and relations on the web
the task of identifying synonymous relations and objects , or synonym resolution ( sr ) , is critical for high-quality information extraction . the bulk of previous sr work assumed strong domain knowledge or hand-tagged training examples . this paper investigates sr in the context of unsupervised information extraction , where neither is available . the paper presents a scalable , fully-implemented system for sr that runs in o ( kn log n ) time in the number of extractions n and the maximum number of synonyms per word , k. the system , called resolver , introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them . given two million assertions extracted from the web , resolver resolves objects with 78 % precision and an estimated 68 % recall and resolves relations with 90 % precision and 35 % recall .

woz simulation of interactive question answering
qaciad ( question answering challenge for information access dialogue ) is an evaluation framework for measuring interactive question answering ( qa ) technologies . it assumes that users interactively collect information using a qa system for writing a report on a given topic and evaluates , among other things , the capabilities needed under such circumstances . this paper reports an experiment for examining the assumptions made by qaciad . in this experiment , dialogues under the situation that qaciad assumes are collected using woz ( wizard of oz ) simulating , which is frequently used for collecting dialogue data for designing speech dialogue systems , and then analyzed . the results indicate that the setting of qaciad is real and appropriate and that one of the important capabilities for future interactive qa systems is providing cooperative and helpful responses .

exploring content models for multi-document summarization
we present an exploration of generative probabilistic models for multi-document summarization . beginning with a simple word frequency based model ( nenkova and vanderwende , 2005 ) , we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting rouge gains along the way . our final model , hiersum , utilizes a hierarchical lda-style model ( blei et al , 2004 ) to represent content specificity as a hierarchy of topic vocabulary distributions . at the task of producing generic duc-style summaries , hiersum yields state-of-the-art rouge performance and in pairwise user evaluation strongly outperforms toutanova et al ( 2007 ) s state-of-the-art discriminative system . we also explore hiersums capacity to produce multiple topical summaries in order to facilitate content discovery and navigation .

creating a comparative dictionary of totonac-tepehua
we apply algorithms for the identification of cognates and recurrent sound correspondences proposed by kondrak ( 2002 ) to the totonac-tepehua family of indigenous languages in mexico . we show that by combining expert linguistic knowledge with computational analysis , it is possible to quickly identify a large number of cognate sets within the family . our objective is to provide tools for rapid construction of comparative dictionaries for relatively unfamiliar language families .

towards a unified approach for opinion question answering and
the aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization . following the guidelines tac 2008 opinion summarization pilot task , we propose new methods for each of the major components of the process . in particular , for the information retrieval , opinion mining and summarization stages . the performance obtained improves with respect to the state of the art by approximately 12.50 % , thus concluding that the suggested approaches for these three components are adequate .

better twitter summaries joel judd & jugal kalita
this paper describes an approach to improve summaries for a collection of twitter posts created using the phrase reinforcement ( pr ) algorithm ( sharifi et al , 2010a ) . the pr algorithm often generates summaries with excess text and noisy speech . we parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries . we compare the results to those obtained using the pr algorithm .

collecting a why-question corpus for development and evaluation of an joanna mrozinski edward whittaker
question answering research has only recently started to spread from short factoid questions to more complex ones . one significant challenge is the evaluation : manual evaluation is a difficult , time-consuming process and not applicable within efficient development of systems . automatic evaluation requires a corpus of questions and answers , a definition of what is a correct answer , and a way to compare the correct answers to automatic answers produced by a system . for this purpose we present a wikipedia-based corpus of whyquestions and corresponding answers and articles . the corpus was built by a novel method : paid participants were contacted through a web-interface , a procedure which allowed dynamic , fast and inexpensive development of data collection methods . each question in the corpus has several corresponding , partly overlapping answers , which is an asset when estimating the correctness of answers . in addition , the corpus contains information related to the corpus collection process . we believe this additional information can be used to post-process the data , and to develop an automatic approval system for further data collection projects conducted in a similar manner .

identifying the information structure of scientific abstracts : an
many practical tasks require accessing specific types of information in scientific literature ; e.g . information about the objective , methods , results or conclusions of the study in question . several schemes have been developed to characterize such information in full journal papers . yet many tasks focus on abstracts instead . we take three schemes of different type and granularity ( those based on section names , argumentative zones and conceptual structure of documents ) and investigate their applicability to biomedical abstracts . we show that even for the finest-grained of these schemes , the majority of categories appear in abstracts and can be identified relatively reliably using machine learning . we discuss the impact of our results and the need for subsequent task-based evaluation of the schemes .

coherence analysis : a lexicon
local coherence analysis is the task of deriving the ( most likely ) coherence relation holding between two elementary discourse units or , recursively , larger spans of text . the primary source of information for this step is the connectives provided by a language for , more or less explicitly , signaling the relations . focusing here on causal coherence relations , we propose a lexical resource that holds both lexicographic and corpusstatistic information on german connectives . it can serve as the central repository of information needed for identifying and disambiguating connectives in text , including determining the coherence relations being signaled . we sketch a procedure performing this task , and describe a manually-annotated corpus of causal relations ( also in german ) , which serves as reference data . 221 222 stede

subcategorization acquisition and evaluation for chinese verbs
this paper describes the technology and an experiment of subcategorization acquisition for chinese verbs . the scf hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods . evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches . besides , simple application of the acquired lexicon to a pcfg parser indicates great potentialities of subcategorization information in the fields of nlp . credits this research is sponsored by national natural science foundation ( grant no . 60373101 and 603750 19 ) , and high-tech research and development program ( grant no . 2002aa117010-09 ) .

word sense disambiguation using static and dynamic sense
it is popular in wsd to use contextual information in training sense tagged data . co-occurring words within a limited window-sized context support one sense among the semantically ambiguous ones of the word . this paper reports on word sense disambiguation of english words using static and dynamic sense vectors . first , context vectors are constructed using contextual words 1 in the training sense tagged data . then , the words in the context vector are weighted with local density . using the whole training sense tagged data , each sense of a target word2 is represented as a static sense vector in word space , which is the centroid of the context vectors . then contextual noise is removed using a automatic selective sampling . a automatic selective sampling method use information retrieval technique , so as to enhance the discriminative power . in each test case , a automatic selective sampling method retrieves n relevant training samples to reduce noise . using them , we construct another sense vectors for each sense of the target word .

two-stage method for large-scale acquisition of contradiction pattern pairs using entailment jong-hoon oh motoki sano kiyonori ohtake
in this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as xdrug prevents ydisease and ydisease caused by xdrug . in the first stage , we train an svm classifier to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity ( hashimoto et al , 2012 ) of the patterns . in the second stage , we enlarge the first stage classifiers training data with new contradiction pairs obtained by combining the output of the first stages classifier and that of an entailment classifier . we acquired this way 750,000 typed japanese contradiction pattern pairs with an estimated precision of 80 % . we plan to release this resource to the nlp community .

automatically building a tunisian lexicon
the sociolinguistic situation in arabic countries is characterized by diglossia ( ferguson , 1959 ) : whereas one variant modern standard arabic ( msa ) is highly codified and mainly used for written communication , other variants coexist in regular everydays situations ( dialects ) . similarly , while a number of resources and tools exist for msa ( lexica , annotated corpora , taggers , parsers . . . ) , very few are available for the development of dialectal natural language processing tools . taking advantage of the closeness of msa and its dialects , one way to solve the problem of the lack of resources for dialects consists in exploiting available msa resources and nlp tools in order to adapt them to process dialects . this paper adopts this general framework : we propose a method to build a lexicon of deverbal nouns for tunisian ( tun ) using msa tools and resources as starting material .

identifying correspondences between words : an approach based on a bilingual
we present a word alignment procedure based on a syntactic dependency analysis of french/english parallel corpora called alignment by syntactic propagation . both corpora are analysed with a deep and robust parser . starting with an anchor pair consisting of two words which are potential translations of one another within aligned sentences , the alignment link is propagated to the syntactically connected words . the method was tested on two corpora and achieved a precision of 94.3 and 93.1 % as well as a recall of 58 and 56 % , respectively for each corpus .

the necessity of combining adaptation methods
problems stemming from domain adaptation continue to plague the statistical natural language processing community . there has been continuing work trying to find general purpose algorithms to alleviate this problem . in this paper we argue that existing general purpose approaches usually only focus on one of two issues related to the difficulties faced by adaptation : 1 ) difference in base feature statistics or 2 ) task differences that can be detected with labeled data . we argue that it is necessary to combine these two classes of adaptation algorithms , using evidence collected through theoretical analysis and simulated and real-world data experiments . we find that the combined approach often outperforms the individual adaptation approaches . by combining simple approaches from each class of adaptation algorithm , we achieve state-of-the-art results for both named entity recognition adaptation task and the preposition sense disambiguation adaptation task . second , we also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data .

umcc_dlsi : textual similarity based on lexical-semantic features
this paper describes the specifications and results of umcc_dlsi system , which participated in the semantic textual similarity task ( sts ) of semeval-2013 . our supervised system uses different types of lexical and semantic features to train a bagging classifier used to decide the correct option . related to the different features we can highlight the resource isr-wn used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities . in order to establish which features are the most appropriate to improve sts results we participated with three runs using different set of features . our best run reached the position 44 in the official ranking , obtaining a general correlation coefficient of 0.61 .

unsupervised induction of natural language morphology inflection classes
we propose a novel language-independent framework for inducing a collection of morphological inflection classes from a monolingual corpus of full form words . our approach involves two main stages . in the first stage , we generate a large data structure of candidate inflection classes and their interrelationships . in the second stage , search and filtering techniques are applied to this data structure , to identify a select collection of `` true '' inflection classes of the language . we describe the basic methodology involved in both stages of our approach and present an evaluation of our baseline techniques applied to induction of major inflection classes of spanish . the preliminary results on an initial training corpus already surpass an f1 of 0.5 against ideal spanish inflectional morphology classes .

quantifying the influence of mt output in the translators performance : a case study in technical translation
this paper presents experiments on the use of machine translation output for technical translation . mt output was used to produced translation memories that were used with a commercial cat tool . our experiments investigate the impact of the use of different translation memories containing mt output in translations quality and speed compared to the same task without the use of translation memory . we evaluated the performance of 15 novice translators translating technical english texts into german . results suggest that translators are on average over 28 % faster when using tm .

preference grammars : softening syntactic constraints to improve statistical machine translation
we propose a novel probabilistic synchoronous context-free grammar formalism for statistical machine translation , in which syntactic nonterminal labels are represented as soft preferences rather than as hard matching constraints . this formalism allows us to efficiently score unlabeled synchronous derivations without forgoing traditional syntactic constraints . using this score as a feature in a log-linear model , we are able to approximate the selection of the most likely unlabeled derivation . this helps reduce fragmentation of probability across differently labeled derivations of the same translation . it also allows the importance of syntactic preferences to be learned alongside other features ( e.g. , the language model ) and for particular labeling procedures . we show improvements in translation quality on small and medium sized chinese-to-english translation tasks .

unsupervised cross-lingual lexical substitution
cross-lingual lexical substitution ( clls ) is the task that aims at providing for a target word in context , several alternative substitute words in another language . the proposed sets of translations may come from external resources or be extracted from textual data . in this paper , we apply for the first time an unsupervised cross-lingual wsd method to this task . the method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity . we evaluate the impact of using clustering information for clls by applying the wsd method to the semeval-2010 clls data set . our system performs better on the out-of-ten measure than the systems that participated in the semeval task , and is ranked medium on the other measures . we analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting .

unsupervised learning of narrative event chains
hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other nlp tasks requiring deep semantic knowledge . we propose unsupervised induction of similar schemata called narrative event chains from raw newswire text . a narrative event chain is a partially ordered set of events related by a common protagonist . we describe a three step process to learning narrative event chains . the first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments . the second applies a temporal classifier to partially order the connected events . finally , the third prunes and clusters self-contained chains from the space of events . we introduce two evaluations : the narrative cloze to evaluate event relatedness , and an order coherence task to evaluate narrative order . we show a 36 % improvement over baseline for narrative prediction and 25 % for temporal coherence .

vsem : an open library for visual semantics representation
vsem is an open library for visual semantics . starting from a collection of tagged images , it is possible to automatically construct an image-based representation of concepts by using off-theshelf vsem functionalities . vsem is entirely written in matlab and its objectoriented design allows a large flexibility and reusability . the software is accompanied by a website with supporting documentation and examples .

fine-grained genre classification using structural learning algorithms
prior use of machine learning in genre classification used a list of labels as classification categories . however , genre classes are often organised into hierarchies , e.g. , covering the subgenres of fiction . in this paper we present a method of using the hierarchy of labels to improve the classification accuracy . as a testbed for this approach we use the brown corpus as well as a range of other corpora , including the bnc , hgc and syracuse . the results are not encouraging : apart from the brown corpus , the improvements of our structural classifier over the flat one are not statistically significant . we discuss the relation between structural learning performance and the visual and distributional balance of the label hierarchy , suggesting that only balanced hierarchies might profit from structural learning .

structured databases of named entities from bayesian nonparametrics
we present a nonparametric bayesian approach to extract a structured database of entities from text . neither the number of entities nor the fields that characterize each entity are provided in advance ; the only supervision is a set of five prototype examples . our method jointly accomplishes three tasks : ( i ) identifying a set of canonical entities , ( ii ) inferring a schema for the fields that describe each entity , and ( iii ) matching entities to their references in raw text . empirical evaluation shows that the approach learns an accurate database of entities and a sensible model of name structure .

how to train your multi bottom-up tree transducer
the local multi bottom-up tree transducer is introduced and related to the ( non-contiguous ) synchronous tree sequence substitution grammar . it is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus . finally , the problem of non-preservation of regularity is addressed . three properties that ensure preservation are introduced , and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled .

automatic scoring of children 's read-aloud text passages and educational testing service
assessment of reading proficiency is typically done by asking subjects to read a text passage silently and then answer questions related to the text . an alternate approach , measuring reading-aloud proficiency , has been shown to correlate well with the aforementioned common method and is used as a paradigm in this paper . we describe a system that is able to automatically score two types of childrens read speech samples ( text passages and word lists ) , using automatic speech recognition and the target criterion correctly read words per minute . its performance is dependent on the data type ( passages vs. word lists ) as well as on the relative difficulty of passages or words for individual readers . pearson correlations with human assigned scores are around 0.86 for passages and around 0.80 for word lists .

whats with the attitude identifying sentences with attitude in online ahmed hassan vahed qazvinian
mining sentiment from user generated content is a very important task in natural language processing . an example of such content is threaded discussions which act as a very important tool for communication and collaboration in the web . threaded discussions include e-mails , e-mail lists , bulletin boards , newsgroups , and internet forums . most of the work on sentiment analysis has been centered around finding the sentiment toward products or topics . in this work , we present a method to identify the attitude of participants in an online discussion toward one another . this would enable us to build a signed network representation of participant interaction where every edge has a sign that indicates whether the interaction is positive or negative . this is different from most of the research on social networks that has focused almost exclusively on positive links . the method is experimentally tested using a manually labeled set of discussion posts . the results show that the proposed method is capable of identifying attitudinal sentences , and their signs , with high accuracy and that it outperforms several other baselines .

single-document summarization as a tree knapsack problem tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda masaaki nagata
recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a knapsack problem , a maximum coverage problem or a budgeted median problem . these methods successfully improved summarization quality , but they did not consider the rhetorical relations between the textual units of a source document . thus , summaries generated by these methods may lack logical coherence . this paper proposes a single document summarization method based on the trimming of a discourse tree . this is a two-fold process . first , we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree , which allows us to take a treetrimming approach to summarization . second , we formulate the problem of trimming a dependency-based discourse tree as a tree knapsack problem , then solve it with integer linear programming ( ilp ) . evaluation results showed that our method improved rouge scores .

grasp : grammar- and syntax-based pattern-finder in call
we introduce a method for learning to describe the attendant contexts of a given query for language learning . in our approach , we display phraseological information in the form of a summary of general patterns as well as lexical bundles anchored at the query . the method involves syntactical analyses and inverted file construction . at run-time , grammatical constructions and their lexical instantiations characterizing the usage of the given query are generated and displayed , aimed at improving learners deep vocabulary knowledge . we present a prototype system , grasp , that applies the proposed method for enhanced collocation learning . preliminary experiments show that language learners benefit more from grasp than conventional dictionary lookup . in addition , the information produced by grasp is potentially useful information for automatic or manual editing process .

towards strict sentence intersection : decoding and evaluation strategies
we examine the task of strict sentence intersection : a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more . our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice ; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disfluency in interleaving text segments . in addition , we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections . our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach .

learning to shift the polarity of words for sentiment classification daisuke ikeda hiroya takamura lev-arie ratinov manabu okumura
we propose a machine learning based method of sentiment classification of sentences using word-level polarity . the polarities of words in a sentence are not always the same as that of the sentence , because there can be polarity-shifters such as negation expressions . the proposed method models the polarity-shifters . our model can be trained in two different ways : word-wise and sentence-wise learning . in sentence-wise learning , the model can be trained so that the prediction of sentence polarities should be accurate . the model can also be combined with features used in previous work such as bag-of-words and n-grams . we empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data .

learning dense models of query similarity from user click logs
the goal of this work is to integrate query similarity metrics as features into a dense model that can be trained on large amounts of query log data , in order to rank query rewrites . we propose features that incorporate various notions of syntactic and semantic similarity in a generalized edit distance framework . we use the implicit feedback of user clicks on search results as weak labels in training linear ranking models on large data sets . we optimize different ranking objectives in a stochastic gradient descent framework . our experiments show that a pairwise svm ranker trained on multipartite rank levels outperforms other pairwise and listwise ranking methods under a variety of evaluation metrics .

using bilingual information for cross-language document
cross-language document summarization is defined as the task of producing a summary in a target language ( e.g . chinese ) for a set of documents in a source language ( e.g . english ) . existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language . in this study , we propose to use the bilingual information from both the source and translated documents for this task . two summarization methods ( simfusion and corank ) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction . experimental results on the duc2001 dataset with manually translated reference chinese summaries show the effectiveness of the proposed methods .

semantic language models for
in this work , we present a new semantic language modeling approach to model news stories in the topic detection and tracking ( tdt ) task . in the new approach , we build a unigram language model for each semantic class in a news story . we also cast the link detection subtask of tdt as a two-class classification problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class . we then compute a linear discriminant classifier using the perceptron learning algorithm on the training set . results on the test set show a marginal improvement over the unigram performance , but are not very encouraging on the whole .

the vocal joystick : a voice-based human-computer interface for individuals with motor impairments
we present a novel voice-based humancomputer interface designed to enable individuals with motor impairments to use vocal parameters for continuous control tasks . since discrete spoken commands are ill-suited to such tasks , our interface exploits a large set of continuous acousticphonetic parameters like pitch , loudness , vowel quality , etc . their selection is optimized with respect to automatic recognizability , communication bandwidth , learnability , suitability , and ease of use . parameters are extracted in real time , transformed via adaptation and acceleration , and converted into continuous control signals . this paper describes the basic engine , prototype applications ( in particular , voice-based web browsing and a controlled trajectory-following task ) , and initial user studies confirming the feasibility of this technology .

sielers : feature analysis and polarity classification of expressions from
in this paper , we describe our system for the semeval-2013 task 2 , sentiment analysis in twitter . we formed features that take into account the context of the expression and take a supervised approach towards subjectivity and polarity classification . experiments were performed on the features to find out whether they were more suited for subjectivity or polarity classification . we tested our model for sentiment polarity classification on twitter as well as sms chat expressions , analyzed their f-measure scores and drew some interesting conclusions from them .

ot syntax : decidability of generation-based optimization
in optimality-theoretic syntax , optimization with unrestricted expressive power on the side of the ot constraints is undecidable . this paper provides a proof for the decidability of optimization based on constraints expressed with reference to local subtrees ( which is in the spirit of ot theory ) . the proof builds on kaplan and wedekinds ( 2000 ) construction showing that lfg generation produces contextfree languages .

a person in the interface : effects on user perceptions of multilinguism & speech technology
in this paper we explore the possibilities that conversational agent technology offers for the improvement of the quality of human-machine interaction in a concrete area of application : the multimodal biometric authentication system . our approach looks at the user perception effects related to the system interface rather than to the performance of the biometric technology itself . for this purpose we have created a multibiometric user test environment with two different interfaces or interaction metaphors : one with an embodied conversational agent and the other with on-screen text messages only . we present the results of an exploratory experiment that reveals interesting effects , related to the presence of a conversational agent , on the users perception of parameters such as privacy , ease of use , invasiveness or system security .

automatic essay grading with probabilistic latent semantic analysis
probabilistic latent semantic analysis ( plsa ) is an information retrieval technique proposed to improve the problems found in latent semantic analysis ( lsa ) . we have applied both lsa and plsa in our system for grading essays written in finnish , called automatic essay assessor ( aea ) . we report the results comparing plsa and lsa with three essay sets from various subjects . the methods were found to be almost equal in the accuracy measured by spearman correlation between the grades given by the system and a human . furthermore , we propose methods for improving the usage of plsa in essay grading .

using an isu-based approach to incremental dialogue management
when dialogue systems , through the use of incremental processing , are not bounded anymore by strict , nonoverlapping turn-taking , a whole range of additional interactional devices becomes available . we explore the use of one such device , trial intonation . we elaborate our approach to dialogue management in incremental systems , based on the information-state-update approach , and discuss an implementation in a microdomain that lends itself to the use of immediate feedback , trial intonations and expansions . in an overhearer evaluation , the incremental system was judged as significantly more human-like and reactive than a non-incremental version .

automatic selection of high quality parses created by a fully
the average results obtained by unsupervised statistical parsers have greatly improved in the last few years , but on many specific sentences they are of rather low quality . the output of such parsers is becoming valuable for various applications , and it is radically less expensive to create than manually annotated training data . hence , automatic selection of high quality parses created by unsupervised parsers is an important problem . in this paper we present pupa , a pos-based unsupervised parse assessment algorithm . the algorithm assesses the quality of a parse tree using pos sequence statistics collected from a batch of parsed sentences . we evaluate the algorithm by using an unsupervised pos tagger and an unsupervised parser , selecting high quality parsed sentences from english ( wsj ) and german ( negra ) corpora . we show that pupa outperforms the leading previous parse assessment algorithm for supervised parsers , as well as a strong unsupervised baseline . consequently , pupa allows obtaining high quality parses without any human involvement .

a cascaded classification approach to semantic head recognition lukas michelbacher alok kothari martin forst christina lioma hinrich schutze
most nlp systems use tokenization as part of preprocessing . generally , tokenizers are based on simple heuristics and do not recognize multi-word units ( mwus ) like hot dog or black hole unless a precompiled list of mwus is available . in this paper , we propose a new cascaded model for detecting mwus of arbitrary length for tokenization , focusing on noun phrases in the physics domain . we adopt a classification approach because unlike other work on mwus tokenization requires a completely automatic approach . we achieve an accuracy of 68 % for recognizing non-compositional mwus and show that our mwu recognizer improves retrieval performance when used as part of an information retrieval system .

towards context sensitive lexical semantics
this paper introduces phrasenet , a contextsensitive lexical semantic knowledge base system . based on the supposition that semantic proximity is not simply a relation between two words in isolation , but rather a relation between them in their context , english nouns and verbs , along with contexts they appear in , are organized in phrasenet into consets ; consets capture the underlying lexical concept , and are connected with several semantic relations that respect contextually sensitive lexical information . phrasenet makes use of wordnet as an important knowledge source . it enhances a wordnet synset with its contextual information and refines its relational structure by maintaining only those relations that respect contextual constraints . the contextual information allows for supporting more functionalities compared with those of wordnet . natural language researchers as well as linguists and language learners can gain from accessing phrasenet with a word token and its context , to retrieve relevant semantic information . we describe the design and construction of phrasenet and give preliminary experimental evidence to its usefulness for nlp researches .

machine translation with grammar association : some improvements and the loco c model
grammar association is a technique for machine translation and language understanding introduced in 1993 by vidal , pieraccini and levin . all the statistical and structural models involved in the translation process are automatically built from bilingual examples , and the optimal translation of new sentences can be efficiently found by dynamic programming algorithms . this paper presents and discusses grammar association state of the art , including a new statistical model : loco c .

quantitative analysis of treebanks using frequent subtree mining methods centrum voor computerlingustiek , ku leuven
the first task of statistical computational linguistics , or any other type of datadriven processing of language , is the extraction of counts and distributions of phenomena . this is much more difficult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts . recent developments in data mining , particularly in the extraction of frequent subtrees from treebanks , offer some solutions . we have applied a modified version of the treeminer algorithm to a small treebank and present some promising results .

a text-based search interface for multimedia dialectics
the growing popularity of multimedia documents requires language technologies to approach automatic language analysis and generation from yet another perspective : that of its use in multimodal communication . in this paper , we present a support tool for cosmoroe , a theoretical framework for modelling multimedia dialectics . the tool is a text-based search interface that facilitates the exploration of a corpus of audiovisual files , annotated with the cosmoroe relations .

deep neural network approach for the dialog state tracking challenge
while belief tracking is known to be important in allowing statistical dialog systems to manage dialogs in a highly robust manner , until recently little attention has been given to analysing the behaviour of belief tracking techniques . the dialogue state tracking challenge has allowed for such an analysis , comparing multiple belief tracking approaches on a shared task . recent success in using deep learning for speech research motivates the deep neural network approach presented here . the model parameters can be learnt by directly maximising the likelihood of the training data . the paper explores some aspects of the training , and the resulting tracker is found to perform competitively , particularly on a corpus of dialogs from a system not found in the training .

learning to merge word senses rion snow sushant prakash
it has been widely observed that different nlp applications require different sense granularities in order to best exploit word sense distinctions , and that for many applications wordnet senses are too fine-grained . in contrast to previously proposed automatic methods for sense clustering , we formulate sense merging as a supervised learning problem , exploiting human-labeled sense clusterings as training data . we train a discriminative classifier over a wide variety of features derived from wordnet structure , corpus-based evidence , and evidence from other lexical resources . our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments , yielding an absolute f-score improvement of 4.1 % on nouns , 13.6 % on verbs , and 4.0 % on adjectives . finally , we propose a model for clustering sense taxonomies using the outputs of our classifier , and we make available several automatically sense-clustered wordnets of various sense granularities .

using reinforcement learning to create communication channel management strategies for diverse users
spoken dialogue systems typically do not manage the communication channel , instead using fixed values for such features as the amplitude and speaking rate . yet , the quality of a dialogue can be compromised if the user has difficulty understanding the system . in this proof-of-concept research , we explore using reinforcement learning ( rl ) to create policies that manage the communication channel to meet the needs of diverse users . towards this end , we first formalize a preliminary communication channel model , in which users provide explicit feedback regarding issues with the communication channel , and the system implicitly alters its amplitude to accommodate the users optimal volume . second , we explore whether rl is an appropriate tool for creating communication channel management strategies , comparing two different hand-crafted policies to policies trained using both a dialogue-length and a novel annoyance cost . the learned policies performed better than hand-crafted policies , with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a users optimal amplitude against dialoguelength . these results suggest that rl can be used to create effective communication channel management policies for diverse users . index terms : communication channel , spoken dialogue systems , reinforcement learning , amplitude , diverse users

millstream systems a formal model for linking language modules by interfaces
we introduce millstream systems , a formal model consisting of modules and an interface , where the modules formalise different aspects of language , and the interface links these aspects with each other .

forest-to-string statistical translation
in this paper , we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models . a forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string . to integrate these rules into tree-to-string translation models , auxiliary rules are introduced to provide a generalization level . experimental results show that , on the nist 2005 chinese-english test set , the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3 % in terms of bleu score over the original model which allows treeto-string rules only .

manual and automatic evaluation of machine translation between european languages
we evaluated machine translation performance for six european language pairs that participated in a shared task : translating french , german , spanish texts to english and back . evaluation was done automatically using the bleu score and manually on fluency and adequacy .

statistical machine translation of texts with misspelled words
this paper investigates the impact of misspelled words in statistical machine translation and proposes an extension of the translation engine for handling misspellings . the enhanced system decodes a word-based confusion network representing spelling variations of the input text . we present extensive experimental results on two translation tasks of increasing complexity which show how misspellings of different types do affect performance of a statistical machine translation decoder and to what extent our enhanced system is able to recover from such errors .

learning image embeddings using convolutional neural networks for improved multi-modal semantics
we construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( cnn ) trained on a large labeled object recognition dataset . this transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach . experimental results are reported on the wordsim353 and men semantic relatedness evaluation tasks . we use visual features computed using either imagenet or esp game images .

learning a lexical simplifier using wikipedia
in this paper we introduce a new lexical simplification approach . we extract over 30k candidate lexical simplifications by identifying aligned words in a sentencealigned corpus of english wikipedia with simple english wikipedia . to apply these rules , we learn a feature-based ranker using svm rank trained on a set of labeled simplifications collected using amazons mechanical turk . using human simplifications for evaluation , we achieve a precision of 76 % with changes in 86 % of the examples .

lingsync & the online linguistic database : new models for the collection and management of data for language
lingsync and the online linguistic database ( old ) are new models for the collection and management of data in endangered language settings . the lingsync and old projects seek to close a feedback loop between field linguists , language communities , software developers , and computational linguists by creating web services and user interfaces ( uis ) which facilitate collaborative and inclusive language documentation . this paper presents the architectures of these tools and the resources generated thus far . we also briefly discuss some of the features of the systems which are particularly helpful to endangered languages fieldwork and which should also be of interest to computational linguists , these being a service that automates the identification of utterances within audio/video , another that automates the alignment of audio recordings and transcriptions , and a number of services that automate the morphological parsing task . the paper discusses the requirements of software used for endangered language documentation , and presents novel data which demonstrates that users are actively seeking alternatives despite existing software .

identifying perspectives at the document and sentence levels using
in this paper we investigate the problem of identifying the perspective from which a document was written . by perspective we mean a point of view , for example , from the perspective of democrats or republicans . can computers learn to identify the perspective of a document furthermore , can computers identify which sentences in a document strongly convey a particular perspective we develop statistical models to capture how perspectives are expressed at the document and sentence levels , and evaluate the proposed models on a collection of articles on the israeli-palestinian conflict . the results show that the statistical models can successfully learn how perspectives are reflected in word usage and identify the perspective of a document with very high accuracy .

structured prediction models via the matrix-tree theorem
this paper provides an algorithmic framework for learning statistical models involving directed spanning trees , or equivalently non-projective dependency structures . we show how partition functions and marginals for directed spanning trees can be computed by an adaptation of kirchhoffs matrix-tree theorem . to demonstrate an application of the method , we perform experiments which use the algorithm in training both log-linear and max-margin dependency parsers . the new training methods give improvements in accuracy over perceptron-trained models .

towards terascale knowledge acquisition
although vast amounts of textual data are freely available , many nlp algorithms exploit only a minute percentage of it . in this paper , we study the challenges of working at the terascale . we present an algorithm , designed for the terascale , for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method . we focus on the accuracy of these two systems as a function of processing time and corpus size .

building a sense tagged corpus with open mind word expert
open mind word expert is an implemented active learning system for collecting word sense tagging from the general public over the web . it is available at http : //teach-computers.org . we expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers . we thus propose a senseval-3 lexical sample activity where the training data is collected via open mind word expert . if successful , the collection process can be extended to create the definitive corpus of word sense information .

an efficient algorithm for easy-first non-directional dependency parsing
we present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner . traditional deterministic parsing algorithms are based on a shift-reduce framework : they traverse the sentence from left-to-right and , at each step , perform one of a possible set of actions , until a complete tree is built . a drawback of this approach is that it is extremely local : while decisions can be based on complex structures on the left , they can look only at a few words to the right . in contrast , our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step . this allows incorporation of features from already built structures both to the left and to the right of the attachment point . the parser learns both the attachment preferences and the order in which they should be performed . the result is a deterministic , best-first , o ( nlogn ) parser , which is significantly more accurate than best-first transition based parsers , and nears the performance of globally optimized parsing models .

an analysis of verbs in financial news articles and their impact on
article terms can move stock prices . by analyzing verbs in financial news articles and coupling their usage with a discrete machine learning algorithm tied to stock price movement , we can build a model of price movement based upon the verbs used , to not only identify those terms that can move a stock price the most , but also whether they move the predicted price up or down .

iterative constrained clustering for subjectivity word sense
subjectivity word sense disambiguation ( swsd ) is a supervised and applicationspecific word sense disambiguation task disambiguating between subjective and objective senses of a word . not surprisingly , swsd suffers from the knowledge acquisition bottleneck . in this work , we use a cluster and label strategy to generate labeled data for swsd semiautomatically . we define a new algorithm called iterative constrained clustering ( icc ) to improve the clustering purity and , as a result , the quality of the generated data . our experiments show that the swsd classifiers trained on the icc generated data by requiring only 59 % of the labels can achieve the same performance as the classifiers trained on the full dataset .

hebrew dependency parsing : initial results
we describe a newly available hebrew dependency treebank , which is extracted from the hebrew ( constituency ) treebank . we establish some baseline unlabeled dependency parsing performance on hebrew , based on two state-of-the-art parsers , mst-parser and maltparser . the evaluation is performed both in an artificial setting , in which the data is assumed to be properly morphologically segmented and pos-tagged , and in a real-world setting , in which the parsing is performed on automatically segmented and pos-tagged text . we present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data . results indicate that ( a ) mst-parser performs better on hebrew data than maltparser , and ( b ) both parsers do not make good use of morphological information when parsing hebrew .

dialogue tagsets in oncology mary mcgee wood
dialogue analysis is widely used in oncology for training health professionals in communication skills . parameters and tagsets have been developed independently of work in natural language processing . in relation to emergent standards in nlp , syntactic tagging is minimal , semantics is domain-specific , pragmatics is comparable , and the analysis of cognitive affect is richly developed . we suggest productive directions for convergence .

using knowledge to facilitate factoid answer pinpointing
in order to answer factoid questions , the webclopedia qa system employs a range of knowledge resources . these include a qa typology with answer patterns , wordnet , information about typical numerical answer ranges , and semantic relations identified by a robust parser , to filter out likely-looking but wrong candidate answers . this paper describes the knowledge resources and their impact on system performance .

incorporating user models in question answering to improve readability
most question answering and information retrieval systems are insensitive to different users needs and preferences , as well as their reading level . in ( quarteroni and manandhar , 2006 ) , we introduce a hybrid qa-ir system based on a a user model . in this paper we focus on how the system filters and re-ranks the search engine results for a query according to their reading difficulty , providing user-tailored answers .

a model for fine-grained alignment of multilingual texts
while alignment of texts on the sentential level is often seen as being too coarse , and word alignment as being too fine-grained , bi- or multilingual texts which are aligned on a level inbetween are a useful resource for many purposes . starting from a number of examples of non-literal translations , which tend to make alignment difficult , we describe an alignment model which copes with these cases by explicitly coding them . the model is based on predicateargument structures and thus covers the middle ground between sentence and word alignment . the model is currently used in a recently initiated project of a parallel english-german treebank ( fuse ) , which can in principle be extended with additional languages .

probabilistic parsing strategies
we present new results on the relation between context-free parsing strategies and their probabilistic counter-parts . we provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies . these results generalize existing results in the literature that were obtained by considering parsing strategies in isolation .

automatically building training examples for entity extraction
in this paper we present methods for automatically acquiring training examples for the task of entity extraction . experimental evidence show that : ( 1 ) our methods compete with a current heavily supervised state-of-the-art system , within 0.04 absolute mean average precision ; and ( 2 ) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision .

partially distribution-free learning of regular languages
regular languages are widely used in nlp today in spite of their shortcomings . efficient algorithms that can reliably learn these languages , and which must in realistic applications only use positive samples , are necessary . these languages are not learnable under traditional distribution free criteria . we claim that an appropriate learning framework is pac learning where the distributions are constrained to be generated by a class of stochastic automata with support equal to the target concept . we discuss how this is related to other learning paradigms . we then present a simple learning algorithm for regular languages , and a self-contained proof that it learns according to this partially distribution free criterion .

automatically detecting and attributing indirect quotations
direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy . however , simply focusing on direct quotations ignores around half of all reported speech , which is in the form of indirect or mixed speech . this work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution . we propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora , one of which is a contribution of this work . we further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution .

poly-co : an unsupervised co-reference detection system
we describe our contribution to the generation challenge 2010 for the tasks of named entity recognition and coreference detection ( grec-ner ) . to extract the ne and the referring expressions , we employ a combination of a part of speech tagger and the conditional random fields ( crf ) learning technique . we finally experiment an original algorithm to detect co-references . we conclude with discussion about our system performances .

broadcast audio and video bimodal corpus exploitation and
the main purpose of this paper is the exploitation and application of an audio and video bimodal corpus of the chinese language in broadcasting . it deals with the designation of the size and structure of speech samples according to radio and television program features . secondly , it discusses annotation method of broadcast speech with achievements made and suggested future improvements . finally , it presents an attempt to describe the distribution of annotated items in our corpus .

using emoticons to reduce dependency in machine learning techniques for sentiment classification
sentiment classification seeks to identify a piece of text according to its authors general feeling toward their subject , be it positive or negative . traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic . this paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with emoticons , which has the potential of being independent of domain , topic and time .

exploiting lexical resources for therapeutic purposes
in this paper , we present an on-going project aiming at extending the wordnet lexical database by encoding common sense featural knowledge elicited from language speakers . such extension of wordnet is required in the framework of the stars.sys project , which has the goal of building tools for supporting the speech therapist during the preparation of exercises to be submitted to aphasic patients for rehabilitation purposes . we review some preliminary results and illustrate what extensions of the existing wordnet model are needed to accommodate for the encoding of commonsense ( featural ) knowledge .

bengali , hindi and telugu to english ad-hoc bilingual task
this paper presents the experiments carried out at jadavpur university as part of participation in the clef 2007 ad-hoc bilingual task . this is our first participation in the clef evaluation task and we have considered bengali , hindi and telugu as query languages for the retrieval from english document collection . we have discussed our bengali , hindi and telugu to english clir system as part of the ad-hoc bilingual task , english ir system for the ad-hoc monolingual task and the associated experiments at clef . query construction was manual for telugu-english ad-hoc bilingual task , while it was automatic for all other tasks .

incremental adaptation of speech-to-speech translation
in building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data . as with all speech systems , it is important to allow the system to adapt to the actual usage situations . this paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two . the platform is the cmu iraqi-english portable two-way speechto-speech system as developed under the darpa transtac program . we show how machine translation , speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way .

improving sparse word similarity models with asymmetric measures
we show that asymmetric models based on tversky ( 1977 ) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words . in accord with tverskys discovery that asymmetric similarity judgments arise when comparing sparse and rich representations , improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high- and midfrequency words .

soat : a semi-automatic domain ontology acquisition tool from chinese corpus
in this paper , we focus on the domain ontology acquisition from chinese corpus by extracting rules designed for chinese phrases . these rules are noun sequences with part-of-speech tags . experiments show that this process can construct domain ontology prototypes efficiently and effectively .

antecedent selection techniques for high-recall coreference resolution
we investigate methods to improve the recall in coreference resolution by also trying to resolve those definite descriptions where no earlier mention of the referent shares the same lexical head ( coreferent bridging ) . the problem , which is notably harder than identifying coreference relations among mentions which have the same lexical head , has been tackled with several rather different approaches , and we attempt to provide a meaningful classification along with a quantitative comparison . based on the different merits of the methods , we discuss possibilities to improve them and show how they can be effectively combined .

in a mobile environment choong-nyoung seon harksoo kim jungyun seo
we propose an information extraction system that is designed for mobile devices with low hardware resources . the proposed system extracts temporal instances ( dates and times ) and named instances ( locations and topics ) from korean short messages in an appointment management domain . to efficiently extract temporal instances with limited numbers of surface forms , the proposed system uses wellrefined finite state automata . to effectively extract various surface forms of named instances with low hardware resources , the proposed system uses a modified hmm based on syllable n-grams . in the experiment on instance boundary labeling , the proposed system showed better performances than traditional classifiers .

extracting key phrases to disambiguate personal name queries in web search danushka bollegala yutaka matsuo
assume that you are looking for information about a particular person . a search engine returns many pages for that persons name . some of these pages may be on other people with the same name . one method to reduce the ambiguity in the query and filter out the irrelevant pages , is by adding a phrase that uniquely identifies the person we are interested in from his/her namesakes . we propose an unsupervised algorithm that extracts such phrases from the web . we represent each document by a term-entity model and cluster the documents using a contextual similarity metric . we evaluate the algorithm on a dataset of ambiguous names . our method outperforms baselines , achieving over 80 % accuracy and significantly reduces the ambiguity in a web search task .

modelling irony in twitter
computational creativity is one of the central research topics of artificial intelligence and natural language processing today . irony , a creative use of language , has received very little attention from the computational linguistics research point of view . in this study we investigate the automatic detection of irony casting it as a classification problem . we propose a model capable of detecting irony in the social network twitter . in cross-domain classification experiments our model based on lexical features outperforms a word-based baseline previously used in opinion mining and achieves state-of-the-art performance . our features are simple to implement making the approach easily replicable .

using the web for language independent spellchecking and casey whitelaw and ben hutchinson and grace y chung and gerard ellis
we have designed , implemented and evaluated an end-to-end system spellchecking and autocorrection system that does not require any manually annotated training data . the world wide web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage . this is used to build an error model and an n-gram language model . a small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers . because no manual annotation is required , our system can easily be instantiated for new languages . when evaluated on human typed data with real misspellings in english and german , our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries . our system achieves 3.8 % total error rate in english . we show similar improvements in preliminary results on artificial data for russian and arabic .

feature embedding for dependency parsing
in this paper , we propose an approach to automatically learning feature embeddings to address the feature sparseness problem for dependency parsing . inspired by word embeddings , feature embeddings are distributed representations of features that are learned from large amounts of auto-parsed data . our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features . based on feature embeddings , we present a set of new features for graph-based dependency parsing models . experiments on the standard chinese and english data sets show that the new parser achieves significant performance improvements over a strong baseline .

a joint model of text and aspect ratings for sentiment summarization
online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects . we propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspect-based sentiment summarization ( hu and liu , 2004a ) . our model achieves high accuracy , without any explicitly labeled data except the user provided opinion ratings . the proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals .

a formal characterization of parsing word alignments by synchronous grammars with empirical evidence to the itg hypothesis
deciding whether a synchronous grammar formalism generates a given word alignment ( the alignment coverage problem ) depends on finding an adequate instance grammar and then using it to parse the word alignment . but what does it mean to parse a word alignment by a synchronous grammar this is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments . this paper proposes an initial , formal characterization of alignment coverage as intersecting two partially ordered sets ( graphs ) of translation equivalence units , one derived by a grammar instance and another defined by the word alignment . as a first sanity check , we report extensive coverage results for itg on automatic and manual alignments . even for the itg formalism , our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work .

cross-document temporal and spatial person tracking
traditional information extraction ( ie ) systems identify many unconnected facts . the objective of this paper is to define a new cross-document information extraction task and demonstrate a system which can extract , rank and track events in two dimensions : temporal and spatial . the system can automatically label the person entities involved in significant events as 'centroid arguments ' , and then present the events involving the same centroid on a time line and on a geographical map .

karolina owczarzak josef van genabith andy way
we present a method for evaluating the quality of machine translation ( mt ) output , using labelled dependencies produced by a lexical-functional grammar ( lfg ) parser . our dependencybased method , in contrast to most popular string-based evaluation metrics , does not unfairly penalize perfectly valid syntactic variations in the translation , and the addition of wordnet provides a way to accommodate lexical variation . in comparison with other metrics on 16,800 sentences of chinese-english newswire text , our method reaches high correlation with human scores .

supervised learning of a probabilistic lexicon of verb semantic classes
the work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of verbnet classes . we intend for the probabilistic model to provide a probability distribution of verb-class associations , over known and unknown verbs , including polysemous words . in our approach , training instances are obtained from an existing lexicon and/or from an annotated corpus , while the features , which represent syntactic frames , semantic similarity , and selectional preferences , are extracted from unannotated corpora . our model is evaluated in type-level verb classification tasks : we measure the prediction accuracy of verbnet classes for unknown verbs , and also measure the dissimilarity between the learned and observed probability distributions . we empirically compare several settings for model learning , while we vary the use of features , source corpora for feature extraction , and disambiguated corpora . in the task of verb classification into all verbnet classes , our best model achieved a 10.69 % error reduction in the classification accuracy , over the previously proposed model .

detecting errors in corpora using support vector machines
while the corpus-based research relies on human annotated corpora , it is often said that a non-negligible amount of errors remain even in frequently used corpora such as penn treebank . detection of errors in annotated corpora is important for corpus-based natural language processing . in this paper , we propose a method to detect errors in corpora using support vector machines ( svms ) . this method is based on the idea of extracting exceptional elements that violate consistency . we propose a method of using svms to assign a weight to each element and to find errors in a pos tagged corpus . we apply the method to english and japanese pos-tagged corpora and achieve high precision in detecting errors .

unifying annotated discourse hierarchies to create a gold standard division of engineering and applied sciences
human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement . this paper presents several techniques for unifying multiple discourse annotations into a single hierarchy , deemed a gold standard the segmentation that best captures the underlying linguistic structure of the discourse . it proposes and analyzes methods that consider the level of embeddedness of a segmentation as well as methods that do not . a corpus containing annotated hierarchical discourses , the boston directions corpus , was used to evaluate the goodness of each technique , by comparing the similarity of the segmentation it derives to the original annotations in the corpus . several metrics of similarity between hierarchical segmentations are computed : precision/recall of matching utterances , pairwise inter-reliability scores ( ) , and non-crossing-brackets . a novel method for unification that minimizes conflicts among annotators outperforms methods that require consensus among a majority for the and precision metrics , while capturing much of the structure of the discourse . when high recall is preferred , methods requiring a majority are preferable to those that demand full consensus among annotators .

supporting rule-based representations with corpus-derived lexical
the pervasive ambiguity of language allows sentences that differ in just one lexical item to have rather different inference patterns . this would be no problem if the different lexical items fell into clearly definable and easy to represent classes . but this is not the case . to draw the correct inferences we need to look how the referents of the lexical items in the sentence ( or broader context ) interact in the described situation . given that the knowledge our systems have of the represented situation will typically be incomplete , the classifications we come up with can only be probabilistic . we illustrate this problem with an investigation of various inference patterns associated with predications of the form verb from x to y , especially go from x to y. we characterize the various readings and make an initial proposal about how to create the lexical classes that will allow us to draw the correct inferences in the different cases .

directional distributional similarity for lexical expansion
distributional word similarity is most commonly perceived as a symmetric relation . yet , one of its major applications is lexical expansion , which is generally asymmetric . this paper investigates the nature of directional ( asymmetric ) similarity measures , which aim to quantify distributional feature inclusion . we identify desired properties of such measures , specify a particular one based on averaged precision , and demonstrate the empirical benefit of directional measures for expansion .

learning to recognize features of valid textual entailments
this paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment . current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text , using a locally decomposable matching score . we argue that there are significant weaknesses in this approach , including flawed assumptions of monotonicity and locality . instead we propose a pipelined approach where alignment is followed by a classification step , in which we extract features representing high-level characteristics of the entailment problem , and pass the resulting feature vector to a statistical classifier trained on development data . we report results on data from the 2005 pascal rte challenge which surpass previously reported results for alignment-based systems .

unsupervised discovery of rhyme schemes
this paper describes an unsupervised , language-independent model for finding rhyme schemes in poetry , using no prior knowledge about rhyme or pronunciation .

a new syntactic metric for evaluation of machine translation
machine translation ( mt ) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation . this comparison can be performed on multiple levels : lexical , syntactic or semantic . in this paper , we propose a new syntactic metric for mt evaluation based on the comparison of the dependency structures of the reference and the candidate translations . the dependency structures are obtained by means of a weighted constraints dependency grammar parser . based on experiments performed on english to german translations , we show that the new metric correlates well with human judgments at the system level .

a cross-language study on automatic speech disfluency detection
we investigate two systems for automatic disfluency detection on english and mandarin conversational speech data . the first system combines various lexical and prosodic features in a conditional random field model for detecting edit disfluencies . the second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition . we compare the contributions of different knowledge sources to detection performance between these two languages .

baseline evaluation of wsd and semantic dependency in
this paper presents the evaluation of a subset of the capabilities of the ontosem semantic analyzer conducted in the framework of the shared task for the step 2008 workshop . we very briefly describe ontosems components and knowledge resources , describe the work preparatory to the evaluation ( the creation of gold standard basic text meaning representations ) and present ontosems performance on word sense disambiguation and determination of semantic dependencies . the paper also contains elements of a methodological discussion .

named entity recognition for ukrainian : a resource-light approach
named entity recognition ( ner ) is a subtask of information extraction ( ie ) which can be used further on for different purposes . in this paper , we discuss named entity recognition for ukrainian language , which is a slavonic language with a rich morphology . the approach we follow uses a restricted number of features . we show that it is feasible to boost performance by considering several heuristics and patterns acquired from the web data .

d-confidence : an active learning strategy which efficiently identifies small
in some classification tasks , such as those related to the automatic building and maintenance of text corpora , it is expensive to obtain labeled examples to train a classifier . in such circumstances it is common to have massive corpora where a few examples are labeled ( typically a minority ) while others are not . semi-supervised learning techniques try to leverage the intrinsic information in unlabeled examples to improve classification models . however , these techniques assume that the labeled examples cover all the classes to learn which might not stand . in the presence of an imbalanced class distribution getting labeled examples from minority classes might be very costly if queries are randomly selected . active learning allows asking an oracle to label new examples , that are criteriously selected , and does not assume a previous knowledge of all classes . d-confidence is an active learning approach that is effective when in presence of imbalanced training sets . in this paper we discuss the performance of dconfidence over text corpora . we show empirically that d-confidence reduces the number of queries required to identify examples from all classes to learn when compared to confidence , a common active learning criterion .

generating lexical analogies using dependency relations
a lexical analogy is a pair of word-pairs that share a similar semantic relation . lexical analogies occur frequently in text and are useful in various natural language processing tasks . in this study , we present a system that generates lexical analogies automatically from text data . our system discovers semantically related pairs of words by using dependency relations , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . empirical evaluation shows that our system generates valid lexical analogies with a precision of 70 % , and produces quality output although not at the level of the best humangenerated lexical analogies .

aligning chinese-english parallel parse trees : is it feasible
we investigate the feasibility of aligning chinese and english parse trees by examining cases of incompatibility between chinese-english parallel parse trees . this work is done in the context of an annotation project wherewe construct a parallel treebank by doingword and phrase alignments simultaneously . we discuss the most common incompatibility patterns identified within vps and nps and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself . this suggests that in principle it is feasible to align the parallel parse trees with somemodification of existing syntactic annotation guidelines . we believe this has implications for the use of parallel parse trees as an important resource for machine translation models .

columbia nlp : sentiment detection of sentences and subjective phrases in social media
we present two supervised sentiment detection systems which were used to compete in semeval-2014 task 9 : sentiment analysis in twitter . the first system ( rosenthal and mckeown , 2013 ) classifies the polarity of subjective phrases as positive , negative , or neutral . it is tailored towards online genres , specifically twitter , through the inclusion of dictionaries developed to capture vocabulary used in online conversations ( e.g. , slang and emoticons ) as well as stylistic features common to social media . the second system ( agarwal et al. , 2011 ) classifies entire tweets as positive , negative , or neutral . it too includes dictionaries and stylistic features developed for social media , several of which are distinctive from those in the first system . we use both systems to participate in subtasks a and b of semeval2014 task 9 : sentiment analysis in twitter . we participated for the first time in subtask b : message-level sentiment detection by combining the two systems to achieve improved results compared to either system alone .

generating complex morphology for machine translation
we present a novel method for predicting inflected word forms for generating morphologically rich languages in machine translation . we utilize a rich set of syntactic and morphological knowledge sources from both source and target sentences in a probabilistic model , and evaluate their contribution in generating russian and arabic sentences . our results show that the proposed model substantially outperforms the commonly used baseline of a trigram target language model ; in particular , the use of morphological and syntactic features leads to large gains in prediction accuracy . we also show that the proposed method is effective with a relatively small amount of data .

decomposition for iso / iec 10646 ideographic characters
ideograph characters are often formed by some smaller functional units , which we call character components . these character components can be ideograph radicals , ideographs proper , or some pure components which must be used with others to form characters . decomposition of ideographs can be used in many applications . it is particularly important in the study of chinese character formation , phonetics and semantics . however , the way a character is decomposed depends on the definition of components as well as the decomposition rules . the 12 ideographic description characters ( idcs ) introduced in iso 10646 are designed to describe characters using components . the hong kong sar government recently published two sets of glyph standards for iso10646 characters . the standards , being the first of its kind , make use of character decomposition to specify a character glyph using its components . in this paper , we will first introduce the idcs and how they can be used with components to describe two dimensional ideograph characters in a linear fashion . next we will briefly discuss the basic references and character decomposition rules .

phonmatrix : visualizing co-occurrence constraints of sounds quantitative language comparison
this paper describes the online tool phonmatrix , which analyzes a word list with respect to the co-occurrence of sounds in a specified context within a word . the cooccurrence counts from the user-specified context are statistically analyzed according to a number of association measures that can be selected by the user . the statistical values then serve as the input for a matrix visualization where rows and columns represent the relevant sounds under investigation and the matrix cells indicate whether the respective ordered pair of sounds occurs more or less frequently than expected . the usefulness of the tool is demonstrated with three case studies that deal with vowel harmony and similar place avoidance patterns .

accurate collocation extraction using a multilingual parser
this paper focuses on the use of advanced techniques of text analysis as support for collocation extraction . a hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from english , french , spanish and italian corpora . the advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .

learning to freestyle : hip hop challenge-response induction via transduction rule segmentation
we present a novel model , freestyle , that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics , by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations . in this attack on the woefully under-explored natural language genre of music lyrics , we exploit a strictly unsupervised transduction grammar induction approach . our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed , even though the domain of hip hop lyrics is particularly noisy and unstructured . we evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction , and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses , measured on fluency and rhyming criteria as judged by human evaluators . to highlight some of the inherent challenges in adapting other algorithms to this novel task , we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based smt system . we tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module , which is also acquired via unsupervised learning and report improved quality of the generated responses . finally , we report results with maghrebi french hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages .

using contextual speller techniques and language modeling for esl error correction
we present a modular system for detection and correction of errors made by nonnative ( english as a second language = esl ) writers . we focus on two error types : the incorrect use of determiners and the choice of prepositions . we use a decisiontree approach inspired by contextual spelling systems for detection and correction suggestions , and a large language model trained on the gigaword corpus to provide additional information to filter out spurious suggestions . we show how this system performs on a corpus of non-native english text and discuss strategies for future enhancements .

a semantic scattering model for the automatic interpretation of genitives
this paper addresses the automatic classification of the semantic relations expressed by the english genitives . a learning model is introduced based on the statistical analysis of the distribution of genitives semantic relations on a large corpus . the semantic and contextual features of the genitives noun phrase constituents play a key role in the identification of the semantic relation . the algorithm was tested on a corpus of approximately 2,000 sentences and achieved an accuracy of 79 % , far better than 44 % accuracy obtained with c5.0 , or 43 % obtained with a naive bayes algorithm , or 27 % accuracy with a support vector machines learner on the same corpus .

a filter-based approach to detect end-of-utterances from prosody in
we propose an efficient method to detect end-of-utterances from prosodic information in conversational speech . our method is based on the application of a large set of binary and ramp filters to the energy and fundamental frequency signals obtained from the speech signal . these filter responses , which can be computed very efficiently , are used as input to a learning algorithm that generates the final detector . preliminary experiments using data obtained from conversations show that an accurate classifier can be trained efficiently and that good results can be obtained without requiring a speech recognition system .

open book : a tool for helping asd users semantic comprehension
persons affected by autism spectrum disorders ( asd ) present impairments in social interaction . a significant percentile of them have inadequate reading comprehension skills . in the ongoing first project we build a multilingual tool called open book that helps the asd people to better understand the texts . the tool applies a series of automatic transformations to user documents to identify and remove the reading obstacles to comprehension . we focus on three semantic components : an image component that retrieves images for the concepts in the text , an idiom detection component and a topic model component . moreover , we present the personalization component that adapts the system output to user preferences .

indonet : a multilingual lexical knowledge network for indian brijesh bhatt lahari poddar pushpak bhattacharyya
we present indonet , a multilingual lexical knowledge base for indian languages . it is a linked structure of wordnets of 18 different indian languages , universal word dictionary and the suggested upper merged ontology ( sumo ) . we discuss various benefits of the network and challenges involved in the development . the system is encoded in lexical markup framework ( lmf ) and we propose modifications in lmf to accommodate universal word dictionary and sumo . this standardized version of lexical knowledge base of indian languages can now easily be linked to similar global resources .

a pilot annotation to investigate discourse connectivity in biomedical text
the goal of the penn discourse treebank ( pdtb ) project is to develop a large-scale corpus , annotated with coherence relations marked by discourse connectives . currently , the primary application of the pdtb annotation has been to news articles . in this study , we tested whether the pdtb guidelines can be adapted to a different genre . we annotated discourse connectives and their arguments in one 4,937-token full-text biomedical article . two linguist annotators showed an agreement of 85 % after simple conventions were added . for the remaining 15 % cases , we found that biomedical domain-specific knowledge is needed to capture the linguistic cues that can be used to resolve inter-annotator disagreement . we found that the two annotators were able to reach an agreement after discussion . thus our experiments suggest that the pdtb annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues .

bridging the gap between dialogue management and dialogue models
why do few working spoken dialogue systems make use of dialogue models in their dialogue management we find out the causes and propose a generic dialogue model . it promises to bridge the gap between practical dialogue management and ( pattern-based ) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act .

desmond darma putra , lili szabo
this paper describes our submission for the conll 2013 shared task , which aims to to improve the detection and correction of the five most common grammatical error types in english text written by non-native speakers . our system concentrates only on two of them ; it employs machine learning classifiers for the artordet- , and a fully deterministic rule based workflow for the sva error type .

applicative structures and immediate discourse in the turkish
various discourse theories have argued for data structures ranging from the simplest trees to the most complex chain graphs . this paper investigates the structure represented by the explicit connectives annotated in the multiplegenre turkish discourse bank ( tdb ) . the dependencies that violate tree-constraints are analyzed . the effects of information structure in the surface form , which result in seemingly complex configurations with underlying simple dependencies , are introduced ; and the structural implications are discussed . the results indicate that our current approach to local discourse structure needs to accommodate properly contained arguments and relations , and partially overlapping as well as shared arguments ; deviating further from simple trees , but not as drastically as a chain graph structure would imply , since no genuine cases of structural crossing dependencies are attested in tdb .

the importance of supertagging for wide-coverage ccg parsing
this paper describes the role of supertagging in a wide-coverage ccg parser which uses a log-linear model to select an analysis . the supertagger reduces the derivation space over which model estimation is performed , reducing the space required for discriminative training . it also dramatically increases the speed of the parser . we show that large increases in speed can be obtained by tightly integrating the supertagger with the ccg grammar and parser . this is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar . we also further reduce the derivation space using constraints on category combination . the result is an accurate wide-coverage ccg parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms .

in the comic fission module mary ellen foster
we give a technical description of the fission module of the comic multimodal dialogue system , which both plans the multimodal content of the system turns and controls the execution of those plans . we emphasise the parts of the implementation that allow the system to begin producing output as soon as possible by preparing and outputting the content in parallel . we also demonstrate how the module was designed to ensure robustness and configurability , and describe how the module has performed successfully as part of the overall system . finally , we discuss how the techniques used in this module can be applied to other similar dialogue systems .

integration of a lexical type database with a linguistically interpreted
we have constructed a large scale and detailed database of lexical types in japanese from a treebank that includes detailed linguistic information . the database helps treebank annotators and grammar developers to share precise knowledge about the grammatical status of words that constitute the treebank , allowing for consistent large scale treebanking and grammar development . in this paper , we report on the motivation and methodology of the database construction .

uspwlv and wlvusp : combining dictionaries and contextual information for cross-lingual lexical substitution
we describe two systems participating in semeval-2010s cross-lingual lexical substitution task : uspwlv and wlvusp . both systems are based on two main components : ( i ) a dictionary to provide a number of possible translations for each source word , and ( ii ) a contextual model to select the best translation according to the context where the source word occurs . these components and the way they are integrated are different in the two systems : they exploit corpus-based and linguistic resources , and supervised and unsupervised learning methods . among the 14 participants in the subtask to identify the best translation , our systems were ranked 2nd and 4th in terms of recall , 3rd and 4th in terms of precision . both systems outperformed the baselines in all subtasks according to all metrics used .

statistical dependency parsing of turkish
this paper presents results from the first statistical dependency parser for turkish . turkish is a free-constituent order language with complex agglutinative inflectional and derivational morphology and presents interesting challenges for statistical parsing , as in general , dependency relations are between portions of words called inflectional groups . we have explored statistical models that use different representational units for parsing . we have used the turkish dependency treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links . our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing , and when contexts around the dependent are employed .

fast and accurate query-based multi-document summarization
we present a fast query-based multi-document summarizer called fastsum based solely on word-frequency features of clusters , documents and topics . summary sentences are ranked by a regression svm . the summarizer does not use any expensive nlp techniques such as parsing , tagging of names or even part of speech information . still , the achieved accuracy is comparable to the best systems presented in recent academic competitions ( i.e. , document understanding conference ( duc ) ) . because of a detailed feature analysis using least angle regression ( lars ) , fastsum can rely on a minimal set of features leading to fast processing times : 1250 news documents in 60 seconds .

an analysis of memory-based processing costs using incremental deep syntactic dependency parsing
reading experiments using naturalistic stimuli have shown unanticipated facilitations for completing center embeddings when frequency effects are factored out . to eliminate possible confounds due to surface structure , this paper introduces a processing model based on deep syntactic dependencies . results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings .

thai national corpus : a progress report
this paper presents problems and solutions in developing thai national corpus ( tnc ) . tnc is designed to be a comparable corpus of british national corpus . the project aims to collect eighty million words . since 2006 , the project can now collect only fourteen million words . the data is accessible from the tnc web . delay in creating the tnc is mainly caused from obtaining authorization of copyright texts . methods used for collecting data and the results are discussed . errors during the process of encoding data and how to handle these errors will be described .

bootstrapping named entity recognition with automatically generated gazetteer lists
current named entity recognition systems suffer from the lack of hand-tagged data as well as degradation when moving to other domain . this paper explores two aspects : the automatic generation of gazetteer lists from unlabeled data ; and the building of a named entity recognition system with labeled and unlabeled data .

applying alternating structure optimization to word sense disambiguation rie kubota ando
this paper presents a new application of the recently proposed machine learning method alternating structure optimization ( aso ) , to word sense disambiguation ( wsd ) . given a set of wsd problems and their respective labeled examples , we seek to improve overall performance on that set by using all the labeled examples ( irrespective of target words ) for the entire set in learning a disambiguator for each individual problem . thus , in effect , on each individual problem ( e.g. , disambiguation of art ) we benefit from training examples for other problems ( e.g. , disambiguation of bar , canal , and so forth ) . we empirically study the effective use of aso for this purpose in the multitask and semi-supervised learning configurations . our performance results rival or exceed those of the previous best systems on several senseval lexical sample task data sets .

identifying emotions , intentions , and attitudes in text using a game with a purpose
subtle social information is available in text such as a speakers emotional state , intentions , and attitude , but current information extraction systems are unable to extract this information at the level that humans can . we describe a methodology for creating databases of messages annotated with social information based on interactive games between humans trying to generate and interpret messages for a number of different social information types . we then present some classification results achieved by using a small-scale database created with this methodology .

learning first-order horn clauses from web text
even the entire web corpus does not explicitly answer all questions , yet inference can uncover many implicit answers . but where do inference rules come from this paper investigates the problem of learning inference rules from web text in an unsupervised , domain-independent manner . the sherlock system , described herein , is a first-order learner that acquires over 30,000 horn clauses from web text . sherlock embodies several innovations , including a novel rule scoring function based on statistical relevance ( salmon et al , 1971 ) which is effective on ambiguous , noisy and incomplete web extractions . our experiments show that inference over the learned rules discovers three times as many facts ( at precision 0.8 ) as the textrunner system which merely extracts facts explicitly stated in web text .

first joint workshop on statistical parsing of morphologically rich languages experiments for dependency parsing of greek and speech processing and speech processing
this paper describes experiments for statistical dependency parsing using two different parsers trained on a recently extended dependency treebank for greek , a language with a moderately rich morphology . we show how scores obtained by the two parsers are influenced by morphology and dependency types as well as sentence and arc length . the best las obtained in these experiments was 80.16 on a test set with manually validated pos tags and lemmas .

a discriminative learning model for coordinate conjunctions
we propose a sequence-alignment based method for detecting and disambiguating coordinate conjunctions . in this method , averaged perceptron learning is used to adapt the substitution matrix to the training data drawn from the target language and domain . to reduce the cost of training data construction , our method accepts training examples in which complete word-by-word alignment labels are missing , but instead only the boundaries of coordinated conjuncts are marked . we report promising empirical results in detecting and disambiguating coordinated noun phrases in the genia corpus , despite a relatively small number of training examples and minimal features are employed .

aligning english strings with abstract meaning representation graphs
we align pairs of english sentences and corresponding abstract meaning representations ( amr ) , at the token level . such alignments will be useful for downstream extraction of semantic interpretation and generation rules . our method involves linearizing amr structures and performing symmetrized em training . we obtain 86.5 % and 83.1 % alignment f score on development and test sets .

gleu : automatic evaluation of sentence-level fluency
in evaluating the output of language technology applicationsmt , natural language generation , summarisationautomatic evaluation techniques generally conflate measurement of faithfulness to source content with fluency of the resulting text . in this paper we develop an automatic evaluation metric to estimate fluency alone , by examining the use of parser outputs as metrics , and show that they correlate with human judgements of generated text fluency . we then develop a machine learner based on these , and show that this performs better than the individual parser metrics , approaching a lower bound on human performance . we finally look at different language models for generating sentences , and show that while individual parser metrics can be fooled depending on generation method , the machine learner provides a consistent estimator of fluency .

toward learning and evaluation of dialogue policies with text examples
we present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data . to facilitate learning and evaluation , our framework enriches a collection of role-play dialogues with additional training data , including paraphrases of user utterances , and multiple independent judgments by external referees about the best policy response for the character at each point . as a case study , we use this framework to train a policy for a limited domain tactical questioning character , reaching promising performance . we also introduce an automatic policy evaluation metric that recognizes the validity of multiple conversational responses at each point in a dialogue . we use this metric to explore the variability in human opinion about optimal policy decisions , and to automatically evaluate several learned policies in our example domain .

a walk on the other side : adding statistical components to a transfer-based translation system ariadna font llitjs
this paper seeks to complement the current trend of adding more structure to statistical machine translation systems , by exploring the opposite direction : adding statistical components to a transfer-based mt system . initial results on the btec data show significant improvement according to three automatic evaluation metrics ( bleu , nist and meteor ) .

hal-based cascaded model for variable-length semantic pattern induction from psychiatry web resources
negative life events play an important role in triggering depressive episodes . developing psychiatric services that can automatically identify such events is beneficial for mental health care and prevention . before these services can be provided , some meaningful semantic patterns , such as < lost , parents > , have to be extracted . in this work , we present a text mining framework capable of inducing variable-length semantic patterns from unannotated psychiatry web resources . this framework integrates a cognitive motivated model , hyperspace analog to language ( hal ) , to represent words as well as combinations of words . then , a cascaded induction process ( cip ) bootstraps with a small set of seed patterns and incorporates relevance feedback to iteratively induce more relevant patterns . the experimental results show that by combining the hal model and relevance feedback , the cip can induce semantic patterns from the unannotated web corpora so as to reduce the reliance on annotated corpora .

learning the hyperparameters to learn morphology
we perform hyperparameter inference within a model of morphology learning ( goldwater et al. , 2011 ) and find that it affects model behaviour drastically . changing the model structure successfully avoids the unsegmented solution , but results in oversegmentation instead .

a generic approach to parallel chart parsing with an application to lingo
multi-processor systems are becoming more commonplace and affordable . based on analyses of actual parsings , we argue that to exploit the capabilities of such machines , unification-based grammar parsers should distribute work at the level of individual unification operations . we present a generic approach to parallel chart parsing that meets this requirement , and show that an implementation of this technique for lingo achieves considerable speedups .

aggregating continuous word embeddings for information retrieval
while words in documents are generally treated as discrete entities , they can be embedded in a euclidean space which reflects an a priori notion of similarity between them . in such a case , a text document can be viewed as a bag-ofembedded-words ( boew ) : a set of realvalued vectors . we propose a novel document representation based on such continuous word embeddings . it consists in non-linearly mapping the wordembeddings in a higher-dimensional space and in aggregating them into a documentlevel representation . we report retrieval and clustering experiments in the case where the word-embeddings are computed from standard topic models showing significant improvements with respect to the original topic models .

automatic question generation using discourse cues
in this paper , we present a system that automatically generates questions from natural language text using discourse connectives . we explore the usefulness of the discourse connectives for question generation ( qg ) that looks at the problem beyond sentence level . our work divides the qg task into content selection and question formation . content selection consists of finding the relevant part in text to frame question from while question formation involves sense disambiguation of the discourse connectives , identification of question type and applying syntactic transformations on the content . the system is evaluated manually for syntactic and semantic correctness .

a just-in-time keyword extraction from meeting transcripts
in a meeting , it is often desirable to extract keywords from each utterance as soon as it is spoken . thus , this paper proposes a just-intime keyword extraction from meeting transcripts . the proposed method considers two major factors that make it different from keyword extraction from normal texts . the first factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones , and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction . our experiments on two data sets in english and korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts .

improving semi-supervised acquisition of relation extraction patterns
this paper presents a novel approach to the semi-supervised learning of information extraction patterns . the method makes use of more complex patterns than previous approaches and determines their similarity using a measure inspired by recent work using kernel methods ( culotta and sorensen , 2004 ) . experiments show that the proposed similarity measure outperforms a previously reported measure based on cosine similarity when used to perform binary relation extraction .

buap : an unsupervised approach to automatic keyphrase extraction from scientific articles
in this paper , it is presented an unsupervised approach to automatically discover the latent keyphrases contained in scientific articles . the proposed technique is constructed on the basis of the combination of two techniques : maximal frequent sequences and pageranking . we evaluated the obtained results by using micro-averaged precision , recall and fscores with respect to two different gold standards : 1 ) readers keyphrases , and 2 ) a combined set of authors and readers keyphrases . the obtained results were also compared against three different baselines : one unsupervised ( tf-idf based ) and two supervised ( nave bayes and maximum entropy ) .

a multi-document multi-lingual automatic summarization system
abstract . in this paper , a new multidocument multi-lingual text summarization technique , based on singular value decomposition and hierarchical clustering , is proposed . the proposed approach relies on only two resources for any language : a word segmentation system and a dictionary of words along with their document frequencies . the summarizer initially takes a collection of related documents , and transforms them into a matrix ; it then applies singular value decomposition to the resulted matrix . after using a binary hierarchical clustering algorithm , the most important sentences of the most important clusters form the summary . the appropriate place of each chosen sentence is determined by a novel technique . the system has been successfully tested on summarizing several persian document collections .

optimizing segmentation strategies for simultaneous speech translation
in this paper , we propose new algorithms for learning segmentation strategies for simultaneous speech translation . in contrast to previously proposed heuristic methods , our method finds a segmentation that directly maximizes the performance of the machine translation system . we describe two methods based on greedy search and dynamic programming that search for the optimal segmentation strategy . an experimental evaluation finds that our algorithm is able to segment the input two to three times more frequently than conventional methods in terms of number of words , while maintaining the same score of automatic evaluation .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

statistical machine reordering
reordering is currently one of the most important problems in statistical machine translation systems . this paper presents a novel strategy for dealing with it : statistical machine reordering ( smr ) . it consists in using the powerful techniques developed for statistical machine translation ( smt ) to translate the source language ( s ) into a reordered source language ( s ) , which allows for an improved translation into the target language ( t ) . the smt task changes from s2t to s2t which leads to a monotonized word alignment and shorter translation units . in addition , the use of classes in smr helps to infer new word reorderings . experiments are reported in the esen wmt06 tasks and the zhen iwslt05 task and show significant improvement in translation quality .

a unigram orientation model for statistical machine translation
in this paper , we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks : pairs of phrases without internal structure . the segmentation model uses a novel orientation component to handle swapping of neighbor blocks . during training , we collect block unigram counts with orientation : we count how often a block occurs to the left or to the right of some predecessor block . the orientation model is shown to improve translation performance over two models : 1 ) no block re-ordering is used , and 2 ) the block swapping is controlled only by a language model . we show experimental results on a standard arabic-english translation task .

optimizing chinese word segmentation for machine translation
previous work has shown that chinese word segmentation is useful for machine translation to english , yet the way different segmentation strategies affect mt is still poorly understood . in this paper , we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance . we find that other factors such as segmentation consistency and granularity of chinese words can be more important for machine translation . based on these findings , we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task , providing an improvement of 0.73 bleu . we also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 bleu increase .

improving nonparameteric bayesian inference : experiments on unsupervised word segmentation with adaptor grammars
one of the reasons nonparametric bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities . adaptor grammars are a framework for defining a variety of hierarchical nonparametric bayesian models . this paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures , and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task . with appropriate adaptor grammars and inference procedures we achieve an 87 % word token f-score on the standard brent version of the bernsteinratner corpus , which is an error reduction of over 35 % over the best previously reported results for this corpus .

automatic extraction of morphological lexicons from morphologically annotated corpora
we present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora . the method consists of a core languageindependent algorithm , which can be optimized for specific languages . the method is demonstrated on egyptian arabic and german , two morphologically rich languages . our best method for egyptian arabic provides an error reduction of 55.6 % over a simple baseline ; our best method for german achieves a 66.7 % error reduction .

continuous space language models for statistical machine translation
statistical machine translation systems are based on one or more translation models and a language model of the target language . while many different translation models and phrase extraction algorithms have been proposed , a standard word n-gram back-off language model is used in most systems . in this work , we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary . a neural network is used to perform the projection and the probability estimation . we consider the translation of european parliament speeches . this task is part of an international evaluation organized by the tc-star project in 2006. the proposed method achieves consistent improvements in the bleu score on the development and test data . we also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks .

utexas : natural language semantics using distributional semantics and
we represent natural language semantics by combining logical and distributional information in probabilistic logic . we use markov logic networks ( mln ) for the rte task , and probabilistic soft logic ( psl ) for the sts task . the system is evaluated on the sick dataset . our best system achieves 73 % accuracy on the rte task , and a pearsons correlation of 0.71 on the sts task .

multi-component word sense disambiguation
this paper describes the system mc-wsd presented for the english lexical sample task . the system is based on a multicomponent architecture . it consists of one classifier with two components . one is trained on the data provided for the task . the second is trained on this data and , additionally , on an external training set extracted from the wordnet glosses . the goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology .

extending the bleu mt evaluation method with frequency weightings
we present the results of an experiment on extending the automatic method of machine translation evaluation blue with statistical weights for lexical items , such as tf.idf scores . we show that this extension gives additional information about evaluated texts ; in particular it allows us to measure translation adequacy , which , for statistical mt systems , is often overestimated by the baseline bleu method . the proposed model uses a single human reference translation , which increases the usability of the proposed method for practical purposes . the model suggests a linguistic interpretation which relates frequency weights and human intuition about translation adequacy and fluency .

wsd based on mutual information and syntactic patterns
this paper describes a hybrid system for wsd , presented to the english all-words and lexical-sample tasks , that relies on two different unsupervised approaches . the first one selects the senses according to mutual information proximity between a context word a variant of the sense . the second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred . this patterns are matched against the disambiguation contexts . we show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. the high precision obtained recommends deeper research of the techniques . results for the lexical sample task are also provided .

interactive predictive parsing using a web-based architecture
this paper introduces a web-based demonstration of an interactive-predictive framework for syntactic tree annotation , where the user is tightly integrated into the interactive parsing system . in contrast with the traditional postediting approach , both the user and the system cooperate to generate error-free annotated trees . user feedback is provided by means of natural mouse gestures and keyboard strokes .

a simple but powerful automatic term extraction method
in this paper , we propose a new idea for the automatic recognition of domain specific terms . our idea is based on the statistics between a compound noun and its component single-nouns . more precisely , we focus basically on how many nouns adjoin the noun in question to form compound nouns . we propose several scoring methods based on this idea and experimentally evaluate them on the ntcir1 tmrec test collection . the results are very promising especially in the low recall area .

a generalized alignment-free phrase extraction
in this paper , we present a phrase extraction algorithm using a translation lexicon , a fertility model , and a simple distortion model . except these models , we do not need explicit word alignments for phrase extraction . for each phrase pair ( a block ) , a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs ; a fertility score is computed to estimate how good the lengths are matched between phrase pairs ; a center distortion score is computed to estimate the relative position divergence between the phrase pairs . we presented the results and our experience in the shared tasks on frenchenglish .

computing locally coherent discourses
we present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse . the algorithm runs very efficiently on a variety of coherence measures from the literature . we also show that the discourse ordering problem is np-complete and can not be approximated .

translators in the loop : understanding how they work with cat tools
the research that we have been carrying out at translators workplaces over the past few years has provided indications that some cat tools are not being used to their full potential or are even being ignored by the users they were ( or should have been ) designed for . since by nature humans seem to resist changing habits and procedures that do the job , it is easy to attribute that to the intransigence of older translators and shift the focus to designing new tools for digital natives . however , the cognitive demands of processing complex input in one language while producing and revising and/or assessing and revising output in another add a new dimension to the usual considerations of the human-machine loop of interaction , which may be independent of the translators age or experience . in fact , the productivity constraints that many professional translators work under means that they might be adjusting more to their tools than adjusting their tools settings to optimize their ( the translators ) performance . and if those tools have not been designed to meet their users cognitive and physical ergonomic needs , their use may actually slow down the translation process and have potentially detrimental effects on quality . maureen ehrensberger-dow is a canadian psycholinguist who has been involved in research into multilingualism and translation in switzerland for the past 15 years . she is professor of translation studies in the zurich university of applied sciences institute of translation and interpreting and principal investigator of the snsf-financed research projects capturing translation processes and the cognitive and physical ergonomics of translation . 28

the impact of deep linguistic processing on parsing technology
as the organizers of the acl 2007 deep linguistic processing workshop ( baldwin et al. , 2007 ) , we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology . we are particularly interested in the ways in which efficient , broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide . this often requires hybrid technologies which use shallow or statistical methods for pre- or post-processing , to extend coverage , or to disambiguate the output .

towards systematic exploration of implemented grammars
when designing grammars of natural language , typically , more than one formal analysis can account for a given phenomenon . moreover , because analyses interact , the choices made by the engineer influence the possibilities available in further grammar development . the order in which phenomena are treated may therefore have a major impact on the resulting grammar . this paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering . i argue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses . the idea is illustrated through a comparative study of auxiliary structures in hpsg-based grammars for german and dutch . auxiliaries form a central phenomenon of german and dutch and are likely to influence many components of the grammar . this study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation .

two-phase lmr-rc tagging for chinese word segmentation tak pang lau and irwin king
in this paper we present a two-phase lmr-rc tagging scheme to perform chinese word segmentation . in the regular tagging phase , chinese sentences are processed similar to the original lmr tagging . tagged sentences are then passed to the correctional tagging phase , in which the sentences are re-tagged using extra information from the first round tagging results . two training methods , separated mode and integrated mode , are proposed to construct the models . experimental results show that our scheme in integrated mode performs the best in terms of accuracy , where separated mode is more suitable under limited computational resources .

playing the telephone game : determining the hierarchical structure of
news articles report on facts , events , and opinions with the intent of conveying the truth . however , the facts , events , and opinions appearing in the text are often known only secondor third-hand , and as any child who has played telephone knows , this relaying of facts often garbles the original message . properly understanding the information filtering structures that govern the interpretation of these facts , then , is critical to appropriately analyzing them . in this work , we present a learning approach that correctly determines the hierarchical structure of information filtering expressions 78.30 % of the time .

unsupervised learning of the morpho-semantic relationship in
morphological analysis as applied to english has generally involved the study of rules for inflections and derivations . recent work has attempted to derive such rules from automatic analysis of corpora . here we study similar issues , but in the context of the biological literature . we introduce a new approach which allows us to assign probabilities of the semantic relatedness of pairs of tokens that occur in text in consequence of their relatedness as character strings . our analysis is based on over 84 million sentences that compose the medline database and over 2.3 million token types that occur in medline and enables us to identify over 36 million token type pairs which have assigned probabilities of semantic relatedness of at least 0.7 based on their similarity as strings .

were not in kansas anymore : detecting domain changes in streams
domain adaptation , the problem of adapting a natural language processing system trained in one domain to perform well in a different domain , has received significant attention . this paper addresses an important problem for deployed systems that has received little attention detecting when such adaptation is needed by a system operating in the wild , i.e. , performing classification over a stream of unlabeled examples . our method uses adistance , a metric for detecting shifts in data streams , combined with classification margins to detect domain shifts . we empirically show effective domain shift detection on a variety of data sets and shift conditions .

chinese named entity recognition with conditional random fields
we present a chinese named entity recognition ( ner ) system submitted to the close track of sighan bakeoff2006 . we define some additional features via doing statistics in training corpus . our system incorporates basic features and additional features based on conditional random fields ( crfs ) . in order to correct inconsistently results , we perform the postprocessing procedure according to n-best results given by the crfs model . our final system achieved a f-score of 85.14 at msra , 89.03 at cityu , and 76.27 at ldc .

comparative news summarization using linear programming
comparative news summarization aims to highlight the commonalities and differences between two comparable news topics . in this study , we propose a novel approach to generating comparative news summaries . we formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics . we consider semantic-related cross-topic concept pairs as comparative evidences , and consider topic-related concepts as representative evidences . the optimization problem is addressed by using a linear programming model . the experimental results demonstrate the effectiveness of our proposed model .

dependency-based bracketing transduction grammar for statistical machine translation
in this paper , we propose a novel dependency-based bracketing transduction grammar for statistical machine translation , which converts a source sentence into a target dependency tree . different from conventional bracketing transduction grammar models , we encode target dependency information into our lexical rules directly , and then we employ two different maximum entropy models to determine the reordering and combination of partial dependency structures , when we merge two neighboring blocks . by incorporating dependency language model further , large-scale experiments on chinese-english task show that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases .

unsupervised adaptation of supervised part-of-speech taggers for closely related languages
when developing nlp tools for low-resource languages , one is often confronted with the lack of annotated data . we propose to circumvent this bottleneck by training a supervised hmm tagger on a closely related language for which annotated data are available , and translating the words in the tagger parameter files into the low-resource language . the translation dictionaries are created with unsupervised lexicon induction techniques that rely only on raw textual data . we obtain a tagging accuracy of up to 89.08 % using a spanish tagger adapted to catalan , which is 30.66 % above the performance of an unadapted spanish tagger , and 8.88 % below the performance of a supervised tagger trained on annotated catalan data . furthermore , we evaluate our model on several romance , germanic and slavic languages and obtain tagging accuracies of up to 92 % .

experimental evaluation of ltag-based features
this paper proposes the use of lexicalized tree-adjoining grammar ( ltag ) formalism as an important additional source of features for the semantic role labeling ( srl ) task . using a set of one-vs-all support vector machines ( svms ) , we evaluate these ltag-based features . our experiments show that ltag-based features can improve srl accuracy significantly . when compared with the best known set of features that are used in state of the art srl systems we obtain an improvement in f-score from 82.34 % to 85.25 % .

hierarchical joint learning : improving joint parsing and named entity recognition
one of the main obstacles to producing high quality joint models is the lack of jointly annotated data . joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data , but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data . in this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model . our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model . experiments on joint parsing and named entity recognition , using the ontonotes corpus , show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data .

annotating subordinators in the turkish discourse bank
in this paper we explain how we annotated subordinators in the turkish discourse bank ( tdb ) , an effort that started in 2007 and is still continuing . we introduce the project and describe some of the issues that were important in annotating three subordinators , namely karsn , ragmen and halde , all of which encode the coherence relation contrast-concession . we also describe the annotation tool .

termweighting schemes for latent dirichlet allocation\
many implementations of latent dirichlet allocation ( lda ) , including those described in blei et al ( 2003 ) , rely at some point on the removal of stopwords , words which are assumed to contribute little to the meaning of the text . this step is considered necessary because otherwise high-frequency words tend to end up scattered across many of the latent topics without much rhyme or reason . we show , however , that the problem of high-frequency words can be dealt with more elegantly , and in a way that to our knowledge has not been considered in lda , through the use of appropriate weighting schemes comparable to those sometimes used in latent semantic indexing ( lsi ) . our proposed weighting methods not only make theoretical sense , but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task .

tuning support vector machines for biomedical named entity recognition junichi kazama takaki makino yoshihiro ohta junichi tsujii crest , jst ( japan science and technology corporation )
we explore the use of support vector machines ( svms ) for biomedical named entity recognition . to make the svm training with the available largest corpus the genia corpus tractable , we propose to split the non-entity class into sub-classes , using part-of-speech information . in addition , we explore new features such as word cache and the states of an hmm trained by unsupervised learning . experiments on the genia corpus show that our class splitting technique not only enables the training with the genia corpus but also improves the accuracy . the proposed new features also contribute to improve the accuracy . we compare our svmbased recognition system with a system using maximum entropy tagging method .

computer science & engineering
many researchers are trying to use information extraction ( ie ) to create large-scale knowledge bases from natural language text on the web . however , the primary approach ( supervised learning of relation-specific extractors ) requires manually-labeled training data for each relation and doesnt scale to the thousands of relations encoded in web text . this paper presents luchs , a self-supervised , relation-specific ie system which learns 5025 relations more than an order of magnitude greater than any previous approach with an average f1 score of 61 % . crucial to luchss performance is an automated system for dynamic lexicon learning , which allows it to learn accurately from heuristically-generated training data , which is often noisy and sparse .

zdenek z abokrtsky
the valency lexicon of czech verbs , version 1.0 ( vallex 1.0 ) is a collection of linguistically annotated data and documentation , resulting from an attempt at formal description of valency frames of czech verbs . vallex 1.0 is closely related to prague dependency treebank . in this paper , the context in which vallex came into existence is briefly outlined , and also three similar projects for english verbs are mentioned . the core of the paper is the description of the logical structure of the vallex data . finally , we suggest a few directions of the future research .

concepts across categories
verbs or adjectives and their nominalizations and certain adverb adjective pairs can be argued to introduce the same concept . this can be shown through inference patterns , which can be explained if we assume davidsonian eventualities underlying all predicates . we make a contribution to the underlying state discussion by investigating the advantages and disadvantages of davidsonian versus kimian states for statives such as copular predicates . findings are implemented in our parser delilah .

the role of roles in classifying annotated biomedical text
this paper investigates the roles of named entities ( nes ) in annotated biomedical text classification . in the annotation schema of biocaster , a text mining system for public health protection , important concepts that reflect information about infectious diseases were conceptually analyzed with a formal ontological methodology . concepts were classified as types , while others were identified as being roles . types are specified as ne classes and roles are integrated into nes as attributes . we focus on the roles of nes by extracting and using them in different ways as features in the classifier . experimental results show that : 1 ) roles for each ne greatly helped improve performance of the system , 2 ) combining information about ne classes with their roles contribute significantly to the improvement of performance . we discuss in detail the effect of each role on the accuracy of text classification .

local semantics in the interpretation of temporal expressions
work on the interpretation of temporal expressions in text has generally been pursued in one of two paradigms : the formal semantics approach , where an attempt is made to provide a well-grounded theoretical basis for the interpretation of these expressions , and the more pragmaticallyfocused approach represented by the development of the timex2 standard , with its origins in work in information extraction . the former emphasises formal elegance and consistency ; the latter emphasises broad coverage for practical applications . in this paper , we report on the development of a framework that attempts to integrate insights from both perspectives , with the aim of achieving broad coverage of the domain in a well-grounded manner from a formal perspective . we focus in particular on the development of a compact notation for representing the semantics of underspecified temporal expressions that can be used to provide component-level evaluation of systems that interpret such expressions .

talp phrase-based statistical translation system for european language
this paper reports translation results for the exploiting parallel texts for statistical machine translation ( hlt-naacl workshop on parallel texts 2006 ) . we have studied different techniques to improve the standard phrase-based translation system . mainly we introduce two reordering approaches and add morphological information .

fbk-tr : svm for semantic relatedness and corpus patterns
this paper reports the description and scores of our system , fbk-tr , which participated at the semeval 2014 task # 1 `` evaluation of compositional distributional semantic models on full sentences through semantic relatedness and entailment '' . the system consists of two parts : one for computing semantic relatedness , based on svm , and the other for identifying the entailment values on the basis of both semantic relatedness scores and entailment patterns based on verb-specific semantic frames . the system ranked 11 th on both tasks with competitive results .

learning semantic hierarchies via word embeddings
semantic hierarchy construction aims to build structures of concepts linked by hypernymhyponym ( is-a ) relations . a major challenge for this task is the automatic discovery of such relations . this paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words . we identify whether a candidate word pair has hypernymhyponym relation by using the word-embedding-based semantic projections between words and their hypernyms . our result , an f-score of 73.74 % , outperforms the state-of-theart methods on a manually labeled test dataset . moreover , combining our method with a previous manually-built hierarchy extension method can further improve fscore to 80.29 % .

combining association measures for collocation extraction
we introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction . first , we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision-recall graphs and mean average precision . second , we describe several classification methods for combining association measures , followed by their evaluation and comparison with individual measures . finally , we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation .

simple syntactic and morphological processing can help english-hindi statistical machine translation cdac mumbai ( formerly ncst )
in this paper , we report our work on incorporating syntactic and morphological information for english to hindi statistical machine translation . two simple and computationally inexpensive ideas have proven to be surprisingly effective : ( i ) reordering the english source sentence as per hindi syntax , and ( ii ) using the suffixes of hindi words . the former is done by applying simple transformation rules on the english parse tree . the latter , by using a simple suffix separation program . with only a small amount of bilingual training data and limited tools for hindi , we achieve reasonable performance and substantial improvements over the baseline phrase-based system . our approach eschews the use of parsing or other sophisticated linguistic tools for the target language ( hindi ) making it a useful framework for statistical machine translation from english to indian languages in general , since such tools are not widely available for indian languages currently .

automatic detection of plagiarized spoken responses educational testing service
this paper addresses the task of automatically detecting plagiarized responses in the context of a test of spoken english proficiency for non-native speakers . a corpus of spoken responses containing plagiarized content was collected from a high-stakes assessment of english proficiency for non-native speakers , and several text-to-text similarity metrics were implemented to compare these responses to a set of materials that were identified as likely sources for the plagiarized content . finally , a classifier was trained using these similarity metrics to predict whether a given spoken response is plagiarized or not . the classifier was evaluated on a data set containing the responses with plagiarized content and non-plagiarized control responses and achieved accuracies of 92.0 % using transcriptions and 87.1 % using asr output ( with a baseline accuracy of 50.0 % ) .

leila : learning to extract information by linguistic analysis for computer science for computer science for computer science
one of the challenging tasks in the context of the semantic web is to automatically extract instances of binary relations from web documents for example all pairs of a person and the corresponding birthdate . in this paper , we present leila , a system that can extract instances of arbitrary given binary relations from natural language web documents without human interaction . different from previous approaches , leila uses a deep syntactic analysis . this results in consistent improvements over comparable systems ( such as e.g . snowball or texttoonto ) .

shallow semantics in fast textual entailment rule learners fabio massimo zanzotto povo di trento , italy
in this paper , we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules : 1 ) the typed anchors and 2 ) a faster computation of the similarity . we will report and comment on the preliminary experiments and on the submission results .

maximum entropy models for framenet classification
the development of framenet , a large database of semantically annotated sentences , has primed research into statistical methods for semantic tagging . we advance previous work by adopting a maximum entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence . further we examine the use of sentence level syntactic pattern features to increase performance . we analyze our strategy on both human annotated and automatically identified frame elements , and compare performance to previous work on identical test data . experiments indicate a statistically significant improvement ( p < 0.01 ) of over 6 % .

towards a matrix-based distributional model of meaning fzi forschungszentrum informatik
vector-based distributional models of semantics have proven useful and adequate in a variety of natural language processing tasks . however , most of them lack at least one key requirement in order to serve as an adequate representation of natural language , namely sensitivity to structural information such as word order . we propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices . the proposed model is applied to the task of free associations . in the end , the first results as well as directions for future work are discussed .

memory-based morphological analysis generation and part-of-speech tagging of arabic
we explore the application of memorybased learning to morphological analysis and part-of-speech tagging of written arabic , based on data from the arabic treebank . morphological analysis the construction of all possible analyses of isolated unvoweled wordforms is performed as a letter-by-letter operation prediction task , where the operation encodes segmentation , part-of-speech , character changes , and vocalization . part-of-speech tagging is carried out by a bi-modular tagger that has a subtagger for known words and one for unknown words . we report on the performance of the morphological analyzer and part-of-speech tagger . we observe that the tagger , which has an accuracy of 91.9 % on new data , can be used to select the appropriate morphological analysis of words in context at a precision of 64.0 and a recall of 89.7 .

evaluating contextual dependency of paraphrases using a latent variable model
this paper presents an evaluation method employing a latent variable model for paraphrases with their contexts . we assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred . a paraphrase is evaluated for whether its sentences are used in the same context . experimental results showed that the proposed method achieves almost 60 % accuracy and that there is not a large performance difference between the two models . the results also revealed an upper bound of accuracy of 77 % with the method when using only topic information .

information navigation system based on pomdp that tracks user focus koichiro yoshino tatsuya kawahara
we present a spoken dialogue system for navigating information ( such as news articles ) , and which can engage in small talk . at the core is a partially observable markov decision process ( pomdp ) , which tracks users state and focus of attention . the input to the pomdp is provided by a spoken language understanding ( slu ) component implemented with logistic regression ( lr ) and conditional random fields ( crfs ) . the pomdp selects one of six action classes ; each action class is implemented with its own module .

a beam-search extraction algorithm for comparable data
this paper extends previous work on extracting parallel sentence pairs from comparable data ( munteanu and marcu , 2005 ) . for a given source sentence s , a maximum entropy ( me ) classifier is applied to a large set of candidate target translations . a beam-search algorithm is used to abandon target sentences as non-parallel early on during classification if they fall outside the beam . this way , our novel algorithm avoids any document-level prefiltering step . the algorithm increases the number of extracted parallel sentence pairs significantly , which leads to a bleu improvement of about 1 % on our spanishenglish data .

learning noun phrase query segmentation
query segmentation is the process of taking a users search-engine query and dividing the tokens into individual phrases or semantic units . identification of these query segments can potentially improve both document-retrieval precision , by first returning pages which contain the exact query segments , and document-retrieval recall , by allowing query expansion or substitution via the segmented units . we train and evaluate a machine-learned query segmentation system that achieves 86 % segmentationdecision accuracy on a gold standard set of segmented noun phrase queries , well above recently published approaches . key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing . for example , token association features beyond simple n-gram counts provide powerful indicators of segmentation .

bootstrapping coreference classifiers with multiple machine learning algorithms
successful application of multi-view cotraining algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated . this can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split . to bootstrap coreference classifiers , we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training . in addition , we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data , aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping .

automatic conversion of dialectal tamil text to standard written tamil text using fsts sobha lalitha devi
we present an efficient method to automatically transform spoken language text to standard written language text for various dialects of tamil . our work is novel in that it explicitly addresses the problem and need for processing dialectal and spoken language tamil . written language equivalents for dialectal and spoken language forms are obtained using finite state transducers ( fsts ) where spoken language suffixes are replaced with appropriate written language suffixes . agglutination and compounding in the resultant text is handled using conditional random fields ( crfs ) based word boundary identifier . the essential sandhi corrections are carried out using a heuristic sandhi corrector which normalizes the segmented words to simpler sensible words . during experimental evaluations dialectal spoken to written transformer ( dswt ) achieved an encouraging accuracy of over 85 % in transformation task and also improved the translation quality of tamil-english machine translation system by 40 % . it must be noted that there is no published computational work on processing tamil dialects . ours is the first attempt to study various dialects of tamil in a computational point of view . thus , the nature of the work reported here is pioneering .

integrating ontological knowledge and textual evidence in estimating gene and gene product similarity
with the rising influence of the gene ontology , new approaches have emerged where the similarity between genes or gene products is obtained by comparing gene ontology code annotations associated with them . so far , these approaches have solely relied on the knowledge encoded in the gene ontology and the gene annotations associated with the gene ontology database . the goal of this paper is to demonstrate that improvements to these approaches can be obtained by integrating textual evidence extracted from relevant biomedical literature .

using corpus statistics on entities to improve semi-supervised relation extraction from the web
many errors produced by unsupervised and semi-supervised relation extraction ( re ) systems occur because of wrong recognition of entities that participate in the relations . this is especially true for systems that do not use separate named-entity recognition components , instead relying on general-purpose shallow parsing . such systems have greater applicability , because they are able to extract relations that contain attributes of unknown types . however , this generality comes with the cost in accuracy . in this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances , improving the overall re performance . we test the methods on sres a self-supervised web relation extraction system . we also compare the performance of corpus-based methods to the performance of validation and correction methods based on supervised ner components .

on the subjectivity of human authored short summaries
we address the issue of human subjectivity when authoring summaries , aiming at a simple , robust evaluation of machine generated summaries . applying a cross comprehension test on human authored short summaries from broadcast news , the level of subjectivity is gauged among four authors . the instruction set is simple , thus there is enough room for subjectivity . however the approach is robust because the test does not use the absolute score , relying instead on relative comparison , effectively alleviating the subjectivity . finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries .

nbest dependency parsing with linguistically rich models
we try to improve the classifier-based deterministic dependency parsing in two ways : by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models . it is experimentally shown on a conll 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation .

investigating pitch accent recognition in non-native speech
acquisition of prosody , in addition to vocabulary and grammar , is essential for language learners . however , it has received less attention in instruction . to enable automatic identification and feedback on learners prosodic errors , we investigate automatic pitch accent labeling for nonnative speech . we demonstrate that an acoustic-based context model can achieve accuracies over 79 % on binary pitch accent recognition when trained on withingroup data . furthermore , we demonstrate that good accuracies are achieved in crossgroup training , where native and nearnative training data result in no significant loss of accuracy on non-native test speech . these findings illustrate the potential for automatic feedback in computer-assisted prosody learning .

a measure of aggregate syntactic distance
we compare vectors containing counts of trigrams of part-of-speech ( pos ) tags in order to obtain an aggregate measure of syntax difference . since lexical syntactic categories reflect more abstract syntax as well , we argue that this procedure reflects more than just the basic syntactic categories . we tag the material automatically and analyze the frequency vectors for pos trigrams using a permutation test . a test analysis of a 305,000 word corpus containing the english of finnish emigrants to australia is promising in that the procedure proposed works well in distinguishing two different groups ( adult vs. child emigrants ) and also in highlighting syntactic deviations between the two groups .

extractive summarization and dialogue act modeling on email threads : an integrated probabilistic approach
in this paper , we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts . we assume that there is a relationship between dialogue acts and important sentences . based on this assumption , we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts . we compare our model with sequential and non-sequential models , which independently conduct the tasks of extractive summarization and dialogue act modeling . an empirical evaluation shows that our approach significantly outperforms all baselines in classifying correct summary sentences without losing performance on dialogue act modeling task .

using probabilistic models as predictors for a symbolic parser
in this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar . by including a chunker , a supertagger , a pp attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the german negra corpus . we attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .

exploiting domain knowledge in aspect extraction
aspect extraction is one of the key tasks in sentiment analysis . in recent years , statistical models have been used for the task . however , such models without any domain knowledge often produce aspects that are not interpretable in applications . to tackle the issue , some knowledge-based topic models have been proposed , which allow the user to input some prior domain knowledge to generate coherent aspects . however , existing knowledge-based topic models have several major shortcomings , e.g. , little work has been done to incorporate the can not -link type of knowledge or to automatically adjust the number of topics based on domain knowledge . this paper proposes a more advanced topic model , called mc-lda ( lda with m-set and c-set ) , to address these problems , which is based on an extended generalized plya urn ( e-gpu ) model ( which is also proposed in this paper ) . experiments on real-life product reviews from a variety of domains show that mclda outperforms the existing state-of-the-art models markedly .

an annotation scheme for automated bias detection in
biasml is a novel annotation scheme with the purpose of identifying the presence as well as nuances of biased language within the subset of wikipedia articles dedicated to service providers . whereas wikipedia currently uses only manual flagging to detect possible bias , our scheme provides a foundation for the automating of bias flagging by improving upon the methodology of annotation schemes in classic sentiment analysis . we also address challenges unique to the task of identifying biased writing within the specific context of wikipedias neutrality policy . we perform a detailed analysis of inter-annotator agreement , which shows that although the agreement scores for intra-sentential tags were relatively low , the agreement scores on the sentence and entry levels were encouraging ( 74.8 % and 66.7 % , respectively ) . based on an analysis of our first implementation of our scheme , we suggest possible improvements to our guidelines , in hope that further rounds of annotation after incorporating them could provide appropriate data for use within a machine learning framework for automated detection of bias within wikipedia .

interactive error resolution strategies for speech-to-speech translation systems speech , language and multimedia business unit raytheon bbn technologies
in this demonstration , we will showcase bbns speech-to-speech ( s2s ) translation system that employs novel interaction strategies to resolve errors through user-friendly dialog with the speaker . the system performs a series of analysis on input utterances to detect out-ofvocabulary ( oov ) named-entities and terms , sense ambiguities , homophones , idioms and ill-formed inputs . this analysis is used to identify potential errors and select an appropriate resolution strategy . our evaluation shows a 34 % ( absolute ) improvement in cross-lingual transfer of erroneous concepts in our english to iraqi-arabic s2s system .

investigation of annotators behaviour using eye-tracking data ryu iida koh mitsuda takenobu tokunaga
this paper presents an analysis of an annotators behaviour during her/his annotation process for eliciting useful information for natural language processing ( nlp ) tasks . text annotation is essential for machine learning-based nlp where annotated texts are used for both training and evaluating supervised systems . since an annotators behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation , analysing the process of text annotation has potential to reveal useful information for nlp tasks , in particular semantic and discourse processing that require deeper language understanding . we conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in japanese texts . our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches .

description of the ncu chinese word segmentation and named entity
asian languages are far from most western-style in their non-separate word sequence especially chinese . the preliminary step of asian-like language processing is to find the word boundaries between words . in this paper , we present a general purpose model for both chinese word segmentation and named entity recognition . this model was built on the word sequence classification with probability model , i.e. , conditional random fields ( crf ) . we used a simple feature set for crf which achieves satisfactory classification result on the two tasks . our model achieved 91.00 in f rate in upuctreebank data , and 78.71 for ner task .

unsupervised methods for developing taxonomies by combining syntactic and statistical information
this paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words . the algorithm works by first using a large corpus to find semantic neighbors of the unknown word , which we accomplish by combining latent semantic analysis with part-of-speech information . we then place the unknown word in the part of the taxonomy where these neighbors are most concentrated , using a class-labelling algorithm developed especially for this task . this method is used to reconstruct parts of the existing wordnet database , obtaining results for common nouns , proper nouns and verbs . we evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy .

predicting strong associations on the basis of corpus data
current approaches to the prediction of associations rely on just one type of information , generally taking the form of either word space models or collocation measures . at the moment , it is an open question how these approaches compare to one another . in this paper , we will investigate the performance of these two types of models and that of a new approach based on compounding . the best single predictor is the log-likelihood ratio , followed closely by the document-based word space model . we will show , however , that an ensemble method that combines these two best approaches with the compounding algorithm achieves an increase in performance of almost 30 % over the current state of the art .

extending nlp tools repositories for the interaction with language data resources repositories
this short paper presents some motivations behind the organization of the acl/eacl01 workshop on sharing tools and resources for research and education , concentrating on the possible connection of tools and resources repositories . taking some papers printed in this volume and the acl natural language software registry as a basis , we outline some of the steps to be done on the side of nlp tool repositories in order to achieve this goal .

automatic recognition of french expletive pronoun occurrences
we present a tool , called ilimp , which takes as input a raw text in french and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ ana ] for anaphoric or [ imp ] for impersonal or expletive . this tool is therefore designed to distinguish between the anaphoric occurrences of il , for which an anaphora resolution system has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent . the precision rate for ilimp is 97,5 % . the few errors are analyzed in detail . other tasks using the method developed for ilimp are described briefly , as well as the use of ilimp in a modular syntactic analysis system .

attribution and the ( non- ) alignment of syntactic and discourse arguments
the annotations of the penn discourse treebank ( pdtb ) include ( 1 ) discourse connectives and their arguments , and ( 2 ) attribution of each argument of each connective and of the relation it denotes . because the pdtb covers the same text as the penn treebank wsj corpus , syntactic and discourse annotation can be compared . this has revealed significant differences between syntactic structure and discourse structure , in terms of the arguments of connectives , due in large part to attribution . we describe these differences , an algorithm for detecting them , and finally some experimental results . these results have implications for automating discourse annotation based on syntactic annotation .

learning with conditional random fields
there is rich knowledge encoded in online web data . for example , punctuation and entity tags in wikipedia data define some word boundaries in a sentence . in this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised chinese word segmentation . the basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge . by integrating some domain adaptation techniques , such as easyadapt , our result reaches an f-measure of 95.98 % on the ctb-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .

using rbmt systems to produce bilingual corpus for smt
this paper proposes a method using the existing rule-based machine translation ( rbmt ) system as a black box to produce synthetic bilingual corpus , which will be used as training data for the statistical machine translation ( smt ) system . we use the existing rbmt system to translate the monolingual corpus into synthetic bilingual corpus . with the synthetic bilingual corpus , we can build an smt system even if there is no real bilingual corpus . in our experiments using bleu as a metric , the system achieves a relative improvement of 11.7 % over the best rbmt system that is used to produce the synthetic bilingual corpora . we also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora . the interpolated model achieves an absolute improvement of 0.0245 bleu score ( 13.1 % relative ) as compared with the individual model trained on the real bilingual corpus .

how much do word embeddings encode about syntax
do continuous word embeddings encode any useful information for constituency parsing we isolate three ways in which word embeddings might augment a stateof-the-art statistical parser : by connecting out-of-vocabulary words to known ones , by encouraging common behavior among related in-vocabulary words , and by directly providing features for the lexicon . we test each of these hypotheses with a targeted change to a state-of-the-art baseline . despite small gains on extremely small supervised training sets , we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data . our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways .

combining task and dialogue streams in unsupervised dialogue act models aysu ezen-can and kristy elizabeth boyer
unsupervised machine learning approaches hold great promise for recognizing dialogue acts , but the performance of these models tends to be much lower than the accuracies reached by supervised models . however , some dialogues , such as task-oriented dialogues with parallel task streams , hold rich information that has not yet been leveraged within unsupervised dialogue act models . this paper investigates incorporating task features into an unsupervised dialogue act model trained on a corpus of human tutoring in introductory computer science . experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification , particularly within a hierarchical framework that gives prominence to dialogue history . this work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems .

comparing phrase-based and syntax-based paraphrase generation
paraphrase generation can be regarded as machine translation where source and target language are the same . we use the moses statistical machine translation toolkit for paraphrasing , comparing phrase-based to syntax-based approaches . data is derived from a recently released , large scale ( 2.1m tokens ) paraphrase corpus for dutch . preliminary results indicate that the phrase-based approach performs better in terms of nist scores and produces paraphrases at a greater distance from the source .

interleaved semantic interpretation in environment-based parsing
this paper extends a polynomial-time parsing algorithm that resolves structural ambiguity in input sentences by calculating and comparing the denotations of rival constituents , given some model of the application environment ( schuler , 2001 ) . the algorithm is extended to incorporate a full set of logical operators , including quanti ers and conjunctions , into this calculation without increasing the complexity of the overall algorithm beyond polynomial time , both in terms of the length of the input and the number of entities in the environment model .

translating treebank annotation for evaluation
in this paper we discuss the need for corpora with a variety of annotations to provide suitable resources to evaluate different natural language processing systems and to compare them . a supervised machine learning technique is presented for translating corpora between syntactic formalisms and is applied to the task of translating the penn treebank annotation into a categorial grammar annotation . it is compared with a current alternative approach and results indicate annotation of broader coverage using a more compact grammar .

computationally rational saccadic control : an explanation of spillover effects based on sampling from noisy perception and memory
eye-movements in reading exhibit frequency spillover effects : fixation durations on a word are affected by the frequency of the previous word . we explore the idea that this effect may be an emergent property of a computationally rational eyemovement strategy that is navigating a tradeoff between processing immediate perceptual input , and continued processing of past input based on memory . we present an adaptive eye-movement control model with a minimal capacity for such processing , based on a composition of thresholded sequential samplers that integrate information from noisy perception and noisy memory . the model is applied to the list lexical decision task and shown to yield frequency spillovera robust property of human eye-movements in this task , even with parafoveal masking . we show that spillover in the model emerges in approximately optimal control policies that sometimes process memory rather than perception . we compare this model with one that is able to give priority to perception over memory , and show that the perception-priority policies in such a model do not perform as well in a range of plausible noise settings . we explain how the frequency spillover arises from a counter-intuitive but fundamental property of sequenced thresholded samplers .

application adaptive electronic dictionary with intelligent interface
the paper presents an electronic dictionary that can be adapted to the needs of different nlp applications . it suggests some ways to save on software customisation and acquisition effort through an intelligent developer interface . the emphasis is made on the flexibility of data representation , handling and access speed .

english to urdu statistical machine translation : establishing a
the aim of this paper is to categorize and present the existence of resources for englishto-urdu machine translation ( mt ) and to establish an empirical baseline for this task . by doing so , we hope to set up a common ground for mt research with urdu to allow for a congruent progress in this field . we build baseline phrase-based mt ( pbmt ) and hierarchical mt systems and report the results on 3 official independent test sets . on all test sets , hierarchial mt significantly outperformed pbmt . the highest single-reference bleu score is achieved by the hierarchical system and reaches 21.58 % but this figure depends on the randomly selected test set . our manual evaluation of 175 sentences suggests that in 45 % of sentences , the hierarchical mt is ranked better than the pbmt output compared to 21 % of sentences where pbmt wins , the rest being equal .

automatic classification of communicative functions of definiteness fatima talib al-raisi
definiteness expresses a constellation of semantic , pragmatic , and discourse propertiesthe communicative functionsof an np . we present a supervised classifier for english nps that uses lexical , morphological , and syntactic features to predict an nps communicative function in terms of a language-universal classification scheme . our classifiers establish strong baselines for future work in this neglected area of computational semantic analysis . in addition , analysis of the features and learned parameters in the model provides insight into the grammaticalization of definiteness in english , not all of which is obvious a priori .

japanese dependency parsing using co-occurrence information and a combination of case elements
in this paper , we present a method that improves japanese dependency parsing by using large-scale statistical information . it takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element . this information can be collected from the results of automatic dependency parsing of large-scale corpora . the results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .

focused web corpus crawling
in web corpus construction , crawling is a necessary step , and it is probably the most costly of all , because it requires expensive bandwidth usage , and excess crawling increases storage requirements . excess crawling results from the fact that the web contains a lot of redundant content ( duplicates and near-duplicates ) , as well as other material not suitable or desirable for inclusion in web corpora or web indexes ( for example , pages with little text or virtually no text at all ) . an optimized crawler for web corpus construction would ideally avoid crawling such content in the first place , saving bandwidth , storage , and post-processing costs . in this paper , we show in three experiments that two simple scores are suitable to improve the ratio between corpus size and crawling effort for web corpus construction . the first score is related to overall text quality of the page containing the link , the other one is related to the likelihood that the local block enclosing a link is boilerplate .

key event detection in video using asr and visual data
multimedia data grow day by day which makes it necessary to index them automatically and efficiently for fast retrieval , and more precisely to automatically index them with key events . in this paper , we present preliminary work on key event detection in british royal wedding videos using automatic speech recognition ( asr ) and visual data . the system first automatically acquires key events of royal weddings from an external corpus such as wikipedia , and then identifies those events in the asr data . the system also models name and face alignment to identify the persons involved in the wedding events . we compare the results obtained with the asr output with results obtained with subtitles . the error is only slightly higher when using asr output in the detection of key events and their participants in the wedding videos compared to the results obtained with subtitles .

a two-stage statistical word segmentation system for chinese
in this paper we present a two-stage statistical word segmentation system for chinese based on word bigram and wordformation models . this system was evaluated on peking university corpora at the first international chinese word segmentation bakeoff . we also give results and discussions on this evaluation .

coreference systems based on kernels methods
various types of structural information e.g. , about the type of constructions in which binding constraints apply , or about the structure of names - play a central role in coreference resolution , often in combination with lexical information ( as in expletive detection ) . kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations , but care is required to ensure they are exploited in the best possible fashion . in this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection , expletive identification , and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution .

weakly supervised learning methods for improving the quality of gene name normalization data the mitre corporation
a pervasive problem facing many biomedical text mining applications is that of correctly associating mentions of entities in the literature with corresponding concepts in a database or ontology . attempts to build systems for automating this process have shown promise as demonstrated by the recent biocreative task 1b evaluation . a significant obstacle to improved performance for this task , however , is a lack of high quality training data . in this work , we explore methods for improving the quality of ( noisy ) task 1b training data using variants of weakly supervised learning methods . we present positive results demonstrating that these methods result in an improvement in training data quality as measured by improved system performance over the same system using the originally labeled data .

discovering global patterns in linguistic networks through spectral analysis : a case study of the consonant inventories
recent research has shown that language and the socio-cognitive phenomena associated with it can be aptly modeled and visualized through networks of linguistic entities . however , most of the existing works on linguistic networks focus only on the local properties of the networks . this study is an attempt to analyze the structure of languages via a purely structural technique , namely spectral analysis , which is ideally suited for discovering the global correlations in a network . application of this technique to phonet , the co-occurrence network of consonants , not only reveals several natural linguistic principles governing the structure of the consonant inventories , but is also able to quantify their relative importance . we believe that this powerful technique can be successfully applied , in general , to study the structure of natural languages .

composition of semantic relations : model and applications
this paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed . a set of 26 relations is introduced , with their arguments defined on an ontology of sorts . a semantic parser is used to extract these relations from noun phrases and verb argument structures . the method was successfully used in two applications : rapid customization of semantic relations to arbitrary domains and recognizing entailments .

automatic generation of conversational utterances and narrative for augmentative and alternative communication : a prototype system martin dempster & norman alm ehud reiter
we detail the design , development and evaluation of augmentative and alternative communication ( aac ) software which encourages rapid conversational interaction . the system uses natural language generation ( nlg ) technology to automatically generate conversational utterances from a domain knowledge base modelled from content suggested by a small aac user group . findings from this work are presented along with a discussion about how nlg might be successfully applied to conversational aac systems in the future .

applying morphological decomposition to statistical machine translation
this paper describes the aalto submission for the german-to-english and the czechto-english translation tasks of the acl 2010 joint fifth workshop on statistical machine translation and metricsmatr . statistical machine translation has focused on using words , and longer phrases constructed from words , as tokens in the system . in contrast , we apply different morphological decompositions of words using the unsupervised morfessor algorithms . while translation models trained using the morphological decompositions did not improve the bleu scores , we show that the minimum bayes risk combination with a word-based translation model produces significant improvements for the germanto-english translation . however , we did not see improvements for the czech-toenglish translations .

unsupervised discovery of a statistical verb lexicon
this paper demonstrates how unsupervised techniques can be used to learn models of deep linguistic structure . determining the semantic roles of a verbs dependents is an important step in natural language understanding . we present a method for learning models of verb argument patterns directly from unannotated text . the learned models are similar to existing verb lexicons such as verbnet and propbank , but additionally include statistics about the linkings used by each verb . the method is based on a structured probabilistic model of the domain , and unsupervised learning is performed with the em algorithm . the learned models can also be used discriminatively as semantic role labelers , and when evaluated relative to the propbank annotation , the best learned model reduces 28 % of the error between an informed baseline and an oracle upper bound .

exemplar-based models for word meaning in context
this paper describes ongoing work on distributional models for word meaning in context . we abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences . on a paraphrasing task , we find that a simple exemplar model outperforms more complex state-of-the-art models .

identifying and ranking topic clusters in the blogosphere
the blogosphere is a huge collaboratively constructed resource containing diverse and rich information . this diversity and richness presents a significant research challenge to the information retrieval community . this paper addresses this challenge by proposing a method for identification of topic clusters within the blogosphere where topic clusters represent the concept of grouping together blogs sharing a common interest i.e . topic , the algorithm takes into account both the hyperlinked social network of blogs along with the content in the blog posts . additionally we use various forms and parts-of-speech of the topic to provide a broader coverage of the blogosphere . the next step of the method is to assign topic-specific ranks to each blog in the cluster using a metric called topic discussion rank , that helps in identifying the most influential blog for a specific topic . we also perform an experimental evaluation of our method on real blog data and show that the proposed method reaches a high level of accuracy .

microblog entity linking by leveraging extra posts
linking name mentions in microblog posts to a knowledge base , namely microblog entity linking , is useful for text mining tasks on microblog . entity linking in long text has been well studied in previous works . however few work has focused on short text such as microblog post . microblog posts are short and noisy . previous method can extract few features from the post context . in this paper we propose to use extra posts for the microblog entity linking task . experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3 % and 7.5 % respectively .

structural topic model for latent topical structure analysis
topic models have been successfully applied to many document analysis tasks to discover topics embedded in text . however , existing topic models generally can not capture the latent topical structures in documents . since languages are intrinsically cohesive and coherent , modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks . in this work , we propose a new topic model , structural topic model , which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order markov chain . experiment results show that the proposed structural topic model can effectively discover topical structures in text , and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering .

a unified model for word sense representation and disambiguation
most word representation methods assume that each word owns a single semantic vector . this is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation . in this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense . the basic idea is that both word sense representation ( wsr ) and word sense disambiguation ( wsd ) will benefit from each other : ( 1 ) highquality wsr will capture rich information about words and senses , which should be helpful for wsd , and ( 2 ) high-quality wsd will provide reliable disambiguated corpora for learning better sense representations . experimental results show that , our model improves the performance of contextual word similarity compared to existing wsr methods , outperforms stateof-the-art supervised methods on domainspecific wsd , and achieves competitive performance on coarse-grained all-words wsd .

frame semantics for stance classification kazi saidul hasan and vincent ng
determining the stance expressed by an author from a post written for a two-sided debate in an online debate forum is a relatively new problem in opinion mining . we extend a state-of-the-art learningbased approach to debate stance classification by ( 1 ) inducing lexico-syntactic patterns based on syntactic dependencies and semantic frames that aim to capture the meaning of a sentence and provide a generalized representation of it ; and ( 2 ) improving the classification of a test post via a novel way of exploiting the information in other test posts with the same stance . empirical results on four datasets demonstrate the effectiveness of our extensions .

coarse-to-fine syntactic machine translation using language projections
the intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding . we propose a multipass , coarse-to-fine approach in which the language model complexity is incrementally introduced . in contrast to previous orderbased bigram-to-trigram approaches , we focus on encoding-based methods , which use a clustered encoding of the target language . across various encoding schemes , and for multiple language pairs , we show speed-ups of up to 50 times over single-pass decoding while improving bleu score . moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder .

using web queries for learner error detection
we investigate the use of web search queries for detecting errors in non-native writing . distinguishing a correct sequence of words from a sequence with a learner error is a baseline task that any error detection and correction system needs to address . using a large corpus of error-annotated learner data , we investigate whether web search result counts can be used to distinguish correct from incorrect usage . in this investigation , we compare a variety of query formulation strategies and a number of web resources , including two major search engine apis and a large web-based n-gram corpus .

text alignment for real-time crowd captioning
the primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates . recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists , each of whom types part of what they hear . in this paper , we describe an improved method for combining partial captions into a final output based on weighted a search and multiple sequence alignment ( msa ) . in contrast to prior work , our method allows the tradeoff between accuracy and speed to be tuned , and provides formal error bounds . our method outperforms the current state-of-the-art on word error rate ( wer ) ( 29.6 % ) , bleu score ( 41.4 % ) , and f-measure ( 36.9 % ) . the end goal is for these captions to be used by people , and so we also compare how these metrics correlate with the judgments of 50 study participants , which may assist others looking to make further progress on this problem .

estimating time to event from tweets using temporal expressions
given a stream of twitter messages about an event , we investigate the predictive power of temporal expressions in the messages to estimate the time to event ( tte ) . from labeled training data we learn average tte estimates of temporal expressions and combinations thereof , and define basic rules to compute the time to event from temporal expressions , so that when they occur in a tweet that mentions an event we can generate a prediction . we show in a case study on soccer matches that our estimations are off by about eight hours on average in terms of mean absolute error .

dynamic feature selection for dependency parsing he he hal daume iii
feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing . we propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence . we model this as a sequential decision-making problem and solve it by imitation learning techniques . we test our method on 7 languages . our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .

improving the estimation of word importance for news multi-document
we introduce a supervised model for predicting word importance that incorporates a rich set of features . our model is superior to prior approaches for identifying words used in human summaries . moreover we show that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art .

maarten van gompel , iris hendrickx , antal van den bosch language and translation technology team ,
we present a new cross-lingual task for semeval concerning the translation of l1 fragments in an l2 context . the task is at the boundary of cross-lingual word sense disambiguation and machine translation . it finds its application in the field of computer-assisted translation , particularly in the context of second language learning . translating l1 fragments in an l2 context allows language learners when writing in a target language ( l2 ) to fall back to their native language ( l1 ) whenever they are uncertain of the right word or phrase .

factoring synchronous grammars by sorting
synchronous context-free grammars ( scfgs ) have been successfully exploited as translation models in machine translation applications . when parsing with an scfg , computational complexity grows exponentially with the length of the rules , in the worst case . in this paper we examine the problem of factorizing each rule of an input scfg to a generatively equivalent set of rules , each having the smallest possible length . our algorithm works in time o ( n log n ) , for each rule of length n. this improves upon previous results and solves an open problem about recognizing permutations that can be factored .

answering definition questions using multiple knowledge sources
definition questions represent a largely unexplored area of question answeringthey are different from factoid questions in that the goal is to return as many relevant nuggets of information about a concept as possible . we describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns , a webbased dictionary , and an off-the-shelf document retriever . results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the trec 2003 question answering track .

tight integration of speech disfluency removal into smt eunah cho jan niehues
speech disfluencies are one of the main challenges of spoken language processing . conventional disfluency detection systems deploy a hard decision , which can have a negative influence on subsequent applications such as machine translation . in this paper we suggest a novel approach in which disfluency detection is integrated into the translation process . we train a crf model to obtain a disfluency probability for each word . the smt decoder will then skip the potentially disfluent word based on its disfluency probability . using the suggested scheme , the translation score of both the manual transcript and asr output is improved by around 0.35 bleu points compared to the crf hard decision system .

extracting aspects of determiner meaning from dialogue in a virtual world environment
we use data from a virtual world game for automated learning of words and grammatical constructions and their meanings . the language data are an integral part of the social interaction in the game and consist of chat dialogue , which is only constrained by the cultural context , as set by the nature of the provided virtual environment . building on previous work , where we extracted a vocabulary for concrete objects in the game by making use of the non-linguistic context , we now target np/dp grammar , in particular determiners . we assume that we have captured the meanings of a set of determiners if we can predict which determiner will be used in a particular context . to this end we train a classifier that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context .

an lr-inspired generalized lexicalized phrase structure parser place paul ricoeur
the paper introduces an lr-based algorithm for efficient phrase structure parsing of morphologically rich languages . the algorithm generalizes lexicalized parsing ( collins , 2003 ) by allowing a structured representation of the lexical items . together with a discriminative weighting component ( collins , 2002 ) , we show that this representation allows us to achieve state of the art accurracy results on a morphologically rich language such as french while achieving more efficient parsing times than the state of the art parsers on the french data set . a comparison with english , a lexically poor language , is also provided .

deterministic word segmentation using maximum matching with fully lexicalized rules yahoo japan corporation
we present a fast algorithm of word segmentation that scans an input sentence in a deterministic manner just one time . the algorithm is based on simple maximum matching which includes execution of fully lexicalized transformational rules . since the process of rule matching is incorporated into dictionary lookup , fast segmentation is achieved . we evaluated the proposed method on word segmentation of japanese . experimental results show that our segmenter runs considerably faster than the state-of-the-art systems and yields a practical accuracy when a more accurate segmenter or an annotated corpus is available .

learning to relate literal and sentimental descriptions of visual properties computer science & engineering conversational understanding sciences computer science & engineering
language can describe our visual world at many levels , including not only what is literally there but also the sentiment that it invokes . in this paper , we study visual language , both literal and sentimental , that describes the overall appearance and style of virtual characters . sentimental properties , including labels such as youthful or country western , must be inferred from descriptions of the more literal properties , such as facial features and clothing selection . we present a new dataset , collected to describe xbox avatars , as well as models for learning the relationships between these avatars and their literal and sentimental descriptions . in a series of experiments , we demonstrate that such learned models can be used for a range of tasks , including predicting sentimental words and using them to rank and build avatars . together , these results demonstrate that sentimental language provides a concise ( though noisy ) means of specifying low-level visual properties .

example-based speech intention understanding and its application to in-car spoken dialogue system shigeki matsubara shinichi kimura nobuo kawaguchi
this paper proposes a method of speech intention understanding based on dialogue examples . the method uses a spoken dialogue corpus with intention tags to regard the intention of each input utterance as that of the sentence to which it is the most similar in the corpus . the degree of similarity is calculated according to the degree of correspondence in morphemes and dependencies between sentences , and it is weighted by the dialogue context information . an experiment on inference of utterance intentions using a large-scale in-car spoken dialogue corpus of ciair has shown 68.9 % accuracy . furthermore , we have developed a prototype system of in-car spoken dialogue processing for a restaurant retrieval task based on our method , and confirmed the feasiblity of the system .

orthonormal explicit topic analysis for cross-lingual document matching
cross-lingual topic modelling has applications in machine translation , word sense disambiguation and terminology alignment . multilingual extensions of approaches based on latent ( lsi ) , generative ( lda , plsi ) as well as explicit ( esa ) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages . in this paper , we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics , but then computes latent relations between these . thus , the method combines the benefits of both explicit and latent topic modelling approaches . we show that on a crosslingual mate retrieval task , our model significantly outperforms lda , lsi , and esa , as well as a baseline that translates every word in a document into the target language .

building bilingual lexicon to create dialect tunisian corpora and adapt language model mariem ellouze khemekhem lamia hadrich belguith
since the tunisian revolution , tunisian dialect ( td ) used in daily life , has became progressively used and represented in interviews , news and debate programs instead of modern standard arabic ( msa ) . this situation has important negative consequences for natural language processing ( nlp ) : since the spoken dialects are not officially written and do not have standard orthography , it is very costly to obtain adequate corpora to use for training nlp tools . furthermore , there are almost no parallel corpora involving td and msa . in this paper , we describe the creation of tunisian dialect text corpus as well as a method for building a bilingual dictionary , in order to create language model for speech recognition system for the tunisian broadcast news . so , we use explicit knowledge about the relation between td and msa .

jointly optimizing a two-step conditional random field model for machine transliteration and its fast decoding algorithm
this paper presents a joint optimization method of a two-step conditional random field ( crf ) model for machine transliteration and a fast decoding algorithm for the proposed method . our method lies in the category of direct orthographical mapping ( dom ) between two languages without using any intermediate phonemic mapping . in the two-step crf model , the first crf segments an input word into chunks and the second one converts each chunk into one unit in the target language . in this paper , we propose a method to jointly optimize the two-step crfs and also a fast algorithm to realize it . our experiments show that the proposed method outperforms the well-known joint source channel model ( jscm ) and our proposed fast algorithm decreases the decoding time significantly . furthermore , combination of the proposed method and the jscm gives further improvement , which outperforms state-of-the-art results in terms of top-1 accuracy .

generating entailment rules from framenet roni ben aharon
many nlp tasks need accurate knowledge for semantic inference . to this end , mostly wordnet is utilized . yet wordnet is limited , especially for inference between predicates . to help filling this gap , we present an algorithm that generates inference rules between predicates from framenet . our experiment shows that the novel resource is effective and complements wordnet in terms of rule coverage .

maise : a flexible , configurable , extensible open source package for mass ai system evaluation
the past few years have seen an increasing interest in using amazons mechanical turk for purposes of collecting data and performing annotation tasks . one such task is the mass evaluation of system output in a variety of tasks . in this paper , we present maise , a package that allows researchers to evaluate the output of their ai system ( s ) using human judgments collected via amazons mechanical turk , greatly streamlining the process . maise is open source , easy to run , and platform-independent . the core of maises codebase was used for the manual evaluation of wmt10 , and the completed package is being used again in the current evaluation for wmt11 . in this paper , we describe the main features , functionality , and usage of maise , which is now available for download and use .

when conset meets synset : a preliminary survey of an ontological lexical resource based on chinese characters
this paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on chinese characters . the ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource . philosophically , chinese ideogram has its ontological status , but its applicability to the nlp task has not been expressed explicitly in terms of language resource . we thus propose the first attempt to locate chinese characters within the context of ontology . having the primary success in applying it to some nlp tasks , we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of chinese lexical semantic resources .

a systematic comparison between inversion transduction grammar and linear transduction grammar for word alignment
we present two contributions to grammar driven translation . first , since both inversion transduction grammar and linear inversion transduction grammars have been shown to produce better alignments then the standard word alignment tool , we investigate how the trade-off between speed and end-to-end translation quality extends to the choice of grammar formalism . second , we prove that linear transduction grammars ( ltgs ) generate the same transductions as linear inversion transduction grammars , and present a scheme for arriving at ltgs by bilingualizing linear grammars . we also present a method for obtaining inversion transduction grammars from linear ( inversion ) transduction grammars , which can speed up grammar induction from parallel corpora dramatically .

integrating punctuation rules and nave bayesian model for chinese creation title recognition
over 7 % of named entities in chinese documents . they are the fourth large sort of named entities in chinese other than personal names , location names , and organization names . however , they are rarely mentioned and studied before . chinese title recognition is challenging for the following reasons . there are few internal features and nearly no restrictions in the naming style of titles . their lengths and structures are varied . the worst of all , they are generally composed of common words , so that they look like common fragments of sentences . in this paper , we integrate punctuation rules , lexicon , and nave bayesian models to recognize creation titles in chinese documents . this pioneer study shows a precision of 0.510 and a recall of 0.685 being achieved . the promising results can be integrated into chinese segmentation , used to retrieve relevant information for specific titles , and so on .

wikipedia as frame information repository
in this paper , we address the issue of automatic extending lexical resources by exploiting existing knowledge repositories . in particular , we deal with the new task of linking framenet and wikipedia using a word sense disambiguation system that , for a given pair frame lexical unit ( f , l ) , finds the wikipage that best expresses the the meaning of l. the mapping can be exploited to straightforwardly acquire new example sentences and new lexical units , both for english and for all languages available in wikipedia . in this way , it is possible to easily acquire good-quality data as a starting point for the creation of framenet in new languages . the evaluation reported both for the monolingual and the multilingual expansion of framenet shows that the approach is promising .

an agreement measure for determining inter-annotator reliability of human judgements on affective text
an affective text may be judged to belong to multiple affect categories as it may evoke different affects with varying degree of intensity . for affect classification of text , it is often required to annotate text corpus with affect categories . this task is often performed by a number of human judges . this paper presents a new agreement measure inspired by kappa coefficient to compute inter-annotator reliability when the annotators have freedom to categorize a text into more than one class . the extended reliability coefficient has been applied to measure the quality of an affective text corpus . an analysis of the factors that influence corpus quality has been provided .

parametric models of linguistic count data
it is well known that occurrence counts of words in documents are often modeled poorly by standard distributions like the binomial or poisson . observed counts vary more than simple models predict , prompting the use of overdispersed models like gamma-poisson or beta-binomial mixtures as robust alternatives . another deficiency of standard models is due to the fact that most words never occur in a given document , resulting in large amounts of zero counts . we propose using zeroinflated models for dealing with this , and evaluate competing models on a naive bayes text classification task . simple zero-inflated models can account for practically relevant variation , and can be easier to work with than overdispersed models .

cross lingual adaptation : an experiment on sentiment classifications
in this paper , we study the problem of using an annotated corpus in english for the same natural language processing task in another language . while various machine translation systems are available , automated translation is still far from perfect . to minimize the noise introduced by translations , we propose to use only key reliable parts from the translations and apply structural correspondence learning ( scl ) to find a low dimensional representation shared by the two languages . we perform experiments on an englishchinese sentiment classification task and compare our results with a previous cotraining approach . to alleviate the problem of data sparseness , we create extra pseudo-examples for scl by making queries to a search engine . experiments on real-world on-line review data demonstrate the two techniques can effectively improve the performance compared to previous work .

multi-layered image representation for image interpretation
in order to bridge the semantic gap between the visual context of an image and semantic concepts people would use to interpret it , we propose a multi-layered image representation model considering different amounts of knowledge needed for the interpretation of the image at each layer . interpretation results on different semantic layers of corel images related to outdoor scenes are presented and compared . obtained results show positive correlation of precision and recall with the abstract level of classes used for image annotation , i.e . more generalized classes have achieved better results .

syntax and semantics in quality estimation of machine translation
we employ syntactic and semantic information in estimating the quality of machine translation from a new data set which contains source text from english customer support forums and target text consisting of its machine translation into french . these translations have been both post-edited and evaluated by professional translators . we find that quality estimation using syntactic and semantic information on this data set can hardly improve over a baseline which uses only surface features . however , the performance can be improved when they are combined with such surface features . we also introduce a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments . while word alignments can be reliably used , the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling ( especially on ill-formed text ) and the lack of nominal predicate annotation .

two graph-based algorithms for state-of-the-art wsd
this paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora . our main contribution is the optimization of the free parameters of those algorithms and its evaluation against publicly available gold standards . we present a thorough evaluation comprising supervised and unsupervised modes , and both lexical-sample and all-words tasks . the results show that , in spite of the information loss inherent to mapping the induced senses to the gold-standard , the optimization of parameters based on a small sample of nouns carries over to all nouns , performing close to supervised systems in the lexical sample task and yielding the second-best wsd systems for the senseval-3 all-words task .

the impact of machine translation quality on human post-editing
we investigate the effect of four different competitive machine translation systems on post-editor productivity and behaviour . the study involves four volunteers postediting automatic translations of news stories from english to german . we see significant difference in productivity due to the systems ( about 20 % ) , and even bigger variance between post-editors .

multilingual mobile-phone translation services for world travelers
this demonstration introduces two new multilingual translation services for mobile phones . the first translation service provides state-of-the-art text-to-text translations of japanese as well as english conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus . the second demonstration is a speech translation service between japanese and english for real environments . it is based on distributed speech recognition with noise suppression . flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide .

machine translation by interaction
a machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes . we have implemented our prototype model for the japanese-chinese language pair . this paper describes our core idea of translation , where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input .

a language-independent transliteration schema using character praneeth shishtla , surya ganesh v , sethuramalingam subramaniam , vasudeva varma
in this paper we present a statistical transliteration technique that is language independent . this technique uses statistical alignment models and conditional random fields ( crf ) . statistical alignment models maximizes the probability of the observed ( source , target ) word pairs using the expectation maximization algorithm and then the character level alignments are set to maximum posterior predictions of the model . crf has efficient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solution .

bmm-based chinese word segmentor with word support model for
this paper describes a chinese word segmentor ( cws ) for the third international chinese language processing bakeoff ( sighan bakeoff 2006 ) . we participate in the word segmentation task at the microsoft research ( msr ) closed testing track . our cws is based on backward maximum matching with word support model ( wsm ) and contextual-based chinese unknown word identification . from the scored results and our experimental results , it shows wsm can improve our previous cws , which was reported at the sighan bakeoff 2005 , about 1 % of f-measure .

synchronization in an asynchronous agent-based architecture for dialogue systems
most dialogue architectures are either pipelined or , if agent-based , are restricted to a pipelined flowof-information . the trips dialogue architecture is agent-based and asynchronous , with several layers of information flow . we present this architecture and the synchronization issues we encountered in building a truly distributed , agentbased dialogue architecture .

a bayesian mixture model for part-of-speech induction using multiple features
in this paper we present a fully unsupervised syntactic class induction system formulated as a bayesian multinomial mixture model , where each word type is constrained to belong to a single class . by using a mixture model rather than a sequence model ( e.g. , hmm ) , we are able to easily add multiple kinds of features , including those at both the type level ( morphology features ) and token level ( context and alignment features , the latter from parallel corpora ) . using only context features , our system yields results comparable to state-of-the art , far better than a similar model without the one-class-per-type constraint . using the additional features provides added benefit , and our final system outperforms the best published results on most of the 25 corpora tested .

categorizing local contexts as a step in grammatical category
building on the use of local contexts , or frames , for human category acquisition , we explore the treatment of contexts as categories . this allows us to examine and evaluate the categorical properties that local unsupervised methods can distinguish and their relationship to corpus pos tags . from there , we use lexical information to combine contexts in a way which preserves the intended category , providing a platform for grammatical category induction .

uow : multi-task learning gaussian process for semantic textual similarity
we report results obtained by the uow method in semeval-2014s task 10 multilingual semantic textual similarity . we propose to model semantic textual similarity in the context of multi-task learning in order to deal with inherent challenges of the task such as unbalanced performance across domains and the lack of training data for some domains ( i.e . unknown domains ) . we show that the multi-task learning approach outperforms previous work on the 2012 dataset , achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset . we highlight the importance of the challenge of unknown domains , as it affects overall performance substantially .

syntactic re-alignment models for machine translation
we present a method for improving word alignment for statistical syntax-based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly-used word alignment models . this leads to extraction of more useful linguistic patterns and improved bleu scores on translation experiments in chinese and arabic .

structural variation in generated health reports
we present a natural language generator that produces a range of medical reports on the clinical histories of cancer patients , and discuss the problem of conceptual restatement in generating various textual views of the same conceptual content . we focus on two features of our system : the demand for loose paraphrases between the various reports on a given patient , with a high degree of semantic overlap but some necessary amount of distinctive content ; and the requirement for paraphrasing at primarily the discourse level .

using distributional similarity of multi-way translations to predict multiword expression compositionality
we predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression , based on translations into multiple languages . we evaluate the method over english noun compounds , english verb particle constructions and german noun compounds . we show that the estimation of compositionality is improved when using translations into multiple languages , as compared to simply using distributional similarity in the source language . we further find that string similarity complements distributional similarity .

automatic prediction of friendship via multi-model dyadic features
in this paper we focus on modeling friendships between humans as a way of working towards technology that can initiate and sustain a lifelong relationship with users . we do this by predicting friendship status in a dyad using a set of automatically harvested verbal and nonverbal features from videos of the interaction of students in a peer tutoring study . we propose a new computational model used to model friendship status in our data , based on a group sparse model ( gsm ) with l2,1 norm which is designed to accommodate the sparse and noisy properties of the multi-channel features . our gsm model achieved the best overall performance compared to a non-sparse linear model ( nlm ) and a regular sparse linear model ( slm ) , as well as outperforming human raters . dyadic features , such as number and length of conversational turns and mutual gaze , in addition to low level features such as f0 and gaze at task , were found to be good predictors of friendship status .

alena neviarouskaya helmut prendinger mitsuru ishizuka
the automatic analysis and classification of text using fine-grained attitude labels is the main task we address in our research . the developed @ am system relies on compositionality principle and a novel approach based on the rules elaborated for semantically distinct verb classes . the evaluation of our method on 1000 sentences , that describe personal experiences , showed promising results : average accuracy on fine-grained level was 62 % , on middle level 71 % , and on top level 88 % .

using gazetteers in discriminative information extraction division of informatics division of informatics
much work on information extraction has successfully used gazetteers to recognise uncommon entities that can not be reliably identified from local context alone . approaches to such tasks often involve the use of maximum entropy-style models , where gazetteers usually appear as highly informative features in the model . although such features can improve model accuracy , they can also introduce hidden negative effects . in this paper we describe and analyse these effects and suggest ways in which they may be overcome . in particular , we show that by quarantining gazetteer features and training them in a separate model , then decoding using a logarithmic opinion pool ( smith et al , 2005 ) , we may achieve much higher accuracy . finally , we suggest ways in which other features with gazetteer feature-like behaviour may be identified .

kernel-based approach for automatic evaluation of natural language generation technologies : application to automatic summarization
in order to promote the study of automatic summarization and translation , we need an accurate automatic evaluation method that is close to human evaluation . in this paper , we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures . we conducted an experiment using automatic summarization evaluation data developed for text summarization challenge 3 ( tsc-3 ) . a comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust .

morpho-syntactic arabic preprocessing for arabic-to-english statistical anas el isbihani shahram khadivi oliver bender
the arabic language has far richer systems of inflection and derivation than english which has very little morphology . this morphology difference causes a large gap between the vocabulary sizes in any given parallel training corpus . segmentation of inflected arabic words is a way to smooth its highly morphological nature . in this paper , we describe some statistically and linguistically motivated methods for arabic word segmentation . then , we show the efficiency of proposed methods on the arabic-english btec and nist tasks .

improving vector space word representations using multilingual correlation
the distributional hypothesis of harris ( 1954 ) , according to which the meaning of words is evidenced by the contexts they occur in , has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora . this paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis ( cca ) for incorporating multilingual evidence into vectors generated monolingually . we evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques .

language resources for a network-based dictionary
in order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of zock ( 2002 ) . however , contrary to zock who proposes just a paradigmatic network with information about the various ways in which words are similar we would like to present several existing language resources ( lrs ) which will be integrated in such a network resulting in more linguistic levels than one with paradigmatically associated words . we argue that just as the mental lexicon exhibits various , possibly interwoven layers of networks , electronic lrs containing syntagmatic , morphological and phonological information need to be integrated into an associative electronic dictionary .

evaluation of features for sentence extraction on different types of corpora
we report evaluation results for our summarization system and analyze the resulting summarization data for three different types of corpora . to develop a robust summarization system , we have created a system based on sentence extraction and applied it to summarize japanese and english newspaper articles , obtained some of the top results at two evaluation workshops . we have also created sentence extraction data from japanese lectures and evaluated our system with these data . in addition to the evaluation results , we analyze the relationships between key sentences and the features used in sentence extraction . we find that discrete combinations of features match distributions of key sentences better than sequential combinations .

evaluating answers to definition questions
this paper describes an initial evaluation of systems that answer questions seeking definitions . the results suggest that humans agree sufficiently as to what the basic concepts that should be included in the definition of a particular subject are to permit the computation of concept recall . computing concept precision is more problematic , however . using the length in characters of a definition is a crude approximation to concept precision that is nonetheless sufficient to correlate with humans subjective assessment of definition quality . the trec question answering track has sponsored a series of evaluations of systems abilities to answer closed class questions in many domains ( voorhees , 2001 ) . closed class questions are fact-based , short answer questions . the evaluation of qa systems for closed class questions is relatively simple because a response to such a question can be meaningfully judged on a binary scale of right/wrong . increasing the complexity of the question type even slightly significantly increases the difficulty of the evaluation because partial credit for responses must then be accommodated . the arda aquaint1 program is a research initiative sponsored by the u.s. department of defense aimed at increasing the kinds and difficulty of the questions automatic systems can answer . a series of pilot evaluations has been planned as part of the research agenda of the aquaint program .

encoding information on metaphoric expressions
in this paper we address the issue of the encoding of information on metaphors in a wordnet-like database , i.e . the italian wordnet in eurowordnet ( italwordnet ) . when analysing corpus data we find a huge number of metaphoric expressions which can be hardly dealt with by using as reference database italwordnet . in particular , we have compared information contained both in dictionaries of italian and in italwordnet with actual uses of words found in a corpus . we thus put forward proposals to enrich a resource like italwordnet with relevant information .

pos tagging of english-hindi code-mixed social media content jatin sharma kalika bali monojit choudhury
code-mixing is frequently observed in user generated content on social media , especially from multilingual users . the linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar . we describe our initial efforts to create a multi-level annotated corpus of hindi-english codemixed text collated from facebook forums , and explore language identification , back-transliteration , normalization and pos tagging of this data . our results show that language identification and transliteration for hindi are two major challenges that impact pos tagging accuracy .

words vector representations meet machine translation eva martnez garcia
distributed vector representations of words are useful in various nlp tasks . we briefly review the cbow approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of machine translation . the primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup .

dynamic use of ontologies in dialogue systems joana paulo pardal
most dialogue systems are built with a single task in mind . this makes the extension of an existing system one of the major problems in the field as large parts of the system have to be modified . some recent work has shown that ontologies have a role on the domain knowledge representation as the knowledge collected in an ontology can be used in all the modules . this work aims to follow the footsteps of the use of ontologies in dialogue systems and take it further as the current state of the art only uses taxonomical knowledge .

towards learning rules from natural texts
in this paper , we consider the problem of inductively learning rules from specific facts extracted from texts . this problem is challenging due to two reasons . first , natural texts are radically incomplete since there are always too many facts to mention . second , natural texts are systematically biased towards novelty and surprise , which presents an unrepresentative sample to the learner . our solutions to these two problems are based on building a generative observation model of what is mentioned and what is extracted given what is true . we first present a multiple-predicate bootstrapping approach that consists of iteratively learning if-then rules based on an implicit observation model and then imputing new facts implied by the learned rules . second , we present an iterative ensemble colearning approach , where multiple decisiontrees are learned from bootstrap samples of the incomplete training data , and facts are imputed based on weighted majority .

named entity recognition : different approaches
the talk deals with different approaches used for named entity recognition and how they are used in developing a robust named entity recognizer . the talk includes the development of tagset for ner and manual annotation of text .

chinese word segmentation in ftrd beijing france telecom r & d beijing france telecom r & d beijing france telecom r & d beijing france telecom r & d beijing
this paper presents a word segmentation system in france telecom r & d beijing , which uses a unified approach to word breaking and oov identification . the output can be customized to meet different segmentation standards through the application of an ordered list of transformation . the system participated in all the tracks of the segmentation bakeoff -- pk-open , pkclosed , as-open , as-closed , hk-open , hk-closed , msr-open and msrclosed -- and achieved the state-of-theart performance in msr-open , msrclose and pk-open tracks . analysis of the results shows that each component of the system contributed to the scores .

latent-variable modeling of string transductions with finite-state methods
string-to-string transduction is a central problem in computational linguistics and natural language processing . it occurs in tasks as diverse as name transliteration , spelling correction , pronunciation modeling and inflectional morphology . we present a conditional loglinear model for string-to-string transduction , which employs overlapping features over latent alignment sequences , and which learns latent classes and latent string pair regions from incomplete training data . we evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results , even when trained on small data sets . on the task of generating morphological forms , we outperform a baseline method reducing the error rate by up to 48 % . on a lemmatization task , we reduce the error rates in wicentowski ( 2002 ) by 3892 % .

generating with a grammar based on tree descriptions : a
while the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps , the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles . we present a generator based on the axiomatic view and argue that when combined with a tag-like grammar and a flat semantics , this axiomatic view permits avoiding drawbacks known to hold either of top-down or of bottom-up generators .

improving word alignment with language model based confidence scores
this paper describes the statistical machine translation systems submitted to the acl-wmt 2008 shared translation task . systems were submitted for two translation directions : englishspanish and spanishenglish . using sentence pair confidence scores estimated with source and target language models , improvements are observed on the newscommentary test sets . genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated .

dynamic language models for streaming text
we present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features . these context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself . we learn our model in an efficient online fashion that is scalable for large , streaming data . with five streaming datasets from two different genres economics news articles and social mediawe evaluate our model on the task of sequential language modeling . our model consistently outperforms competing models .

precise medication extraction using agile text mining
agile text mining is widely used for commercial text mining in the pharmaceutical industry . it can be applied without building an annotated training corpus , so is well-suited to novel or one-off extraction tasks . in this work we wanted to see how efficiently it could be adapted for healthcare extraction tasks such as medication extraction . the aim was to identify medication names , associated dosage , route of administration , frequency , duration and reason , as specified in the 2009 i2b2 medication challenge . queries were constructed based on 696 discharge summaries available as training data . performance was measured on a test dataset of 251 unseen documents . f1-scores were calculated by comparing system annotations against ground truth provided for the test data . despite the short amount of time spent in adapting the system to this task , it achieved high precision and reasonable recall ( precision of 0.92 , recall of 0.715 ) . it would have ranked fourth in comparison to the original challenge participants on the basis of its f-score of 0.805 for phrase level horizontal evaluation . this shows that agile text mining is an effective approach towards information extraction that can yield highly accurate results .

a feature-enriched tree kernel for relation extraction
tree kernel is an effective technique for relation extraction . however , the traditional syntactic tree representation is often too coarse or ambiguous to accurately capture the semantic relation information between two entities . in this paper , we propose a new tree kernel , called feature-enriched tree kernel ( ftk ) , which can enhance the traditional tree kernel by : 1 ) refining the syntactic tree representation by annotating each tree node with a set of discriminant features ; and 2 ) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration . experimental results show that our method can achieve a 5.4 % f-measure improvement over the traditional convolution tree kernel .

atilf ( analyse et traitement jacques dendien atilf ( analyse et traitement atilf ( analyse et traitement
this paper presents one of the main computerized resources of the research laboratory atilf ( analyse et traitement informatique de la langue franaise ) available via the web : the computerized dictionary tlfi ( trsor de la langue franaise informatis ) . its highly detailed xml structure ( over 3,6 million tags ) is powered by stella : the extended capacities and potentialities of this software allows high level queries as well as hypernavigation through and between different databases .

chinese word segmentation with multiple postprocessors
this paper presents the results of the system irlas1 from hit-irlab in the second international chinese word segmentation bakeoff . irlas consists of several basic components and multiple postprocessors . the basic components include basic segmentation , factoid recognition , and named entity recognition . these components maintain a segment graph together . the postprocessors include merging of adjoining words , morphologically derived word recognition , and new word identification . these postprocessors do some modifications on the best word sequence which is selected from the segment graph . our system participated in the open and closed tracks of pk corpus and ranked # 4 and # 3 respectively . our scores were very close to the highest level . it proves that our system has reached the current state of the art .

verification and validation of language processing systems : is it
if natural language processing ( nlp ) systems are viewed as intelligent systems then we should be able to make use of verification and validation ( v & v ) approaches and methods that have been developed in the intelligent systems community . this paper addresses language engineering infrastructure issues by considering whether standard v & v methods are fundamentally different than the evaluation practices commonly used for nlp systems , and proposes practical approaches for applying v & v in the context of language processing systems . we argue that evaluation , as it is performed in the nl community , can be improved by supplementing it with methods from the v & v community .

parse selection with a german hpsg grammar
we report on some recent parse selection experiments carried out with gg , a large-scale hpsg grammar for german . using a manually disambiguated treebank derived from the verbmobil corpus , we achieve over 81 % exact match accuracy compared to a 21.4 % random baseline , corresponding to an error reduction rate of 3.8 .

identifying broken plurals in unvowelised arabic text
irregular ( so-called broken ) plural identification in modern standard arabic is a problematic issue for information retrieval ( ir ) and language engineering applications , but their effect on the performance of ir has never been examined . broken plurals ( bps ) are formed by altering the singular ( as in english : tooth teeth ) through an application of interdigitating patterns on stems , and singular words can not be recovered by standard affix stripping stemming techniques . we developed several methods for bp detection , and evaluated them using an unseen test set . we incorporated the bp detection component into a new light-stemming algorithm that conflates both regular and broken plurals with their singular forms . we also evaluated the new light-stemming algorithm within the context of information retrieval , comparing its performance with other stemming algorithms .

learning a stopping criterion for active learning for word sense
in this paper , we address the problem of knowing when to stop the process of active learning . we propose a new statistical learning approach , called minimum expected error strategy , to defining a stopping criterion through estimation of the classifiers expected error on future unlabeled examples in the active learning process . in experiments on active learning for word sense disambiguation and text classification tasks , experimental results show that the new proposed stopping criterion can reduce approximately 50 % human labeling costs in word sense disambiguation with degradation of 0.5 % average accuracy , and approximately 90 % costs in text classification with degradation of 2 % average accuracy .

using paraphrases for parameter tuning in statistical machine translation
most state-of-the-art statistical machine translation systems use log-linear models , which are defined in terms of hypothesis features and weights for those features . it is standard to tune the feature weights in order to maximize a translation quality metric , using held-out test sentences and their corresponding reference translations . however , obtaining reference translations is expensive . in this paper , we introduce a new full-sentence paraphrase technique , based on english-to-english decoding with an mt system , and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning , without a significant decrease in translation quality .

manawi : using multi-word expressions and named entities to improve
we describe the manawi 1 ( manev ) system submitted to the 2014 wmt translation shared task . we participated in the english-hindi ( en-hi ) and hindi-english ( hi-en ) language pair and achieved 0.792 for the translation error rate ( ter ) score 2 for en-hi , the lowest among the competing systems . our main innovations are ( i ) the usage of outputs from nlp tools , viz . billingual multi-word expression extractor and named-entity recognizer to improve smt quality and ( ii )

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

emdros a text database engine for analyzed or annotated text
emdros is a text database engine for linguistic analysis or annotation of text . it is appliccable especially in corpus linguistics for storing and retrieving linguistic analyses of text , at any linguistic level . emdros implements the emdf text database model and the mql query language . in this paper , i present both , and give an example of how emdros can be useful in computational linguistics .

minimum imputed risk : unsupervised discriminative training for
discriminative training for machine translation has been well studied in the recent past . a limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training . we present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough reverse translation system . intuitively , our method strives to ensure that probabilistic round-trip translation from a targetlanguage sentence to the source-language and back will have low expected loss . theoretically , this may be justified as ( discriminatively ) minimizing an imputed empirical risk . empirically , we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both iwslt and nist tasks .

measuring syntactic difference in british english
recent work by nerbonne and wiersma ( 2006 ) has provided a foundation for measuring syntactic differences between corpora . it uses part-of-speech trigrams as an approximation to syntactic structure , comparing the trigrams of two corpora for statistically significant differences . this paper extends the method and its application . it extends the method by using leafpath ancestors of sampson ( 2000 ) instead of trigrams , which capture internal syntactic structureevery leaf in a parse tree records the path back to the root . the corpus used for testing is the international corpus of english , great britain ( nelson et al , 2002 ) , which contains syntactically annotated speech of great britain . the speakers are grouped into geographical regions based on place of birth . this is different in both nature and number than previous experiments , which found differences between two groups of norwegian l2 learners of english . we show that dialectal variation in eleven british regions from the icegb is detectable by our algorithm , using both leaf-ancestor paths and trigrams .

a machine learning-based coreference detection system for ontonotes
in this paper , we describe the algorithms and experimental results of brandeis university in the participation of the conll task 2011 closed track . we report the features used in our system , and describe a novel cluster-based chaining algorithm to improve performance of coreference identification . we evaluate the system using the ontonotes data set and describe our results .

active learning for the identification of nonliteral language
in this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs . the model uses nearly unsupervised word-sense disambiguation and clustering techniques . we report on experiments in which a human expert is asked to correct system predictions in different stages of learning : ( i ) after the last iteration when the clustering step has converged , or ( ii ) during each iteration of the clustering algorithm . the model obtains an f-score of 53.8 % on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts . in comparison , the same model augmented with active learning obtains 64.91 % . we also measure the number of examples required when model confidence is used to select examples for human correction as compared to random selection . the results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context .

topic independent identification of agreement and disagreement in social media dialogue
research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available . this has impacted the dialogue research communitys ability to develop better theories , as well as good off-the-shelf tools for dialogue processing . happily , an increasing amount of information and opinion exchange occur in natural dialogue in online forums , where people share their opinions about a vast range of topics . in particular we are interested in rejection in dialogue , also called disagreement and denial , where the size of available dialogue corpora , for the first time , offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue . in this paper , we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way . our results show that our theoretically motivated features achieve 66 % accuracy , an improvement over a unigram baseline of an absolute 6 % .

support vector machines for paraphrase identification and corpus construction
the lack of readily-available large corpora of aligned monolingual sentence pairs is a major obstacle to the development of statistical machine translation-based paraphrase models . in this paper , we describe the use of annotated datasets and support vector machines to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the world wide web . features include : morphological variants ; wordnet synonyms and hypernyms ; loglikelihood-based word pairings dynamically obtained from baseline sentence alignments ; and formal string features such as word-based edit distance . use of this technique dramatically reduces the alignment error rate of the extracted corpora over heuristic methods based on position of the sentences in the text .

hedge trimmer : a parse-and-trim approach to headline generation
this paper presents hedge trimmer , a headline generation system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline . we present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story . in addition , we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a hmm-based model , using both human evaluation and automatic metrics for comparing the two approaches .

analysis of summarization evaluation experiments
the goals of my dissertation are : 1 ) to propose a french terminology for the presentation of evaluation results of automatic summaries , 2 ) to identify and describe experimental variables in evaluations of automatic summaries , 3 ) to highlight the most common tendencies , inconsistencies and methodological problems in summarization evaluation experiments , and 4 ) to make recommendations for the presentation of evaluation results of automatic summaries . in this paper , i focus on the second objective , i.e . identifying and describing variables in summarization evaluation experiments .

predicting response to political blog posts with topic models
in this paper we model discussions in online political blogs . to do this , we extend latent dirichlet allocation ( blei et al , 2003 ) , in various ways to capture different characteristics of the data . our models jointly describe the generation of the primary documents ( posts ) as well as the authorship and , optionally , the contents of the blog communitys verbal reactions to each post ( comments ) . we evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post . we also provide a qualitative discussion about what the models discover .

annotation and data mining of the penn discourse treebank
the penn discourse treebank ( pdtb ) is a new resource built on top of the penn wall street journal corpus , in which discourse connectives are annotated along with their arguments . its use of standoff annotation allows integration with a stand-off version of the penn treebank ( syntactic structure ) and propbank ( verbs and their arguments ) , which adds value for both linguistic discovery and discourse modeling . here we describe the pdtb and some experiments in linguistic discovery based on the pdtb alone , as well as on the linked ptb and pdtb corpora .

fine-grained tree-to-string translation rule extraction xianchao wu takuya matsuzaki junichi tsujii
tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems . in this paper , we propose to use deep syntactic information for obtaining fine-grained translation rules . a head-driven phrase structure grammar ( hpsg ) parser is used to obtain the deep syntactic information , which includes a fine-grained description of the syntactic property and a semantic representation of a sentence . we extract fine-grained rules from aligned hpsg tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems . extensive experiments on largescale bidirectional japanese-english translations testified the effectiveness of our approach .

plaesarn : machine-aided translation tool for english-to-thai and intelligent information system technology
english-thai mt systems are nowadays restricted by incomplete vocabularies and translation knowledge . users must consequently accept only one translation result that is sometimes semantically divergent or ungrammatical . with the according reason , we propose novel internet-based translation assistant software in order to facilitate document translation from english to thai . in this project , we utilize the structural transfer model as the mechanism . this project differs from current english-thai mt systems in the aspects that it empowers the users to manually select the most appropriate translation from every possibility and to manually train new translation rules to the system if it is necessary . with the applied model , we overcome four translation problemslexicon rearrangement , structural ambiguity , phrase translation , and classifier generation . finally , we started the system evaluation with 322 randomly selected sentences on the future magazine bilingual corpus and the system yielded 59.87 % and 83.08 % translation accuracy for the best case and the worse case based on 90.1 % average precision of the parser .

upv-wsd : combining different wsd methods by means of fuzzy borda voting
this paper describes the wsd system developed for our participation to the semeval-1 . it combines various methods by means of a fuzzy borda voting . the fuzzy borda votecounting scheme is one of the best known methods in the field of collective decision making . in our system the different disambiguation methods are considered as experts that give a preference ranking for the senses a word can be assigned . then the preferences are evaluated using the fuzzy borda scheme in order to select the best sense . the methods we considered are the sense frequency probability calculated over semcor , the conceptual density calculated over both hyperonyms and meronyms hyerarchies in wordnet , the extended lesk by banerjee and pedersen , and finally a method based on wordnet domains .

confidence driven unsupervised semantic parsing
current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain . this supervision bottleneck is one of the major difficulties in scaling up semantic parsing . we argue that a semantic parser can be trained effectively without annotated data , and introduce an unsupervised learning algorithm . the algorithm takes a self training approach driven by confidence estimation . evaluated over geoquery , a standard dataset for this task , our system achieved 66 % accuracy , compared to 80 % of its fully supervised counterpart , demonstrating the promise of unsupervised approaches for this task .

unsupervised construction of a lexicon and a repository of variation patterns for arabic modal multiword expressions
we present an unsupervised approach to build a lexicon of arabic modal multiword expressions ( am-mwes ) and a repository of their variation patterns . these novel resources are likely to boost the automatic identification and extraction of am-mwes1 .

automatic seed word selection for unsupervised sentiment classification of chinese text
we describe and evaluate a new method of automatic seed word selection for unsupervised sentiment classification of product reviews in chinese . the whole method is unsupervised and does not require any annotated training data ; it only requires information about commonly occurring negations and adverbials . unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods . the results obtained are close to those of supervised classifiers and sometimes better , up to an f1 of 92 % .

generating constituent order in german clauses
we investigate the factors which determine constituent order in german clauses and propose an algorithm which performs the task in two steps : first , the best candidate for the initial sentence position is chosen . then , the order for the remaining constituents is determined . the first task is more difficult than the second one because of properties of the german sentence-initial position . experiments show a significant improvement over competing approaches . our algorithm is also more efficient than these .

prior disambiguation of word tensors for constructing sentence vectors and computer science mile end road
recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit

chinese sentence segmentation as comma classification
we describe a method for disambiguating chinese commas that is central to chinese sentence segmentation . chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas . trained and tested on data derived from the chinese treebank , our model achieves a classification accuracy of close to 90 % overall , which translates to an f1 score of 70 % for detecting commas that signal sentence boundaries .

chinese word segmentation and named entity recognition based on conditional random fields models
this paper mainly describes a chinese named entity recognition ( ner ) system ner @ iscas , which integrates text , part-of-speech and a small-vocabularycharacter-lists feature for msra ner open track under the framework of conditional random fields ( crfs ) model . the techniques used for the close ner and word segmentation tracks are also presented .

dkpro tc : a java-based framework for supervised learning experiments on textual data and torsten zesch
we present dkpro tc , a framework for supervised learning experiments on textual data . the main goal of dkpro tc is to enable researchers to focus on the actual research task behind the learning problem and let the framework handle the rest . it enables rapid prototyping of experiments by relying on an easy-to-use workflow engine and standardized document preprocessing based on the apache unstructured information management architecture ( ferrucci and lally , 2004 ) . it ships with standard feature extraction modules , while at the same time allowing the user to add customized extractors . the extensive reporting and logging facilities make dkpro tc experiments fully replicable .

joint parsing and named entity recognition
for many language technology applications , such as question answering , the overall system runs several independent processors over the data ( such as a named entity recognizer , a coreference system , and a parser ) . this easily results in inconsistent annotations , which are harmful to the performance of the aggregate system . we begin to address this problem with a joint model of parsing and named entity recognition , based on a discriminative feature-based constituency parser . our model produces a consistent output , where the named entity spans do not conflict with the phrasal spans of the parse tree . the joint representation also allows the information from each type of annotation to improve performance on the other , and , in experiments with the ontonotes corpus , we found improvements of up to 1.36 % absolute f1 for parsing , and up to 9.0 % f1 for named entity recognition .

analysis and detection of reading miscues for interactive literacy tutors
the colorado literacy tutor ( clt ) is a technology-based literacy program , designed on the basis of cognitive theory and scientifically motivated reading research , which aims to improve literacy and student achievement in public schools . one of the critical components of the clt is a speech recognition system which is used to track the childs progress during oral reading and to provide sufficient information to detect reading miscues . in this paper , we extend on prior work by examining a novel labeling of childrens oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors . while these events make up nearly 8 % of the data , they are shown to account for approximately 30 % of the word errors in a state-of-the-art speech recognizer . next , we consider the problem of detecting miscues during oral reading . using features derived from the speech recognizer , we demonstrate that 67 % of reading miscues can be detected at a false alarm rate of 3 % .

collective opinion target extraction in chinese microblogs
microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style . in this paper , we study the problem of extracting opinion targets of chinese microblog messages . such fine-grained word-level task has not been well investigated in microblogs yet . we propose an unsupervised label propagation algorithm to address the problem . the opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets . topics in microblogs are identified by hashtags or using clustering algorithms . experimental results on chinese microblogs show the effectiveness of our framework and algorithms .

a semantic approach to textual entailment :
this paper discusses our contribution to the third rte challenge the salsa rte system . it builds on an earlier system based on a relatively deep linguistic analysis , which we complement with a shallow component based on word overlap . we evaluate their ( combined ) performance on various data sets . however , earlier observations that the combination of features improves the overall accuracy could be replicated only partly .

intex : a syntactic role driven protein-protein interaction extractor for bio-medical text
in this paper , we present a fully automated extraction system , named intex , to identify gene and protein interactions in biomedical text . our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles . then , tagging biological entities with the help of biomedical and linguistic ontologies . finally , extracting complete interactions by analyzing the matching contents of syntactic roles and their linguistically significant combinations . our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence . experimental evaluations with two other state of the art extraction systems indicate that the intex system achieves better performance without the labor intensive pattern engineering requirement .

dkpro similarity : an open source framework for text similarity
we present dkpro similarity , an open source framework for text similarity . our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces . dkpro similarity comprises a wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural , stylistic , and phonetic measures . in order to promote the reproducibility of experimental results and to provide reliable , permanent experimental conditions for future studies , dkpro similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon .

evaluating models of latent document semantics in the presence of ocr errors
models of latent document semantics such as the mixture of multinomials model and latent dirichlet allocation have received substantial attention for their ability to discover topical semantics in large collections of text . in an effort to apply such models to noisy optical character recognition ( ocr ) text output , we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling . we show the effects both with document-level topic analysis ( document clustering ) and with word-level topic analysis ( lda ) on both synthetic and real-world ocr data . as expected , experimental results show that performance declines as word error rates increase . common techniques for alleviating these problems , such as filtering low-frequency words , are successful in enhancing model quality , but exhibit failure trends similar to models trained on unprocessed ocr output in the case of lda . to our knowledge , this study is the first of its kind .

rethinking chinese word segmentation : tokenization , character classification , or wordbreak identification
this paper addresses two remaining challenges in chinese word segmentation . the challenge in hlt is to find a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new types of data . the challenge in modelling human cognition and acquisition it to segment words efficiently without using knowledge of wordhood . we propose a radical method of word segmentation to meet both challenges . the most critical concept that we introduce is that chinese word segmentation is the classification of a string of character-boundaries ( cbs ) into either word-boundaries ( wbs ) and non-word-boundaries . in chinese , cbs are delimited and distributed in between two characters . hence we can use the distributional properties of cb among the background character strings to predict which cbs are wbs .

lexical relationships from temporal patterns of web search queries enrique alfonseca massimiliano ciaramita keith hall
in this paper we investigate temporal patterns of web search queries . we carry out several evaluations to analyze the properties of temporal profiles of queries , revealing promising semantic and pragmatic relationships between words . we focus on two applications : query suggestion and query categorization . the former shows a potential for time-series similarity measures to identify specific semantic relatedness between words , which results in state-of-the-art performance in query suggestion while providing complementary information to more traditional distributional similarity measures . the query categorization evaluation suggests that the temporal profile alone is not a strong indicator of broad topical categories .

unsupervised approaches for automatic keyword extraction using
this paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts . in the tfidf ( term frequency , inverse document frequency ) weighting framework , we incorporated partof-speech ( pos ) information , word clustering , and sentence salience score . we also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words . the system performance is evaluated in different ways , including comparison to human annotated keywords using f-measure and a weighted score relative to the oracle system performance , as well as a novel alternative human evaluation . our results have shown that the simple unsupervised tfidf approach performs reasonably well , and the additional information from pos and sentence score helps keyword extraction . however , the graph method is less effective for this domain . experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts .

estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging
we present a hmm part-of-speech tagging method which is particularly suited for pos tagsets with a large number of fine-grained tags . it is based on three ideas : ( 1 ) splitting of the pos tags into attribute vectors and decomposition of the contextual pos probabilities of the hmm into a product of attribute probabilities , ( 2 ) estimation of the contextual probabilities with decision trees , and ( 3 ) use of high-order hmms . in experiments on german and czech data , our tagger outperformed stateof-the-art pos taggers .

everyday language is highly
there has recently been a great deal of work aimed at trying to extract information from substantial texts for tasks such as question answering . much of this work has dealt with texts which are reasonably large , but which are known to contain reliable relevant information , e.g . faq lists , on-line encyclopaedias , rather than looking at huge unorganised resources such as the web . we believe , however , that even this work underestimates the complexity and subtlety of language , and hence will inevitably be restricted in what it can cope with . in particular , everyday use of language involves considerable amounts of reasoning over intensional objects ( properties and propositions ) . in order to respond appropriately to simple-seeming questions such as is going for a walk good for me , for instance , you have to be able to talk about event-types , which are intrinsically intensional . we discuss the issues involved in handling such items , and shows the kind of background knowledge that is required for drawing the appropriate conclusions about them .

how spoken language corpora can refine current speech motor training methodologies
the growing availability of spoken language corpora presents new opportunities for enriching the methodologies of speech and language therapy . in this paper , we present a novel approach for constructing speech motor exercises , based on linguistic knowledge extracted from spoken language corpora . in our study with the dutch spoken corpus , syllabic inventories were obtained by means of automatic syllabification of the spoken language data . our experimental syllabification method exhibited a reliable performance , and allowed for the acquisition of syllabic tokens from the corpus . consequently , the syllabic tokens were integrated in a tool for clinicians , a result which holds the potential of contributing to the current state of speech motor training methodologies .

alexandre denis , guillaume pitel , matthieu quignard
this paper presents an extension to the reference domain theory ( salmon-alt , 2001 ) in order to solve plural references . while this theory doesnt take plural reference into account in its original form , this paper shows how several entities can be grouped together by building a new domain and how they can be accessed later on . we introduce the notion of super-domain , representing the access structure to all the plural referents of a given type .

optimizing textual entailment recognition using particle swarm fbk - irst
this paper introduces a new method to improve tree edit distance approach to textual entailment recognition , using particle swarm optimization . currently , one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations , which is a difficult and challenging task in dealing with the entailment problem and datasets . we tried to estimate the cost of edit operations in tree edit distance algorithm automatically , in order to improve the results for textual entailment . automatically estimating the optimal values of the cost operations over all rte development datasets , we proved a significant enhancement in accuracy obtained on the test sets .

learning better monolingual models with unannotated bilingual text
this work shows how to improve state-of-the-art monolingual natural language processing models using unannotated bilingual text . we build a multiview learning objective that enforces agreement between monolingual and bilingual models . in our method the first , monolingual view consists of supervised predictors learned separately for each language . the second , bilingual view consists of log-linear predictors learned over both languages on bilingual text . our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views . for the task of named entity recognition , using bilingual predictors increases f1 by 16.1 % absolute over a supervised monolingual model , and retraining on bilingual predictions increases monolingual model f1 by 14.6 % . for syntactic parsing , our bilingual predictor increases f1 by 2.1 % absolute , and retraining a monolingual model on its output gives an improvement of 2.0 % .

exploiting structure for event discovery using the mdi algorithm
effectively identifying events in unstructured text is a very difficult task . this is largely due to the fact that an individual event can be expressed by several sentences . in this paper , we investigate the use of clustering methods for the task of grouping the text spans in a news article that refer to the same event . the key idea is to cluster the sentences , using a novel distance metric that exploits regularities in the sequential structure of events within a document . when this approach is compared to a simple bag of words baseline , a statistically significant increase in performance is observed .

dialog input ranking in a multi-domain environment using transferable belief model
this paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input . the information under consideration is the intended task to be performed and the information provided for the completion of the task . as an example , we use the task of information access in a multi-domain dialogue system . currently , the system contains knowledge of ten different domains . callers calling in are greeted with an open-ended how may i help you prompt ( thomson and wisowaty , 1999 ; chu-carroll and carpenter , 1999 ; gorin et al , 1997 ) . after receiving a reply from the caller , we extract word evidences from the recognized utterances . by using transferable belief model ( tbm ) , we in turn determine the task that the caller intends to perform as well as any information provided .

integrating multi-level linguistic knowledge with a unified framework for mandarin speech recognition
to improve the mandarin large vocabulary continuous speech recognition ( lvcsr ) , a unified framework based approach is introduced to exploit multi-level linguistic knowledge . in this framework , each knowledge source is represented by a weighted finite state transducer ( wfst ) , and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources . due to the uniform transducer representation , any knowledge source can be easily integrated into the analyzer , as long as it can be encoded into wfsts . moreover , as the knowledge in each level is modeled independently and the combination is processed in the model level , the information inherently in each knowledge source has a chance to be thoroughly exploited . by simulations , the effectiveness of the analyzer is investigated , and then a lvcsr system embedding the presented analyzer is evaluated . experimental results reveal that this unified framework is an effective approach which significantly improves the performance of speech recognition with a 9.9 % relative reduction of character error rate on the hub-4 test set , a widely used mandarin speech recognition task .

expressing owl axioms by english sentences : dubious in theory , feasible in practice
with owl ( web ontology language ) established as a standard for encoding ontologies on the semantic web , interest has begun to focus on the task of verbalising owl code in controlled english ( or other natural language ) . current approaches to this task assume that axioms in owl can be mapped to sentences in english . we examine three potential problems with this approach ( concerning logical sophistication , information structure , and size ) , and show that although these could in theory lead to insuperable difficulties , in practice they seldom arise , because ontology developers use owl in ways that favour a transparent mapping . this result is evidenced by an analysis of patterns from a corpus of over 600,000 axioms in about 200 ontologies .

computational analysis to explore authors depiction of characters cecilia ovesdotter alm
this study involves automatically identifying the sociolinguistic characteristics of fictional characters in plays by analyzing their written speech . we discuss three binary classification problems : predicting the characters gender ( male vs. female ) , age ( young vs. old ) , and socioeconomic standing ( upper-middle class vs. lower class ) . the text corpus used is an annotated collection of august strindberg and henrik ibsen plays , translated into english , which are in the public domain . these playwrights were chosen for their known attention to relevant socioeconomic issues in their work . linguistic and textual cues are extracted from the characters lines ( turns ) for modeling purposes . we report on the dataset as well as the performance and important features when predicting each of the sociolinguistic characteristics , comparing intra- and inter-author testing .

inference of phrase-based translation models via minimum description length
we present an unsupervised inference procedure for phrase-based translation models based on the minimum description length principle . in comparison to current inference techniques that rely on long pipelines of training heuristics , this procedure represents a theoretically wellfounded approach to directly infer phrase lexicons . empirical results show that the proposed inference procedure has the potential to overcome many of the problems inherent to the current inference approaches for phrase-based models .

entity annotation based on inverse index operations
entity annotation involves attaching a label such as name or organization to a sequence of tokens in a document . all the current rule-based and machine learningbased approaches for this task operate at the document level . we present a new and generic approach to entity annotation which uses the inverse index typically created for rapid key-word based searching of a document collection . we define a set of operations on the inverse index that allows us to create annotations defined by cascading regular expressions . the entity annotations for an entire document corpus can be created purely of the index with no need to access the original documents . experiments on two publicly available data sets show very significant performance improvements over the documentbased annotators .

catching the drift : probabilistic content models , with applications to
we consider the problem of modeling the content structure of texts within a specific domain , in terms of the topics the texts address and the order in which these topics appear . we first present an effective knowledge-lean method for learning content models from unannotated documents , utilizing a novel adaptation of algorithms for hidden markov models . we then apply our method to two complementary tasks : information ordering and extractive summarization . our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods .

learning condensed feature representations from large unsupervised data sets for supervised learning
this paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning . we use unsupervised data to generate informative condensed feature representations from the original feature set used in supervised nlp systems . the main contribution of our method is that it can offer dense and low-dimensional feature spaces for nlp tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique . our method matches the results of current state-of-the-art systems with very few features , i.e. , f-score 90.72 with 344 features for conll-2003 ner data , and uas 93.55 with 12.5k features for dependency parsing data derived from ptb-iii .

automatic keyphrase extraction : a survey of the state of the art kazi saidul hasan and vincent ng
while automatic keyphrase extraction has been examined extensively , state-of-theart performance on this task is still much lower than that on many core natural language processing tasks . we present a survey of the state of the art in automatic keyphrase extraction , examining the major sources of errors made by existing systems and discussing the challenges ahead .

recursive deep models for semantic compositionality over a sentiment treebank
semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way . further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition . to remedy this , we introduce a sentiment treebank . it includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality . to address them , we introduce the recursive neural tensor network . when trained on the new treebank , this model outperforms all previous methods on several metrics . it pushes the state of the art in single sentence positive/negative classification from 80 % up to 85.4 % . the accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7 % , an improvement of 9.7 % over bag of features baselines . lastly , it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases .

k-means and graph-based approaches for chinese word sense
this paper details our experiments carried out at word sense induction task . for the foreign language ( especially english ) , there have been many studies of word sense induction ( wsi ) , and the approaches and the techniques are more and more mature . however , the study of chinese wsi is just getting started , and there has not been a better way to solve the problems encountered . wsi can be divided into two categories : supervised manner and unsupervised manner . but in the light of the high cost of supervised manner , we introduce novel solutions to automatic and unsupervised wsi . in this paper , we propose two different systems . the first one is called k-means-based chinese word sense induction in an unsupervised manner while the second one is graph-based chinese word sense induction . in the experiments , the first system has achieved a 0.7729 fscore on average while the second one has achieved a 0.6067 fscore .

modelling annotator bias with multi-task gaussian processes : an application to machine translation quality estimation
annotating linguistic data is often a complex , time consuming and expensive endeavour . even with strict annotation guidelines , human subjects often deviate in their analyses , each bringing different biases , interpretations of the task and levels of consistency . we present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour . these techniques use multi-task gaussian processes to learn jointly a series of annotator and metadata specific models , while explicitly representing correlations between models which can be learned directly from data . our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning , and consistently outperform strong baselines .

identifying opinion holders and targets with dependency parser in chinese news texts
in this paper , we propose to identify opinion holders and targets with dependency parser in chinese news texts , i.e . to identify opinion holders by means of reporting verbs and to identify opinion targets by considering both opinion holders and opinion-bearing words . the experiments with ntcir-7 moats chinese test data show that our approach provides better performance than the baselines and most systems reported at ntcir-7 .

arabic tokenization system
tokenization is a necessary and non-trivial step in natural language processing . in the case of arabic , where a single word can comprise up to four independent tokens , morphological knowledge needs to be incorporated into the tokenizer . in this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage ( white space normalizer ) , and a post-processing stage ( token filter ) . we also show how it handles multiword expressions , and how ambiguity is resolved .

enforcing transitivity in coreference resolution
a desirable quality of a coreference resolution system is the ability to handle transitivity constraints , such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions , it will also consider the likelihood of those two mentions being coreferent when making a final assignment . this is exactly the kind of constraint that integer linear programming ( ilp ) is ideal for , but , surprisingly , previous work applying ilp to coreference resolution has not encoded this type of constraint . we train a coreference classifier over pairs of mentions , and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments . we present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance , including improvements of up to 3.6 % using the b3 scorer , and up to 16.5 % using cluster f-measure .

generation of referring expressions : managing structural
existing algorithms for the generation of referring expressions tend to generate distinguishing descriptions at the semantic level , disregarding the ways in which surface issues can affect their quality . this paper considers how these algorithms should deal with surface ambiguity , focussing on structural ambiguity . we propose that not all ambiguity is worth avoiding , and suggest some ways forward that attempt to avoid unwanted interpretations . we sketch the design of an algorithm motivated by our experimental findings .

text summarization of turkish texts using latent semantic analysis makbule gulcin ozsoy ferda nur alpaslan
text summarization solves the problem of extracting important information from huge amount of text data . there are various methods in the literature that aim to find out well-formed summaries . one of the most commonly used methods is the latent semantic analysis ( lsa ) . in this paper , different lsa based summarization algorithms are explained and two new lsa based summarization algorithms are proposed . the algorithms are evaluated on turkish documents , and their performances are compared using their rouge-l scores . one of our algorithms produces the best scores .

application of the tightness continuum measure to chinese information retrieval
most word segmentation methods employed in chinese information retrieval systems are based on a static dictionary or a model trained against a manually segmented corpus . these general segmentation approaches may not be optimal because they disregard information within semantic units . we propose a novel method for improving word-based chinese ir , which performs segmentation according to the tightness of phrases . in order to evaluate the effectiveness of our method , we employ a new test collection of 203 queries , which include a broad distribution of phrases with different tightness values . the results of our experiments indicate that our method improves ir performance as compared with a general word segmentation approach . the experiments also demonstrate the need for the development of better evaluation corpora .

reducing vsm data sparseness by generalizing contexts : application to health text mining
vector space models are limited with low frequency words due to few available contexts and data sparseness . to tackle this problem , we generalize contexts by integrating semantic relations acquired with linguistic approaches . we use three methods that acquire hypernymy relations on a ehr corpus . context generalization obtains the best results when performed with hypernyms , the quality of the relations being more important than the quantity .

on-line language model biasing for statistical machine translation raytheon bbn technologies
the language model ( lm ) is a critical component in most statistical machine translation ( smt ) systems , serving to establish a probability distribution over the hypothesis space . most smt systems use a static lm , independent of the source language input . while previous work has shown that adapting lms based on the input improves smt performance , none of the techniques has thus far been shown to be feasible for on-line systems . in this paper , we develop a novel measure of cross-lingual similarity for biasing the lm based on the test input . we also illustrate an efficient on-line implementation that supports integration with on-line smt systems by transferring much of the computational load off-line . our approach yields significant reductions in target perplexity compared to the static lm , as well as consistent improvements in smt performance across language pairs ( english-dari and english-pashto ) .

latent semantic grammar induction :
this paper presents latent semantic grammars for the unsupervised induction of english grammar . latent semantic grammars were induced by applying singular value decomposition to n-gram by context-feature matrices . parsing was used to evaluate performance . experiments with context , projectivity , and prior distributions show the relative performance effects of these kinds of prior knowledge . results show that prior distributions , projectivity , and part of speech information are not necessary to beat the right branching baseline .

language models based on semantic composition
in this paper we propose a novel statistical language model to capture long-range semantic dependencies . specifically , we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words . we also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones . the composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone . we also obtain perplexity reductions when integrating our models with a structured language model .

combining pcfg-la models with dual decomposition : a case study with joseph le roux , antoine rozenknop , jennifer foster
it has recently been shown that different nlp models can be effectively combined using dual decomposition . in this paper we demonstrate that pcfg-la parsing models are suitable for combination in this way . we experiment with the different models which result from alternative methods of extracting a grammar from a treebank ( retaining or discarding function labels , left binarization versus right binarization ) and achieve a labeled parseval f-score of 92.4 on wall street journal section 23 this represents an absolute improvement of 0.7 and an error reduction rate of 7 % over a strong pcfg-la product-model baseline . although we experiment only with binarization and function labels in this study , there is much scope for applying this approach to other grammar extraction strategies .

analysis and synthesis of the distribution of consonants over languages : a complex network approach
cross-linguistic similarities are reflected by the speech sound systems of languages all over the world . in this work we try to model such similarities observed in the consonant inventories , through a complex bipartite network . we present a systematic study of some of the appealing features of these inventories with the help of the bipartite network . an important observation is that the occurrence of consonants follows a two regime power law distribution . we find that the consonant inventory size distribution together with the principle of preferential attachment are the main reasons behind the emergence of such a two regime behavior . in order to further support our explanation we present a synthesis model for this network based on the general theory of preferential attachment .

criterion for judging request intention in response texts of open-ended questionnaires
our general research aim is to extract the actual intentions of persons when they respond to open-ended questionnaires . these intentions include the desire to make requests , complaints , expressions of resignation and so forth , but here we focus on extracting the intention to make a request . to do so , we first have to judge whether their responses contain the intent to make a request . therefore , as a first step , we have developed a criterion for judging the existence of request intentions in responses . this criterion , which is based on paraphrasing , is described in detail in this paper . our assumption is that a response with request intentions can be paraphrased into a typical request expression , e.g. , i would like to ... , while responses without request are not paraphrasable . the criterion is evaluated in terms of objectivity , reproducibility and effectiveness . objectivity is demonstrated by showing that machine learning methods can learn the criterion from a set of intention-tagged data , while reproducibility , that the judgments of three annotators are reasonably consistent , and effectiveness , that judgments based not on the criterion but on intuition do not agree . this means the criterion is necessary to achieve reproducibility . these experiments indicate that the criterion can be used to judge the existence of request intentions in responses reliably .

the fewer , the better a contrastive study about ways to simplify
simplified texts play an important role in providing accessible and easy-to-understand information for a whole range of users who , due to linguistic , developmental or social barriers , would have difficulty in understanding materials which are not adapted and/or simplified . however , the production of simplified texts can be a time-consuming and labour-intensive task . in this paper we show that the employment of a short list of simple simplification rules could result in texts of comparable readability to those written as a result of applying a long list of more fine-grained rules . we also prove that the simplification process based on the short list of simple rules is more time efficient and consistent .

user goal change model for spoken dialog state tracking
in this paper , a maximum entropy markov model ( memm ) for dialog state tracking is proposed to efficiently handle user goal evolvement in two steps . the system first predicts the occurrence of a user goal change based on linguistic features and dialog context for each dialog turn , and then the proposed model could utilize this user goal change information to infer the most probable dialog state sequence which underlies the evolvement of user goal during the dialog . it is believed that with the suggested various domain independent feature functions , the proposed model could better exploit not only the intra-dependencies within long asr n-best lists but also the inter-dependencies of the observations across dialog turns , which leads to more efficient and accurate dialog state inference .

part of speech tagging for amharic using conditional random fields sisay fissaha adafre
we applied conditional random fields ( crfs ) to the tasks of amharic word segmentation and pos tagging using a small annotated corpus of 1000 words . given the size of the data and the large number of unknown words in the test corpus ( 80 % ) , an accuracy of 84 % for amharic word segmentation and 74 % for pos tagging is encouraging , indicating the applicability of crfs for a morphologically complex language like amharic .

improving tree-to-tree translation with packed forests
current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding . we instead propose a forest-based tree-to-tree model that uses packed forests . the model is based on a probabilistic synchronous tree substitution grammar ( stsg ) , which can be learned from aligned forest pairs automatically . the decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of stsg while building target forest in parallel . comparable to the state-of-the-art phrase-based system moses , using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 bleu points over using 1-best trees .

language identification in code-switching scenario riyaz ahmad bhat
this paper describes a crf based token level language identification system entry to language identification in codeswitched ( cs ) data task of codeswitch 2014. our system hinges on using conditional posterior probabilities for the individual codes ( words ) in code-switched data to solve the language identification task . we also experiment with other linguistically motivated language specific as well as generic features to train the crf based sequence labeling algorithm achieving reasonable results .

detecting speculative language using syntactic dependencies and logistic regression
in this paper we describe our approach to the conll-2010 shared task on detecting speculative language in biomedical text . we treat the detection of sentences containing uncertain information ( task1 ) as a token classification task since the existence or absence of cues determines the sentence label . we distinguish words that have speculative and non-speculative meaning by employing syntactic features as a proxy for their semantic content . in order to identify the scope of each cue ( task2 ) , we learn a classifier that predicts whether each token of a sentence belongs to the scope of a given cue . the features in the classifier are based on the syntactic dependency path between the cue and the token . in both tasks , we use a bayesian logistic regression classifier incorporating a sparsity-enforcing laplace prior . overall , the performance achieved is 85.21 % f-score and 44.11 % f-score in task1 and task2 , respectively .

enhancing cross document coreference of web documents with context similarity and very large scale text categorization lockheed martin is & gs
cross document coreference ( cdc ) is the task of constructing the coreference chain for mentions of a person across a set of documents . this work offers a holistic view of using document-level categories , sub-document level context and extracted entities and relations for the cdc task . we train a categorization component with an efficient flat algorithm using thousands of odp categories and over a million web documents . we propose to use ranked categories as coreference information , particularly suitable for web documents that are widely different in style and content . an ensemble composite coreference function , amenable to inactive features , combines these three levels of evidence for disambiguation . a thorough feature importance study is conducted to analyze how these three components contribute to the coreference results . the overall solution is evaluated using the weps benchmark data and demonstrate superior performance .

on the predictability of human assessment : when matrix completion meets nlp evaluation
this paper tackles the problem of collecting reliable human assessments . we show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality . to reduce the cost of collecting these multiple ratings , we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings . even if prediction performance is pretty low , decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example .

question answering using enhanced lexical semantic models wen-tau yih ming-wei chang christopher meek andrzej pastusiak
in this paper , we study the answer sentence selection problem for question answering . unlike previous work , which primarily leverages syntactic analysis through dependency tree matching , we focus on improving the performance using models of lexical semantic resources . experiments show that our systems can be consistently and significantly improved with rich lexical semantic information , regardless of the choice of learning algorithms . when evaluated on a benchmark dataset , the map and mrr scores are increased by 8 to 10 points , compared to one of our baseline systems using only surface-form matching . moreover , our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin .

an integrated architecture for generating parenthetical constructions
the aim of this research is to provide a principled account of the generation of embedded constructions ( called parentheticals ) and to implement the results in a natural language generation system . parenthetical constructions are frequently used in texts written in a good writing style and have an important role in text understanding . we propose a framework to model the rhetorical properties of parentheticals based on a corpus study and develop a unified natural language generation architecture which integrates syntax , semantics , rhetorical and document structure into a complex representation , which can be easily extended to handle parentheticals .

phonotactic probability and the maori passive : a computational approach oiwi parker jones
two analyses of maori passives and gerunds have been debated in the literature . both assume that the thematic consonants in these forms are unpredictable . this paper reports on three computational experiments designed to test whether this assumption is sound . the results suggest that thematic consonants are predictable from the phonotactic probabilities of their active counterparts . this study has potential implications for allomorphy in other polynesian languages . it also exemplifies the benefits of using computational methods in linguistic analyses .

emotion classification using massive examples extracted from the web nagakute aichi japan ikoma nara japan
in this paper , we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance . we first fully automatically obtain a huge collection of emotion-provoking event instances from the web . with japanese chosen as a target language , about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns . we then decompose the emotion classification task into two sub-steps : sentiment polarity classification ( coarsegrained emotion classification ) , and emotion classification ( fine-grained emotion classification ) . for each subtask , the collection of emotion-proviking event instances is used as labelled examples to train a classifier . the results of our experiments indicate that our method significantly outperforms the baseline method . we also find that compared with the singlestep model , which applies the emotion classifier directly to inputs , our two-step model significantly reduces sentiment polarity errors , which are considered fatal errors in real dialog applications .

multi-human dialogue understanding for assisting
in this paper we present the dialogueunderstanding components of an architecture for assisting multi-human conversations in artifact-producing meetings : meetings in which tangible products such as project planning charts are created . novel aspects of our system include multimodal ambiguity resolution , modular ontologydriven artifact manipulation , and a meeting browser for use during and after meetings . we describe the software architecture and demonstrate the system using an example multimodal dialogue .

dependency forest for statistical machine translation
we propose a structure called dependency forest for statistical machine translation . a dependency forest compactly represents multiple dependency trees . we develop new algorithms for extracting string-todependency rules and training dependency language models . our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 bleu points over the tree-based baseline on the nist 2004/2005/2006 chinese-english test sets .

improving semantic role classification with selectional preferences
this work incorporates selectional preferences ( sp ) into a semantic role ( sr ) classification system . we learn separate selectional preferences for noun phrases and prepositional phrases and we integrate them in a state-of-the-art sr classification system both in the form of features and individual class predictors . we show that the inclusion of the refined sps yields statistically significant improvements on both in domain and out of domain data ( 14.07 % and 11.67 % error reduction , respectively ) . the key factor for success is the combination of several sp methods with the original classification model using metaclassification .

interpretation of chinese discourse connectives for explicit discourse relation recognition
this paper addresses the specific features of chinese discourse connectives , including types ( word-pair and single-word ) , linking directions ( forward and backward linking ) , positions and ambiguous degrees , and discusses how they affect the discourse relation recognition . a semisupervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset . the statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions , multiple discourse functions of connectives , and couple-linking elements providing strong clues for discourse relation resolution .

an example-based decoder for spoken language machine translation
in this paper , we propose an example-based decoder for a statistical machine translation ( smt ) system , which is used for spoken language machine translation . in this way , it will help to solve the re-ordering problem and other problems for spoken language mt , such as lots of omissions , idioms etc . through experiments , we show that this approach obtains improvements over the baseline on a chinese-english spoken language translation task .

text linkage in the wiki medium a comparative study
we analyze four different types of document networks with respect to their small world characteristics . these characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks . the study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems . it puts emphasize on their topological differences as a result of wikirelated linking compared to other textbased networks .

computational lexicography : a feature-based approach in designing an e-dictionary of chinese classifiers
chinese noun classifiers are obligatory as a category in association with nouns . conventional dictionaries include classifiers as lexical entries but explanations given are very brief and thus hardly helpful for l2 learners . this paper presents a new design of an e-dictionary of chinese classifiers . the design is based on both theoretical studies of chinese classifiers and empirical studies of chinese classifier acquisition by both children and adults . my main argument with regards to chinese classifier acquisition is that cognitive strategies with a bottom-up approach are the key to the understanding of the complexity of classifier and noun associations . the noun-dependent semantic features of classifiers are evidence to support my argument . these features are categorically defined and stored in a separated database in an e-learning environment linked to the e-dictionary . the aim of making such a design is to provide a platform for l2 learners to explore and learn with a bottom-up approach the associations of classifiers with nouns . the computational agent-based model that automatically links noun features to that of classifiers is the technical part of the design that will be described in detail in the paper . future development of the e-dictionary will be discussed as well .

a system to solve language tests for second grade students
this paper describes a system which solves language tests for second grade students ( 7 years old ) . in japan , there are materials for students to measure understanding of what they studied , just like sat for high school students in us . we use textbooks for the students as the target material of this study . questions in the materials are classified into four types : questions about chinese character ( kanji ) , about word knowledge , reading comprehension , and composition . this program doesnt resolve the composition and some other questions which are not easy to be implemented in text forms . we built a subsystem for each finer type of questions . as a result , we achieved 55 % - 83 % accuracy in answering questions in unseen materials .

type-aware distantly supervised relation extraction with linked arguments
distant supervision has become the leading method for training large-scale relation extractors , with nearly universal adoption in recent tac knowledge-base population competitions . however , there are still many questions about the best way to learn such extractors . in this paper we investigate four orthogonal improvements : integrating named entity linking ( nel ) and coreference resolution into argument identification for training and extraction , enforcing type constraints of linked arguments , and partitioning the model by relation type signature . we evaluate sentential extraction performance on two datasets : the popular set of ny times articles partially annotated by hoffmann et al . ( 2011 ) and a new dataset , called goreco , that is comprehensively annotated for 48 common relations . we find that using nel for argument identification boosts performance over the traditional approach ( named entity recognition with string match ) , and there is further improvement from using argument types . our best system boosts precision by 44 % and recall by 70 % .

a feature type classification for therapeutic purposes : a preliminary evaluation with non-expert speakers
we propose a feature type classification thought to be used in a therapeutic context . such a scenario lays behind our need for a easily usable and cognitively plausible classification . nevertheless , our proposal has both a practical and a theoretical outcome , and its applications range from computational linguistics to psycholinguistics . an evaluation through inter-coder agreement has been performed to highlight the strength of our proposal and to conceive some improvements for the future .

a robust and extensible exemplar-based model of thematic fit
this paper presents a new , exemplar-based model of thematic fit . in contrast to previous models , it does not approximate thematic fit as argument plausibility or fit with verb selectional preferences , but directly as semantic role plausibility for a verb-argument pair , through similaritybased generalization from previously seen verb-argument pairs . this makes the model very robust for data sparsity . we argue that the model is easily extensible to a model of semantic role ambiguity resolution during online sentence comprehension . the model is evaluated on human semantic role plausibility judgments . its predictions correlate significantly with the human judgments . it rivals two state-of-theart models of thematic fit and exceeds their performance on previously unseen or lowfrequency items .

benchmarking noun compound interpretation su nam kim and timothy baldwin
in this paper we provide benchmark results for two classes of methods used in interpreting noun compounds ( ncs ) : semantic similarity-based methods and their hybrids . we evaluate the methods using 7-way and binary class data from the nominal pair interpretation task of semeval-2007.1 we summarize and analyse our results , with the intention of providing a framework for benchmarking future research in this area .

growing finely-discriminating taxonomies from seeds of varying quality and size
concept taxonomies offer a powerful means for organizing knowledge , but this organization must allow for many overlapping and fine-grained perspectives if a general-purpose taxonomy is to reflect concepts as they are actually employed and reasoned about in everyday usage . we present here a means of bootstrapping finely-discriminating taxonomies from a variety of different starting points , or seeds , that are acquired from three different sources : wordnet , conceptnet and the web at large .

a tabular method for dynamic oracles in transition-based parsing
we develop parsing oracles for two transition-based dependency parsers , including the arc-standard parser , solving a problem that was left open in ( goldberg and nivre , 2013 ) . we experimentally show that using these oracles during training yields superior parsing accuracies on many languages .

report on the first nlg challenge on generating instructions in virtual environments ( give )
we describe the first installment of the challenge on generating instructions in virtual environments ( give ) , a new shared task for the nlg community . we motivate the design of the challenge , describe how we carried it out , and discuss the results of the system evaluation .

a unified model of phrasal and sentential evidence for information extraction
information extraction ( ie ) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it . often , however , role fillers occur in clauses that are not directly linked to an event word . we present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework . our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences . we evaluate our system on two ie data sets and show that our model performs well in comparison to existing ie systems that rely on local phrasal context .

real-time web text classification and analysis of reading difficulty
the automatic analysis and categorization of web text has witnessed a booming interest due to the increased text availability of different formats , content , genre and authorship . we present a new tool that searches the web and performs in real-time a ) html-free text extraction , b ) classification for thematic content and c ) evaluation of expected reading difficulty . this tool will be useful to adolescent and adult low-level reading students who face , among other challenges , a troubling lack of reading material for their age , interests and reading level .

incorporating topic information into sentiment analysis models
this paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic . the results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information .

evaluating neighbor rank and distance measures as predictors of semantic
this paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments . the tasks at issue are ( i ) identification of consistent primes based on their semantic relatedness to the target and ( ii ) correlation of semantic relatedness with latency times . we also provide an evaluation of the impact of specific model parameters on the prediction of priming . to the best of our knowledge , this is the first systematic evaluation of a wide range of dsm parameters in all possible combinations . an important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming .

symbolic preference using simple scoring
despite the popularity of stochastic parsers , symbolic parsing still has some advantages , but is not practical without an effective mechanism for selecting among alternative analyses . this paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . the hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy . the preference system is novel in using a simple , three-valued scoring method ( -1 , 0 , or +1 ) for assigning preferences to constituents viewed in the context of their containing constituents . the approach addresses problems associated with earlier preference systems , and has considerably facilitated development . it is ultimately based on viewing preference scoring as an engineering mechanism , and only indirectly related to cognitive principles or corpus-based frequencies .

a weakly supervised learning approach for spoken language understanding
in this paper , we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems . we model the task of spoken language understanding as a successive classification problem . the first classifier ( topic classifier ) is used to identify the topic of an input utterance . with the restriction of the recognized target topic , the second classifier ( semantic classifier ) is trained to extract the corresponding slot-value pairs . it is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language . most importantly , it allows the employment of weakly supervised strategies for training the two classifiers . we first apply the training strategy of combining active learning and self-training ( tur et al , 2005 ) for topic classifier . also , we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences . experiments have been conducted in the context of chinese public transportation information inquiry domain . the experimental results demonstrate the effectiveness of our proposed slu framework and show the possibility to reduce human labeling efforts significantly .

preemptive information extraction using unrestricted relation discovery
we are trying to extend the boundary of information extraction ( ie ) systems . existing ie systems require a lot of time and human effort to tune for a new scenario . preemptive information extraction is an attempt to automatically create all feasible ie systems in advance without human intervention . we propose a technique called unrestricted relation discovery that discovers all possible relations from texts and presents them as tables . we present a preliminary system that obtains reasonably good results .

learning script knowledge with web experiments
we describe a novel approach to unsupervised learning of the events that make up a script , along with constraints on their temporal ordering . we collect naturallanguage descriptions of script-specific event sequences from volunteers over the internet . then we compute a graph representation of the scripts temporal structure using a multiple sequence alignment algorithm . the evaluation of our system shows that we outperform two informed baselines .

a note on contextual binary feature grammars
contextual binary feature grammars were recently proposed by ( clark et al , 2008 ) as a learnable representation for richly structured context-free and context sensitive languages . in this paper we examine the representational power of the formalism , its relationship to other standard formalisms and language classes , and its appropriateness for modelling natural language .

detecting problematic turns in human-machine interactions : rule-induction versus memory-based learning approaches
we address the issue of on-line detection of communication problems in spoken dialogue systems . the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances . by applying both ruleinduction and memory-based learning techniques to data obtained with a dutch train time-table information system , the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline . the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead . the results are interesting from a machine learning perspective , since they show that the rule-based method performs significantly better than the memory-based method , because the former is better capable of representing interactions between features .

faster parsing by supertagger adaptation
we propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger , focusing on increasing the speed of the parser rather than its accuracy . the idea is to train the supertagger on large amounts of parser output , so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highestscoring derivation . since the supertagger supplies fewer supertags overall , the parsing speed is increased . we demonstrate the effectiveness of the method using a ccg supertagger and parser , obtaining significant speed increases on newspaper text with no loss in accuracy . we also show that the method can be used to adapt the ccg parser to new domains , obtaining accuracy and speed improvements for wikipedia and biomedical text .

stts 2.0 ? improving the tagset for the part-of-speech-tagging of german spoken data
part-of-speech tagging ( pos-tagging ) of spoken data requires different means of annotation than pos-tagging of written and edited texts . in order to capture the features of german spoken language , a distinct tagset is needed to respond to the kinds of elements which only occur in speech . in order to create such a coherent tagset the most prominent phenomena of spoken language need to be analyzed , especially with respect to how they differ from written language . first evaluations have shown that the most prominent cause ( over 50 % ) of errors in the existing automatized pos-tagging of transcripts of spoken german with the stuttgart tbingen tagset ( stts ) and the treetagger was the inaccurate interpretation of speech particles . one reason for this is that this class of words is virtually absent from the current stts . this paper proposes a recategorization of the stts in the field of speech particles based on distributional factors rather than semantics . the ultimate aim is to create a comprehensive reference corpus of spoken german data for the global research community . it is imperative that all phenomena are reliably recorded in future part-of-speech tag labels .

towards automatic wayang ontology construction using relation extraction from free text hadaiq rolis sanabila
this paper reports on our work to automatically construct and populate an ontology of wayang ( indonesian shadow puppet ) mythology from free text using relation extraction and relation clustering . a reference ontology is used to evaluate the generated ontology . the reference ontology contains concepts and properties within the wayang character domain . we examined the influence of corpus data variations , threshold value variations in the relation clustering process , and the usage of entity pairs or entity pair types during the feature extraction stages . the constructed ontology is examined using three evaluation methods , i.e . cluster purity ( cp ) , instance knowledge ( ik ) , and relation concept ( rc ) . based on the evaluation results , the proposed method generates the best ontology when using a consolidated corpus , the threshold value in relation clustering is 1 , and entity pairs are used during feature extraction .

chunking clinical text containing non-canonical language
free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language . shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment . we present an exploratory study into chunking such text using offthe-shelf language processing tools and pre-trained statistical models . we evaluate chunking accuracy with respect to partof-speech tagging quality , choice of chunk representation , and breadth of context features . our results indicate that narrow context feature windows give the best results , but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy .

of distribution algorithm
in statistical machine translation , an alignment defines a mapping between the words in the source and in the target sentence . alignments are used , on the one hand , to train the statistical models and , on the other , during the decoding process to link the words in the source sentence to the words in the partial hypotheses generated . in both cases , the quality of the alignments is crucial for the success of the translation process . in this paper , we propose an algorithm based on an estimation of distribution algorithm for computing alignments between two sentences in a parallel corpus . this algorithm has been tested on different tasks involving different pair of languages . in the different experiments presented here for the two word-alignment shared tasks proposed in the hlt-naacl 2003 and in the acl 2005 , the edabased algorithm outperforms the best participant systems .

navigation dialog of blind people : recovery from getting lost
navigation of blind people is different from the navigation of sighted people and there is also difference when the blind person is recovering from getting lost . in this paper we focus on qualitative analysis of dialogs between lost blind person and navigator , which is done through the mobile phone . the research was done in two outdoor and one indoor location . the analysis revealed several areas where the dialog model must focus on detailed information , like evaluation of instructions provided by blind person and his/her ability to reliably locate navigation points .

word segmentation as general chunking
during language acquisition , children learn to segment speech into phonemes , syllables , morphemes , and words . we examine word segmentation specifically , and explore the possibility that children might have generalpurpose chunking mechanisms to perform word segmentation . the voting experts ( ve ) and bootstrapped voting experts ( bve ) algorithms serve as computational models of this chunking ability . ve finds chunks by searching for a particular information-theoretic signature : low internal entropy and high boundary entropy . bve adds to ve the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations . we evaluate the general chunking model on phonemicallyencoded corpora of child-directed speech , and show that it is consistent with empirical results in the developmental literature . we argue that it offers a parsimonious alternative to specialpurpose linguistic models .

lexicalization in crosslinguistic probabilistic parsing : the case of french
this paper presents the first probabilistic parsing results for french , using the recently released french treebank . we start with an unlexicalized pcfg as a baseline model , which is enriched to the level of collins model 2 by adding lexicalization and subcategorization . the lexicalized sister-head model and a bigram model are also tested , to deal with the flatness of the french treebank . the bigram model achieves the best performance : 81 % constituency f-score and 84 % dependency accuracy . all lexicalized models outperform the unlexicalized baseline , consistent with probabilistic parsing results for english , but contrary to results for german , where lexicalization has only a limited effect on parsing performance .

[ lvic-limsi ] : using syntactic features and multi-polarity words for sentiment analysis in twitter
this paper presents the contribution of our team at task 2 of semeval 2013 : sentiment analysis in twitter . we submitted a constrained run for each of the two subtasks . in the contextual polarity disambiguation subtask , we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers . in the message polarity classification subtask , we focus on the influence of domain information on sentiment classification .

a joint model for parsing syntactic and semantic dependencies
this paper describes a system that jointly parses syntactic and semantic dependencies , presented at the conll-2008 shared task ( surdeanu et al , 2008 ) . it combines online peceptron learning ( collins , 2002 ) with a parsing model based on the eisner algorithm ( eisner , 1996 ) , extended so as to jointly assign syntactic and semantic labels . overall results are 78.11 global f 1 , 85.84 las , 70.35 semantic f 1 . official results for the shared task ( 63.29 global f 1 ; 71.95 las ; 54.52 semantic f 1 ) were significantly lower due to bugs present at submission time .

how to avoid burning ducks : combining linguistic analysis and corpus statistics for german compound processing
compound splitting is an important problem in many nlp applications which must be solved in order to address issues of data sparsity . previous work has shown that linguistic approaches for german compound splitting produce a correct splitting more often , but corpus-driven approaches work best for phrase-based statistical machine translation from german to english , a worrisome contradiction . we address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance .

multi-modal question-answering : questions without keyboards
this paper describes our work to allow players in a virtual world to pose questions without relying on textual input . our approach is to create enhanced virtual photographs by annotating them with semantic information from the 3d environments scene graph . the player can then use these annotated photos to interact with inhabitants of the world through automatically generated queries that are guaranteed to be relevant , grammatical and unambiguous . while the range of queries is more limited than a text input system would permit , in the gaming environment that we are exploring these limitations are offset by the practical concerns that make text input inappropriate .

entity extraction via ensemble semantics
combining information extraction systems yields significantly higher quality resources than each system in isolation . in this paper , we generalize such a mixing of sources and features in a framework called ensemble semantics . we show very large gains in entity extraction by combining state-of-the-art distributional and patternbased systems with a large set of features from a webcrawl , query logs , and wikipedia . experimental results on a webscale extraction of actors , athletes and musicians show significantly higher mean average precision scores ( 29 % gain ) compared with the current state of the art .

analysis of stopping active learning based on stabilizing predictions
within the natural language processing ( nlp ) community , active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new nlp systems and technologies . this paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions ( sp ) . the analysis has revealed three elements that are central to the success of the sp method : ( 1 ) bounds on cohens kappa agreement between successively trained models impose bounds on differences in f-measure performance of the models ; ( 2 ) since the stop set does not have to be labeled , it can be made large in practice , helping to guarantee that the results transfer to previously unseen streams of examples at test/application time ; and ( 3 ) good ( low variance ) sample estimates of kappa between successive models can be obtained . proofs of relationships between the level of kappa agreement and the difference in performance between consecutive models are presented .

context-dependent term relations for information retrieval
co-occurrence analysis has been used to determine related words or terms in many nlp-related applications such as query expansion in information retrieval ( ir ) . however , related words are usually determined with respect to a single word , without relevant information for its application context . for example , the word programming may be considered to be strongly related to java , and applied inappropriately to expand a query on java travel . to solve this problem , we propose to add another context word in the relation to specify the appropriate context of the relation , leading to term relations of the form ( java , travel ) indonesia . the extracted relations are used for query expansion in ir . our experiments on several trec collections show that this new type of context-dependent relations performs much better than the traditional co-occurrence relations .

syntactic features and word similarity for supervised metonymy
we present a supervised machine learning algorithm for metonymy resolution , which exploits the similarity between examples of conventional metonymy . we show that syntactic head-modifier relations are a high precision feature for metonymy recognition but suffer from data sparseness . we partially overcome this problem by integrating a thesaurus and introducing simpler grammatical features , thereby preserving precision and increasing recall . our algorithm generalises over two levels of contextual similarity . resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation . we also compare automatic and manual methods for syntactic feature extraction .

the best of two worlds : cooperation of statistical and rule-based taggers for czech drahomra johanka spoustova ibm czech republic ,
several hybrid disambiguation methods are described which combine the strength of hand-written disambiguation rules and statistical taggers . three different statistical ( hmm , maximum-entropy and averaged perceptron ) taggers are used in a tagging experiment using prague dependency treebank . the results of the hybrid systems are better than any other method tried for czech tagging so far .

segmentation of multiple objects in multi-camera video streams
in this paper , we propose a new software tool called dales to extract semantic information from multi-view videos based on the analysis of their visual content . our system is fully automatic and is well suited for multi-camera environment . once the multi-view video sequences are loaded into dales , our software performs the detection , counting , and segmentation of the visual objects evolving in the provided video streams . then , these objects of interest are processed in order to be labelled , and the related frames are thus annotated with the corresponding semantic content . moreover , a textual script is automatically generated with the video annotations . dales system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization .

topic segmentation with a structured topic model national ict australia
we present a new hierarchical bayesian model for unsupervised topic segmentation . this new model integrates a point-wise boundary sampling algorithm used in bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents . we develop an mcmc inference algorithm to split/merge segment ( s ) . experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on chois datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets .

estimating strictly piecewise distributions
strictly piecewise ( sp ) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages . like the classes in the chomsky and subregular hierarchies , there are many independently converging characterizations of the sp class ( rogers et al , to appear ) . here we define sp distributions and show that they can be efficiently estimated from positive data .

deterministic parsing using pcfgs
we propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar . several variants are presented . one of these deterministically constructs a parse structure while postponing commitment to labels . we investigate theoretical time complexities and report experiments .

abstract meaning representation for sembanking
we describe abstract meaning representation ( amr ) , a semantic representation language in which we are writing down the meanings of thousands of english sentences . we hope that a sembank of simple , whole-sentence semantic structures will spur new work in statistical natural language understanding and generation , like the penn treebank encouraged work on statistical parsing . this paper gives an overview of amr and tools associated with it .

sentiment analysis in social media texts
this paper presents a method for sentiment analysis specifically designed to work with twitter data ( tweets ) , taking into account their structure , length and specific language . the approach employed makes it easily extendible to other languages and makes it able to process tweets in near real time . the main contributions of this work are : a ) the pre-processing of tweets to normalize the language and generalize the vocabulary employed to express sentiment ; b ) the use minimal linguistic processing , which makes the approach easily portable to other languages ; c ) the inclusion of higher order n-grams to spot modifications in the polarity of the sentiment expressed ; d ) the use of simple heuristics to select features to be employed ; e ) the application of supervised learning using a simple support vector machines linear classifier on a set of realistic data . we show that using the training models generated with the method described we can improve the sentiment classification performance , irrespective of the domain and distribution of the test sets .

bootstrapping lexical choice via multiple-sequence alignment
an important component of any generation system is the mapping dictionary , a lexicon of elementary semantic expressions and corresponding natural language realizations . typically , labor-intensive knowledge-based methods are used to construct the dictionary . we instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment , a technique commonly used in bioinformatics . crucially , our method leverages latent information contained in multiparallel corpora datasets that supply several verbalizations of the corresponding semantics rather than just one . we used our techniques to generate natural language versions of computer-generated mathematical proofs , with good results on both a per-component and overall-output basis . for example , in evaluations involving a dozen human judges , our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system .

automatic extraction of data deposition sentences :
research in the biomedical domain can have a major impact through open sharing of data produced . in this study , we use machine learning for the automatic identification of data deposition sentences in research articles . articles containing deposition sentences are correctly identified with 73 % f-measure . these results show the potential impact of our method for literature curation .

relational inference for wikification
wikification , commonly referred to as disambiguation to wikipedia ( d2w ) , is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding wikipedia pages . previous approaches to d2w focused on the use of local and global statistics over the given text , wikipedia articles and its link structures , to evaluate context compatibility among a list of probable candidates . however , these methods fail ( often , embarrassingly ) , when some level of text understanding is needed to support wikification . in this paper we introduce a novel approach to wikification by incorporating , along with statistical methods , richer relational analysis of the text . we provide an extensible , efficient and modular integer linear programming ( ilp ) formulation of wikification that incorporates the entity-relation inference problem , and show that the ability to identify relations in text helps both candidate generation and ranking wikipedia titles considerably . our results show significant improvements in both wikification and the tac entity linking task .

swedish program for ict in developing regions
the lack of persons trained in computational linguistic methods is a severe obstacle to making the internet and computers accessible to people all over the world in their own languages . the paper discusses the experiences of designing and teaching an introductory course in natural language processing to graduate computer science students at addis ababa university , ethiopia , in order to initiate the education of computational linguists in the horn of africa region .

shourya roy and l venkata subramaniam
call centers handle customer queries from various domains such as computer sales and support , mobile phones , car rental , etc . each such domain generally has a domain model which is essential to handle customer complaints . these models contain common problem categories , typical customer issues and their solutions , greeting styles . currently these models are manually created over time . towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions . we use a state of the art automatic speech recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model . the domain model is comprised of primarily a topic taxonomy where every node is characterized by topic ( s ) , typical questions-answers ( q & as ) , typical actions and call statistics . we show how such a domain model can be used for topic identification of unseen calls . we also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .

wiktionary and nlp : improving synonymy networks clle-erss & irit , cnrs &
wiktionary , a satellite of the wikipedia initiative , can be seen as a potential resource for natural language processing . it requires however to be processed before being used efficiently as an nlp resource . after describing the relevant aspects of wiktionary for our purposes , we focus on its structural properties . then , we describe how we extracted synonymy networks from this resource . we provide an in-depth study of these synonymy networks and compare them to those extracted from traditional resources . finally , we describe two methods for semiautomatically improving this network by adding missing relations : ( i ) using a kind of semantic proximity measure ; ( ii ) using translation relations of wiktionary itself . note : the experiments of this paper are based on wiktionarys dumps downloaded in year 2008. differences may be observed with the current versions available online .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

aspect extraction with automated prior knowledge learning
aspect extraction is an important task in sentiment analysis . topic modeling is a popular method for the task . however , unsupervised topic models often generate incoherent aspects . to address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling . in this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the web . such knowledge can then be used by a topic model to discover more coherent aspects . there are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge . a novel approach is proposed to solve these problems . experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .

fast full parsing by linear-chain conditional random fields yoshimasa tsuruoka junichi tsujii sophia ananiadou
this paper presents a chunking-based discriminative approach to full parsing . we convert the task of full parsing into a series of chunking tasks and apply a conditional random field ( crf ) model to each level of chunking . the probability of an entire parse tree is computed as the product of the probabilities of individual chunking results . the parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm . experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser .

learning sentential paraphrases from bilingual parallel corpora for text-to-text generation
previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora . however , it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases , which are more obviously learnable from monolingual parallel corpora . we extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations , including passivization , dative shift , and topicalization . we discuss how our model can be adapted to many text generation tasks by augmenting its feature set , development data , and parameter estimation routine . we illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems .

incremental text structuring with online hierarchical ranking
many emerging applications require documents to be repeatedly updated . such documents include newsfeeds , webpages , and shared community resources such as wikipedia . in this paper we address the task of inserting new information into existing texts . in particular , we wish to determine the best location in a text for a given piece of new information . for this process to succeed , the insertion algorithm should be informed by the existing document structure . lengthy real-world texts are often hierarchically organized into chapters , sections , and paragraphs . we present an online ranking model which exploits this hierarchical structure representationally in its features and algorithmically in its learning procedure . when tested on a corpus of wikipedia articles , our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods .

