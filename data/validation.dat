automatic evaluation of topic coherence
this paper introduces the novel task of topic coherence evaluation , whereby a set of words , as generated by a topic model , is rated for coherence or interpretability . we apply a range of topic scoring models to the evaluation task , drawing on wordnet , wikipedia and the google search engine , and existing research on lexical similarity/relatedness . in comparison with human scores for a set of learned topics over two distinct datasets , we show a simple co-occurrence measure based on pointwise mutual information over wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation , and that other wikipedia-based lexical relatedness methods also achieve strong results . google produces strong , if less consistent , results , while our results over wordnet are patchy at best .

whats in a domain multi-domain learning for multi-attribute data
multi-domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains . however , real-world datasets often have multiple metadata attributes that can divide the data into domains . it is not always apparent which single attribute will lead to the best domains , and more than one attribute might impact classification . we propose extensions to two multi-domain learning techniques for our multi-attribute setting , enabling them to simultaneously learn from several metadata attributes . experimentally , they outperform the multi-domain learning baseline , even when it selects the single best attribute .

automatic evaluation of students answers using syntactically enhanced lsa
latent semantic analysis ( lsa ) has been used in several intelligent tutoring systems ( itss ) for assessing students learning by evaluating their answers to questions in the tutoring domain . it is based on word-document cooccurrence statistics in the training corpus and a dimensionality reduction technique . however , it doesnt consider the word-order or syntactic information , which can improve the knowledge representation and therefore lead to better performance of an its . we present here an approach called syntactically enhanced lsa ( selsa ) which generalizes lsa by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word , as a unit of knowledge representation . the experimental results on autotutor task to evaluate students answers to basic computer science questions by selsa and its comparison with lsa are presented in terms of several cognitive measures . selsa is able to correctly evaluate a few more answers than lsa but is having less correlation with human evaluators than lsa has . it also provides better discrimination of syntactic-semantic knowledge representation than lsa .

combining linguistic and machine learning techniques for email
this paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages . we describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction . three main conclusions can be drawn from this study : ( i ) the modifiers of a noun phrase can be semantically as important as the head , for the task of gisting , ( ii ) linguistic filtering improves the performance of machine learning algorithms , ( iii ) a combination of classifiers improves accuracy .

a chain-starting classifier of definite nps in spanish
given the great amount of definite noun phrases that introduce an entity into the text for the first time , this paper presents a set of linguistic features that can be used to detect this type of definites in spanish . the efficiency of the different features is tested by building a rule-based and a learning-based chain-starting classifier . results suggest that the classifier , which achieves high precision at the cost of recall , can be incorporated as either a filter or an additional feature within a coreference resolution system to boost its performance .

controlled ascent : imbuing statistical mt with linguistic knowledge
we explore the intersection of rule-based and statistical approaches in machine translation , with a particular focus on past and current work here at microsoft research . until about ten years ago , the only machine translation systems worth using were rule-based and linguistically-informed . along came statistical approaches , which use large corpora to directly guide translations toward expressions people would actually say . rather than making local decisions when writing and conditioning rules , goodness of translation was modeled numerically and free parameters were selected to optimize that goodness . this led to huge improvements in translation quality as more and more data was consumed . by necessity , the pendulum is swinging towards the inclusion of linguistic features in mt systems . we describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems , showing what is currently working well , and what isnt . we also look at trade-offs in using linguistic knowledge ( rules ) in pre- or post-processing by language pair , with a particular eye on the return on investment as training data increases in size .

tacit contracts for wheelchairs
in this paper , we propose a novel approach to infer dialogue acts using the notion of tacit contracts . we describe the interpersonal linguistic features that our analysis grammar can identify in uttered texts and present an inference procedure that strictly separates the semantic and pragmatic steps of utterance understanding , thereby meeting a higher degree of modularity , a prerequisite for extending robot functionality .

a study of using syntactic and semantic structures
this paper presents an empirical study on using syntactic and semantic information for concept segmentation and labeling ( csl ) , a well-known component in spoken language understanding . our approach is based on reranking n -best outputs from a state-of-the-art csl parser . we perform extensive experimentation by comparing different tree-based kernels with a variety of representations of the available linguistic information , including semantic concepts , words , pos tags , shallow and full syntax , and discourse trees . the results show that the structured representation with the semantic concepts yields significant improvement over the base csl parser , much larger compared to learning with an explicit feature vector representation . we also show that shallow syntax helps improve the results and that discourse relations can be partially beneficial .

named entities translation based on comparable corpora
in this paper we present a system for translating named entities from basque to spanish based on comparable corpora . for that purpose we have tried two approaches : one based on basque linguistic features , and a language-independent tool . for both tools we have used basquespanish comparable corpora , a bilingual dictionary and the web as resources .

end-to-end coreference resolution via hypergraph partitioning
we describe a novel approach to coreference resolution which implements a global decision via hypergraph partitioning . in constrast to almost all previous approaches , we do not rely on separate classification and clustering steps , but perform coreference resolution globally in one step . our hypergraph-based global model implemented within an endto-end coreference resolution system outperforms two strong baselines ( soon et al , 2001 ; bengtson & roth , 2008 ) using system mentions only .

real-time stochastic language generation for dialogue systems
this paper describes acorn , a sentence planner and surface realizer for dialogue systems . improvements to previous stochastic word-forest based approaches are described , countering recent criticism of this class of algorithms for their slow speed . an evaluation of the approach with semantic input shows runtimes of a fraction of a second and presents results that suggest it is also portable across domains .

automatic editing in a back-end speech-to-text system maximilian bisani paul vozila olivier divay jeff adams one wayside road
written documents created through dictation differ significantly from a true verbatim transcript of the recorded speech . this poses an obstacle in automatic dictation systems as speech recognition output needs to undergo a fair amount of editing in order to turn it into a document that complies with the customary standards . we present an approach that attempts to perform this edit from recognized words to final document automatically by learning the appropriate transformations from example documents . this addresses a number of problems in an integrated way , which have so far been studied independently , in particular automatic punctuation , text segmentation , error correction and disfluency repair . we study two different learning methods , one based on rule induction and one based on a probabilistic sequence model . quantitative evaluation shows that the probabilistic method performs more accurately .

imposing hierarchical browsing structures onto spoken documents
this paper studies the problem of imposing a known hierarchical structure onto an unstructured spoken document , aiming to help browse such archives . we formulate our solutions within a dynamic-programming-based alignment framework and use minimum errorrate training to combine a number of global and hierarchical constraints . this pragmatic approach is computationally efficient . results show that it outperforms a baseline that ignores the hierarchical and global features and the improvement is consistent on transcripts with different wers . directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results .

semi-supervised relation extraction with large-scale word clustering
we present a simple semi-supervised relation extraction system with large-scale word clustering . we focus on systematically exploring the effectiveness of different cluster-based features . we also propose several statistical methods for selecting clusters at an appropriate level of granularity . when training on different sizes of data , our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system .

joint modeling of news readers and comment writers emotions
emotion classification can be generally done from both the writers and readers perspectives . in this study , we find that two foundational tasks in emotion classification , i.e. , readers emotion classification on the news and writers emotion classification on the comments , are strongly related to each other in terms of coarse-grained emotion categories , i.e. , negative and positive . on the basis , we propose a respective way to jointly model these two tasks . in particular , a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks . experimental evaluation shows the effectiveness of our joint modeling approach . *

simple is best : experiments with different document segmentation strategies for passage retrieval
passage retrieval is used in qa to filter large document collections in order to find text units relevant for answering given questions . in our qa system we apply standard ir techniques and index-time passaging in the retrieval component . in this paper we investigate several ways of dividing documents into passages . in particular we look at semantically motivated approaches ( using coreference chains and discourse clues ) compared with simple window-based techniques . we evaluate retrieval performance and the overall qa performance in order to study the impact of the different segmentation approaches . from our experiments we can conclude that the simple techniques using fixedsized windows clearly outperform the semantically motivated approaches , which indicates that uniformity in size seems to be more important than semantic coherence in our setup .

phrase based decoding using a discriminative model
in this paper , we present an approach to statistical machine translation that combines the power of a discriminative model ( for training a model for machine translation ) , and the standard beam-search based decoding technique ( for the translation of an input sentence ) . a discriminative approach for learning lexical selection and reordering utilizes a large set of feature functions ( thereby providing the power to incorporate greater contextual and linguistic information ) , which leads to an effective training of these models . this model is then used by the standard state-of-art moses decoder ( koehn et al , 2007 ) for the translation of an input sentence . we conducted our experiments on spanish-english language pair . we used maximum entropy model in our experiments . we show that the performance of our approach ( using simple lexical features ) is comparable to that of the state-of-art statistical mt system ( koehn et al , 2007 ) . when additional syntactic features ( pos tags in this paper ) are used , there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model .

user simulations for context-sensitive speech recognition in spoken
we use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition ( asr ) hypotheses to a spoken dialogue system ( sds ) . our novel approach is to use a simple statistical user simulation ( us ) for this task , which measures the likelihood that the user would say each hypothesis in the current context . such us models are now common in machine learning approaches to sds , are trained on real dialogue data , and are related to theories of alignment in psycholinguistics . we use a us to predict the users next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances . the method achieved a significant relative reduction of word error rate ( wer ) of 5 % ( this is 44 % of the possible wer improvement on this data ) , and 62 % of the possible semantic improvement ( dialogue move accuracy ) , compared to the baseline policy of selecting the topmost asr hypothesis . the majority of the improvement is attributable to the user simulation feature , as shown by information gain analysis .

middleware for incremental processing in conversational agents germany germany sweden
we describe work done at three sites on designing conversational agents capable of incremental processing . we focus on the middleware layer in these systems , which takes care of passing around and maintaining incremental information between the modules of such agents . all implementations are based on the abstract model of incremental dialogue processing proposed by schlangen and skantze ( 2009 ) , and the paper shows what different instantiations of the model can look like given specific requirements and application areas .

measuring the public accountability of new modes of governance
we present an encompassing research endeavour on the public accountability of new modes of governance in europe . the aim of this project is to measure the salience , tonality and framing of regulatory bodies and public interest organisations in newspaper coverage and parliamentary debates over the last 15 years . in order to achieve this , we use language technology which is still underused in political science text analyses . institutionally , the project has emerged from a collaboration between a computational linguistics and a political science department .

not all seeds are equal : measuring the quality of text mining seeds
open-class semantic lexicon induction is of great interest for current knowledge harvesting algorithms . we propose a general framework that uses patterns in bootstrapping fashion to learn open-class semantic lexicons for different kinds of relations . these patterns require seeds . to estimate the goodness ( the potential yield ) of new seeds , we introduce a regression model that considers the connectivity behavior of the seed during bootstrapping . the generalized regression model is evaluated on six different kinds of relations with over 10000 different seeds for english and spanish patterns . our approach reaches robust performance of 90 % correlation coefficient with 15 % error rate for any of the patterns when predicting the goodness of seeds .

named entity recognition with character-level models and joseph smarr and huy nguyen symbolic systems program
we discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation . the first model is a character-level hmm with minimal context information , and the second model is a maximum-entropy conditional markov model with substantially richer context features . our best model achieves an overall f of 86.07 % on the english test data ( 92.31 % on the development data ) . this number represents a 25 % error reduction over the same model without word-internal ( substring ) features .

the effect of sensor errors in situated human-computer dialogue brian mac namee
errors in perception are a problem for computer systems that use sensors to perceive the environment . if a computer system is engaged in dialogue with a human user , these problems in perception lead to problems in the dialogue . we present two experiments , one in which participants interact through dialogue with a robot with perfect perception to fulfil a simple task , and a second one in which the robot is affected by sensor errors and compare the resulting dialogues to determine whether the sensor problems have an impact on dialogue success .

word sense induction :
in this paper a novel solution to automatic and unsupervised word sense induction ( wsi ) is introduced . it represents an instantiation of the one sense per collocation observation ( gale et al , 1992 ) . like most existing approaches it utilizes clustering of word co-occurrences . this approach differs from other approaches to wsi in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs . the combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results . additionally , a novel and likewise automatic and unsupervised evaluation method inspired by schutzes ( 1992 ) idea of evaluation of word sense disambiguation algorithms is employed . offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the wsi algorithm .

phrasal : a toolkit for new directions in statistical machine translation
we present a new version of phrasal , an open-source toolkit for statistical phrasebased machine translation . this revision includes features that support emerging research trends such as ( a ) tuning with large feature sets , ( b ) tuning on large datasets like the bitext , and ( c ) web-based interactive machine translation . a direct comparison with moses shows favorable results in terms of decoding speed and tuning time .

seedling : building and using a seed corpus for the human language project
a broad-coverage corpus such as the human language project envisioned by abney and bird ( 2010 ) would be a powerful resource for the study of endangered languages . existing corpora are limited in the range of languages covered , in standardisation , or in machine-readability . in this paper we present seedling , a seed corpus for the human language project . we first survey existing efforts to compile cross-linguistic resources , then describe our own approach . to build the foundation text for a universal corpus , we crawl and clean texts from several web sources that contain data from a large number of languages , and convert them into a standardised form consistent with the guidelines of abney and bird ( 2011 ) . the resulting corpus is more easily-accessible and machine-readable than any of the underlying data sources , and , with data from 1451 languages covering 105 language families , represents a significant base corpus for researchers to draw on and add to in the future . to demonstrate the utility of seedling for cross-lingual computational research , we use our data in the test application of detecting similar languages .

chasing the ghost : recovering empty categories in the chinese treebank
empty categories represent an important source of information in syntactic parses annotated in the generative linguistic tradition , but empty category recovery has only started to receive serious attention until very recently , after substantial progress in statistical parsing . this paper describes a unified framework in recovering empty categories in the chinese treebank . our results show that given skeletal gold standard parses , the empty categories can be detected with very high accuracy . we report very promising results for empty category recovery for automatic parses as well .

in-domain relation discovery with meta-constraints via posterior regularization
we present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain . our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain . these constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure , including lexicography , syntax , and document-level context . we capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference . across two domains our approach successfully recovers hidden relation structure , comparable to or outperforming previous state-of-the-art approaches . furthermore , we find that a small set of constraints is applicable across the domains , and that using domain-specific constraints can further improve performance .

reranking with linguistic and semantic features for arabic optical character recognition pradeep dasigi mona diab
optical character recognition ( ocr ) systems for arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency . in this paper we incorporate linguistically and semantically motivated features to an existing ocr system . to do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques . we achieve 10.1 % and 11.4 % reduction in recognition word error rate ( wer ) relative to a standard baseline system on typewritten and handwritten arabic respectively .

within an end-user application dipartimento di informatica
in this paper we describe our semeval-2013 task on word sense induction and disambiguation within an end-user application , namely web search result clustering and diversification . given a target query , induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query . the task enables the end-to-end evaluation and comparison of systems .

towards automatic annotation of communicative gesturing
we report on-going work on automatic annotation of head and hand gestures in videos of conversational interaction . the anvil annotation tool was extended by two plugins for automatic face and hand tracking . the results of automatic annotation are compared with the human annotations on the same data .

identifying idiomatic expressions using automatic word-alignment begona villada moiron and jorg tiedemann
for nlp applications that require some sort of semantic interpretation it would be helpful to know what expressions exhibit an idiomatic meaning and what expressions exhibit a literal meaning . we investigate whether automatic word-alignment in existing parallel corpora facilitates the classification of candidate expressions along a continuum ranging from literal and transparent expressions to idiomatic and opaque expressions . our method relies on two criteria : ( i ) meaning predictability that is measured as semantic entropy and ( ii ) , the overlap between the meaning of an expression and the meaning of its component words . we approximate the mentioned overlap as the proportion of default alignments . we obtain a significant improvement over the baseline with both measures .

emotional visualization of news stories
the newsviz system aims to enhance news reading experiences by integrating 30 seconds long flash-animations into news article web pages depicting their content and emotional aspects . newsviz interprets football match news texts automatically and creates abstract 2d visualizations . the user interface enables animators to further refine the animations . here , we focus on the emotion extraction component of newsviz which facilitates subtle background visualization . newsviz detects moods from news reports . the original text is part-of-speech tagged and adjectives and/or nouns , the word types conveying most emotional meaning , are filtered out and labeled with an emotion and intensity value . subsequently reoccurring emotions are joined into longer lasting moods and matched with appropriate animation presets . different linguistic analysis methods were tested on newsviz : word-by-word , sentence-based and minimum threshold summarization , to find a minimum number of occurrences of an emotion in forming a valid mood . newsviz proved to be viable for the fixed domain of football news , grasping the overall moods and some more detailed emotions precisely . newsviz offers an efficient technique to cater for the production of a large number of daily updated news stories .

enhancing authorship attribution by utilizing syntax tree profiles
the aim of modern authorship attribution approaches is to analyze known authors and to assign authorships to previously unseen and unlabeled text documents based on various features . in this paper we present a novel feature to enhance current attribution methods by analyzing the grammar of authors . to extract the feature , a syntax tree of each sentence of a document is calculated , which is then split up into length-independent patterns using pq-grams . the mostly used pq-grams are then used to compose sample profiles of authors that are compared with the profile of the unlabeled document by utilizing various distance metrics and similarity scores . an evaluation using three different and independent data sets reveals promising results and indicate that the grammar of authors is a significant feature to enhance modern authorship attribution methods .

mildly context-sensitive dependency languages
dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity . in previous work , several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense ; the best-known but also most restrictive of these is projectivity . most constraints are formulated on fully specified structures , which makes them hard to integrate into models where structures are composed from lexical information . in this paper , we show how two empirically relevant relaxations of projectivity can be lexicalized , and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages .

kul-eval : a combinatory categorial grammar approach for improving semantic parsing of robot commands using spatial context
when executing commands , a robot has a certain level of contextual knowledge about the environment in which it operates . taking this knowledge into account can be beneficial to disambiguate commands with multiple interpretations . we present an approach that uses combinatory categorial grammars for improving the semantic parsing of robot commands that takes into account the spatial context of the robot . the results indicate a clear improvement over non-contextual semantic parsing . this work was done in the context of the semeval-2014 task on supervised semantic parsing of spatial robot commands .

learning mixed initiative dialog strategies by using reinforcement learning on both conversants
this paper describes an application of reinforcement learning to determine a dialog policy for a complex collaborative task where policies for both the system and a proxy for a user of the system are learned simultaneously . with this approach a useful dialog policy is learned without the drawbacks of other approaches that require significant human interaction . the specific task that the agents were trained on was chosen for its complexity and requirement that both conversants bring task knowledge to the interaction , thus ensuring its collaborative nature . the results of our experiment show that you can use reinforcement learning to create an effective dialog policy , which employs a mixed initiative strategy , without the drawbacks of large amounts of data or significant human input .

data-driven classification of linguistic styles in spoken dialogues
language users have individual linguistic styles . a spoken dialogue system may benefit from adapting to the linguistic style of a user in input analysis and output generation . to investigate the possibility to automatically classify speakers according to their linguistic style three corpora of spoken dialogues were analyzed . several numerical parameters were computed for every speaker . these parameters were reduced to linguistically interpretable components by means of a principal component analysis . classes were established from these components by cluster analysis . unseen input was classified by trained neural networks with varying error rates depending on corpus type . a first investigation in using special language models for speaker classes was carried out .

route communication in dialogue : a matter of principles
the present study uses the dialogue paradigm to explore route communication . it revolves around the analysis of a corpus of route instructions produced in real-time interaction with the follower . it explores the variation in forming route instructions and the factors that contribute in it . the results show that visual co-presence influences the performance , conversation patterns and configuration of instructions . most importantly , the results suggest an analogy between the choices of instructiongivers and the communicative actions of their partners .

multilingual video and audio news alerting
this paper describes a fully-automated realtime broadcast news video and audio processing system . the system combines speech recognition , machine translation , and crosslingual information retrieval components to enable real-time alerting from live english and arabic news sources .

information extraction for social media
the rapid growth in it in the last two decades has led to a growth in the amount of information available online . a new style for sharing information is social media . social media is a continuously instantly updated source of information . in this position paper , we propose a framework for information extraction ( ie ) from unstructured user generated contents on social media . the framework proposes solutions to overcome the ie challenges in this domain such as the short context , the noisy sparse contents and the uncertain contents . to overcome the challenges facing ie from social media , state-of-the-art approaches need to be adapted to suit the nature of social media posts . the key components and aspects of our proposed framework are noisy text filtering , named entity extraction , named entity disambiguation , feedback loops , and uncertainty handling .

language specific issue and feature exploration in chinese event extraction
in this paper , we present a chinese event extraction system . we point out a language specific issue in chinese trigger labeling , and then commit to discussing the contributions of lexical , syntactic and semantic features applied in trigger labeling and argument labeling . as a result , we achieved competitive performance , specifically , f-measure of 59.9 in trigger labeling and f-measure of 43.8 in argument labeling .

discriminative classifiers for deterministic dependency parsing
deterministic parsing guided by treebankinduced classifiers has emerged as a simple and efficient alternative to more complex models for data-driven parsing . we present a systematic comparison of memory-based learning ( mbl ) and support vector machines ( svm ) for inducing classifiers for deterministic dependency parsing , using data from chinese , english and swedish , together with a variety of different feature models . the comparison shows that svm gives higher accuracy for richly articulated feature models across all languages , albeit with considerably longer training times . the results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models .

joint hebrew segmentation and parsing using a pcfg-la lattice parser
we experiment with extending a lattice parsing methodology for parsing hebrew ( goldberg and tsarfaty , 2008 ; golderg et al , 2009 ) to make use of a stronger syntactic model : the pcfg-la berkeley parser . we show that the methodology is very effective : using a small training set of about 5500 trees , we construct a parser which parses and segments unsegmented hebrew text with an f-score of almost 80 % , an error reduction of over 20 % over the best previous result for this task . this result indicates that lattice parsing with the berkeley parser is an effective methodology for parsing over uncertain inputs .

analysis and processing of lecture audio data : preliminary investigations
in this paper we report on our recent efforts to collect a corpus of spoken lecture material that will enable research directed towards fast , accurate , and easy access to lecture content . thus far , we have collected a corpus of 270 hours of speech from a variety of undergraduate courses and seminars . we report on an initial analysis of the spontaneous speech phenomena present in these data and the vocabulary usage patterns across three courses . finally , we examine language model perplexities trained from written and spoken materials , and describe an initial recognition experiment on one course .

multiple aspect ranking using the good grief algorithm
we address the problem of analyzing multiple related opinions in a text . for instance , in a restaurant review such opinions may include food , ambience and service . we formulate this task as a multiple aspect ranking problem , where the goal is to produce a set of numerical scores , one for each aspect . we present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks . this algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions , such as agreement and contrast . we prove that our agreementbased joint model is more expressive than individual ranking models . our empirical results further confirm the strength of the model : the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model .

improved tree-to-string transducer for machine translation
we propose three enhancements to the treeto-string ( tts ) transducer for machine translation : first-level expansion-based normalization for tts templates , a syntactic alignment framework integrating the insertion of unaligned target words , and subtree-based ngram model addressing the tree decomposition probability . empirical results show that these methods improve the performance of a tts transducer based on the standard bleu4 metric . we also experiment with semantic labels in a tts transducer , and achieve improvement over our baseline system .

on cross-language information retrieval
this paper presents a first attempt of an application-driven evaluation exercise of wsd . we used a clir testbed from the cross lingual evaluation forum . the expansion , indexing and retrieval strategies where fixed by the organizers . the participants had to return both the topics and documents tagged with wordnet 1.6 word senses . the organization provided training data in the form of a pre-processed semcor which could be readily used by participants . the task had two participants , and the organizer also provide an in-house wsd system for comparison .

building and rening rhetorical-semantic relation models sasha blair-goldensohn and
we report results of experiments which build and refine models of rhetoricalsemantic relations such as cause and contrast . we adopt the approach of marcu and echihabi ( 2002 ) , using a small set of patterns to build relation models , and extend their work by refining the training and classification process using parameter optimization , topic segmentation and syntactic parsing . using human-annotated and automatically-extracted test sets , we find that each of these techniques results in improved relation classification accuracy .

posslt : a korean to english spoken language translation system
the posslt 1 is a korean to english spoken language translation ( slt ) system . like most other slt systems , automatic speech recognition ( asr ) , machine translation ( mt ) , and text-to-speech ( tts ) are coupled in a cascading manner in our posslt . however , several novel techniques are applied to improve overall translation quality and speed . models used in posslt are trained on a travel domain conversational corpus .

the impact of dimensionality on natural language route directions in unconstrained dialogue
in this paper we examine the influence of dimensionality on natural language route directions in dialogue . specifically , we show that giving route instructions in a quasi-3d environment leads to experiential descriptive accounts , as manifested by a higher proportion of location descriptions , lack of chunking , use of 1st person singular personal pronouns , and more frequent use of temporal and spatial deictic terms . 2d scenarios lead to informative instructions , as manifested by a frequent use of motion expressions , chunking of route elements , and use of mainly 2nd person singular personal pronouns .

joint unsupervised coreference resolution with markov logic
machine learning approaches to coreference resolution are typically supervised , and require expensive labeled data . some unsupervised approaches have been proposed ( e.g. , haghighi and klein ( 2007 ) ) , but they are less accurate . in this paper , we present the first unsupervised approach that is competitive with supervised ones . this is made possible by performing joint inference across mentions , in contrast to the pairwise classification typically used in supervised methods , and by usingmarkov logic as a representation language , which enables us to easily express relations like apposition and predicate nominals . on muc and ace datasets , our model outperforms haghigi and kleins one using only a fraction of the training data , and often matches or exceeds the accuracy of state-of-the-art supervised models .

subjectivity and sentiment analysis of modern standard arabic
although subjectivity and sentiment analysis ( ssa ) has been witnessing a flurry of novel research , there are few attempts to build ssa systems for morphologically-rich languages ( mrl ) . in the current study , we report efforts to partially fill this gap . we present a newly developed manually annotated corpus of modern standard arabic ( msa ) together with a new polarity lexicon.the corpus is a collection of newswire documents annotated on the sentence level . we also describe an automatic ssa tagging system that exploits the annotated data . we investigate the impact of different levels of preprocessing settings on the ssa classification task . we show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance .

function-based question classification for general qa
in contrast with the booming increase of internet data , state-of-art qa ( question answering ) systems , otherwise , concerned data from specific domains or resources such as search engine snippets , online forums and wikipedia in a somewhat isolated way . users may welcome a more general qa system for its capability to answer questions of various sources , integrated from existed specialized sub-qa engines . in this framework , question classification is the primary task . however , the current paradigms of question classification were focused on some specified type of questions , i.e . factoid questions , which are inappropriate for the general qa . in this paper , we propose a new question classification paradigm , which includes a question taxonomy suitable to the general qa and a question classifier based on mln ( markov logic network ) , where rule-based methods and statistical methods are unified into a single framework in a fuzzy discriminative learning approach . experiments show that our method outperforms traditional question classification approaches .

type-inheritance combinatory categorial grammar
in this paper i outline type-inheritance combinatory categorial grammar ( tccg ) , an implemented feature structure based ccg fragment of english . tccg combines the fully lexical nature of ccg with the type-inheritance hierarchies and complex feature structures of headdriven phrase structure grammars ( hpsg ) . the result is a ccg/hpsg hybrid that combines linguistic generalizations previously only statable in one theory or the other , even extending the set of statable generalizations to those not easily captured by either theory .

mitigation of data sparsity in classifier-based translation
the concept classifier has been used as a translation unit in speech-to-speech translation systems . however , the sparsity of the training data is the bottle neck of its effectiveness . here , a new method based on using a statistical machine translation system has been introduced to mitigate the effects of data sparsity for training classifiers . also , the effects of the background model which is necessary to compensate the above problem , is investigated . experimental evaluation in the context of crosslingual doctor-patient interaction application show the superiority of the proposed method .

the perils and rewards of developing restricted domain applications
over the last three decades , the development of restricted domain applications has been an ongoing theme in computational linguistic research . speech recognition , machine translation , summarization , and question answering researchers have all built at one time or another restricted domain systems . in retrospect , it is long due to examine both the successes and failures of these previous attempts . in this talk , i will examine the circumstances in which the development of restricted domain applications has led to significant advances in the state of the art and the circumstances in which restricted domain research has had little impact on our field . i will use the lessons learned from previous attempts to building restricted domain natural language processing applications in order to examine the potential impact of current research in restricted domain question answering .

semtag : a platform for specifying tree adjoining grammars and performing tag-based semantic construction
in this paper , we introduce semtag , a free and open software architecture for the development of tree adjoining grammars integrating a compositional semantics . semtag differs from xtag in two main ways . first , it provides an expressive grammar formalism and compiler for factorising and specifying tags . second , it supports semantic construction .

making uima truly interoperable with sparql
unstructured information management architecture ( uima ) has been gaining popularity in annotating text corpora . the architecture defines common data structures and interfaces to support interoperability of individual processing components working together in a uima application . the components exchange data by sharing common type systemsschemata of data type structureswhich extend a generic , top-level type system built into uima . this flexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems ; however , components coming from different repositories , and thus not sharing type systems , remain incompatible . commonly , this problem has been solved programmatically by implementing uima components that perform the alignment of two type systems , an arduous task that is impractical with a growing number of type systems . we alleviate this problem by introducing a conversion mechanism based on sparql , a query language for the data retrieval and manipulation of rdf graphs . we provide a uima component that serialises data coming from a source component into rdf , executes a user-defined , typeconversion query , and deserialises the updated graph into a target component . the proposed solution encourages ad hoc conversions , enables the usage of heterogeneous components , and facilitates highly customised uima applications .

for spelling correction with web-scale n-gram models
we propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction . the syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems . experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4 % over the current state-of-the-art . the word co-occurrence information shows potential but only improves overall accuracy slightly .

a latent variable model for geographic lexical variation
the rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation . in this paper , we present a multi-level generative model that reasons jointly about latent topics and geographical regions . high-level topics such as sports or entertainment are rendered differently in each geographic region , revealing topic-specific regional distinctions . applied to a new dataset of geotagged microblogs , our model recovers coherent topics and their regional variants , while identifying geographic areas of linguistic consistency . the model also enables prediction of an authors geographic location from raw text , outperforming both text regression and supervised topic models .

multiple word alignment with profile hidden markov models
profile hidden markov models ( profile hmms ) are specific types of hidden markov models used in biological sequence analysis . we propose the use of profile hmms for word-related tasks . we test their applicability to the tasks of multiple cognate alignment and cognate set matching , and find that they work well in general for both tasks . on the latter task , the profile hmm method outperforms average and minimum edit distance . given the success for these two tasks , we further discuss the potential applications of profile hmms to any task where consideration of a set of words is necessary .

language independent transliteration mining system using finite state
we propose a named entities transliteration mining system using finite state automata ( fsa ) . we compare the proposed approach with a baseline system that utilizes the editex technique to measure the length-normalized phonetic based edit distance between the two words . we submitted three standard runs in news2010 shared task and ranked first for english to arabic ( wm-enar ) and obtained an fmeasure of 0.915 , 0.903 , and 0.874 respectively .

species disambiguation for biomedical term identification
an important task in information extraction ( ie ) from biomedical articles is term identification ( ti ) , which concerns linking entity mentions ( e.g. , terms denoting proteins ) in text to unambiguous identifiers in standard databases ( e.g. , refseq ) . previous work on ti has focused on species-specific documents . however , biomedical documents , especially full-length articles , often talk about entities across a number of species , in which case resolving species ambiguity becomes an indispensable part of ti . this paper describes our rule-based and machine-learning based approaches to species disambiguation and demonstrates that performance of ti can be improved by over 20 % if the correct species are known . we also show that using the species predicted by the automatic species taggers can improve ti by a large margin .

interpreting anaphoric shell nouns using antecedents of cataphoric shell nouns as training data
interpreting anaphoric shell nouns ( asns ) such as this issue and this fact is essential to understanding virtually any substantial natural language text . one obstacle in developing methods for automatically interpreting asns is the lack of annotated data . we tackle this challenge by exploiting cataphoric shell nouns ( csns ) whose construction makes them particularly easy to interpret ( e.g. , the fact that x ) . we propose an approach that uses automatically extracted antecedents of csns as training data to interpret asns . we achieve precisions in the range of 0.35 ( baseline = 0.21 ) to 0.72 ( baseline = 0.44 ) , depending upon the shell noun .

user-directed sentiment analysis : visualizing the affective content of
recent advances in text analysis have led to finer-grained semantic analysis , including automatic sentiment analysis the task of measuring documents , or chunks of text , based on emotive categories , such as positive or negative . however , considerably less progress has been made on efficient ways of exploring these measurements . this paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection .

universal morphological analysis using structured nearest neighbor
in this paper , we consider the problem of unsupervised morphological analysis from a new angle . past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand . we propose instead to treat morphological analysis as a structured prediction problem , where languages with labeled data serve as training examples for unlabeled languages , without the assumption of parallel data . we define a universal morphological feature space in which every language and its morphological analysis reside . we develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language . we apply our model to eight inflecting languages , and induce nominal morphology with substantially higher accuracy than a traditional , mdlbased approach . our analysis indicates that accuracy continues to improve substantially as the number of training languages increases .

joint versus independent phonological feature models within crf phone recognition
we compare the effect of joint modeling of phonological features to independent feature detectors in a conditional random fields framework . joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes . we find that joint modeling provides superior performance to the independent models on the timit phone recognition task . we explore the effects of varying relationships between phonological features , and suggest that in an asr system , phonological features should be handled as correlated , rather than independent .

considerations on automatic mapping large-scale heterogeneous language resources : sejong semantic classes and korlex woo chul park and
this paper presents an automatic mapping method among large-scale heterogeneous language resources : sejong semantic classes ( sjsc ) and korlex . korlex is a large-scale korean wordnet , but it lacks specific syntactic & semantic information . sejong electronic dictionary ( sjd ) , of which semantic segmentation depends on sjsc , has much lower lexical coverage than korlex , but shows refined syntactic & semantic information . the goal of this study is to build a rich language resource for improving korean semantico-syntactic parsing technology . therefore , we consider integration of them and propose automatic mapping method with three approaches : 1 ) information of monosemy/polysemy of word senses ( impw ) , 2 ) instances between nouns of sjd and word senses of korlex ( inw ) , and 3 ) semantically related words between nouns of sjd and synsets of korlex ( srns ) . we obtain good performance using combined three approaches : recall 0.837 , precision 0.717 , and f1 0.773 .

title generation with quasi-synchronous grammar
the task of selecting information and rendering it appropriately appears in multiple contexts in summarization . in this paper we present a model that simultaneously optimizes selection and rendering preferences . the model operates over a phrase-based representation of the source document which we obtain by merging pcfg parse trees and dependency graphs . selection preferences for individual phrases are learned discriminatively , while a quasi-synchronous grammar ( smith and eisner , 2006 ) captures rendering preferences such as paraphrases and compressions . based on an integer linear programming formulation , the model learns to generate summaries that satisfy both types of preferences , while ensuring that length , topic coverage and grammar constraints are met . experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications .

recognising the predicateargument structure of tagalog
this paper describes research on parsing tagalog text for predicateargument structure ( pas ) . we first outline the linguistic phenomenon and corpus annotation process , then detail a series of pas parsing experiments .

instance selection for machine translation using feature decay
we present an empirical study of instance selection techniques for machine translation . in an active learning setting , instance selection minimizes the human effort by identifying the most informative sentences for translation . in a transductive learning setting , selection of training instances relevant to the test set improves the final translation quality . after reviewing the state of the art in the field , we generalize the main ideas in a class of instance selection algorithms that use feature decay . feature decay algorithms increase diversity of the training set by devaluing features that are already included . we show that the feature decay rate has a very strong effect on the final translation quality whereas the initial feature values , inclusion of higher order features , or sentence length normalizations do not . we evaluate the best instance selection methods using a standard moses baseline using the whole 1.6 million sentence english-german section of the europarl corpus . we show that selecting the best 3000 training sentences for a specific test sentence is sufficient to obtain a score within 1 bleu of the baseline , using 5 % of the training data is sufficient to exceed the baseline , and a 2 bleu improvement over the baseline is possible by optimally selected subset of the training data . in out-of-domain translation , we are able to reduce the training set size to about 7 % and achieve a similar performance with the baseline .

a nonparametric method for extraction of candidate phrasal terms educational testing service
this paper introduces a new method for identifying candidate phrasal terms ( also known as multiword units ) which applies a nonparametric , rank-based heuristic measure . evaluation of this measure , the mutual rank ratio metric , shows that it produces better results than standard statistical measures when applied to this task .

representing predicative terms in the field of the environment marie-claude lhomme benot robichaud
terminological resources have traditionally focused on terms referring to entities , thereby ignoring other important concepts ( processes , events and properties ) in specialized fields of knowledge . consequently , large parts of the conceptual structure of these fields are not taken into consideration nor represented . in this article , we show how terms that refer to processes and events ( and , to a lesser extent , properties ) can be characterized using frame semantics ( fillmore , 1982 ) and the methodology developed within the framenet project ( ruppenhofer et al. , 2010 ) . more specifically , we applied the framework to a subset of terms in the field of the environment . frames are unveiled first by comparing similarities between the argument structures of terms already recorded in a terminological database and the relationships they share with other terms . a comparison is also carried out with the lexical units recorded in framenet . then , relations between frames are defined that allow us to build small conceptual scenarios that are specific to the field of the environment . these relations are determined on the basis of the set of relations listed in the framenet project . this article reports on the methodology , the frames defined up to now and two specific conceptual scenarios ( risk_scenario and managing_waste ) .

complex lexico-syntactic reformulation of sentences using typed
we present a framework for reformulating sentences by applying transfer rules on a typed dependency representation . we specify a list of operations that the framework needs to support and argue that typed dependency structures are currently the most suitable formalism for complex lexico-syntactic paraphrasing . we demonstrate our approach by reformulating sentences expressing the discourse relation of causation using four lexico-syntactic discourse markers cause as a verb and as a noun , because as a conjunction and because of as a preposition .

automatic arabic diacritics restoration based on deep nets
in this paper , arabic diacritics restoration problem is tackled under the deep learning framework presenting confused subset resolution ( csr ) method to improve the classification accuracy , in addition to arabic part-of-speech ( pos ) tagging framework using deep neural nets . special focus is given to syntactic diacritization , which still suffer low accuracy as indicated by related works . evaluation is done versus state-of-the-art systems reported in literature , with quite challenging datasets , collected from different domains . standard datasets like ldc arabic tree bank is used in addition to custom ones available online for results replication . results show significant improvement of the proposed techniques over other approaches , reducing the syntactic classification error to 9.9 % and morphological classification error to 3 % compared to 12.7 % and 3.8 % of the best reported results in literature , improving the error by 22 % over the best reported systems

flexible guidance generation using user model in spoken dialogue systems
we address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems . unlike previous studies that focus on users knowledge or typical kinds of users , the user model we propose is more comprehensive . specifically , we set up three dimensions of user models : skill level to the system , knowledge level on the target domain and the degree of hastiness . moreover , the models are automatically derived by decision tree learning using real dialogue data collected by the system . we obtained reasonable classification accuracy for all dimensions . dialogue strategies based on the user modeling are implemented in kyoto city bus information system that has been developed at our laboratory . experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users .

applying unsupervised learning to support vector space model based educational testing service
vector space models ( vsm ) have been widely used in the language assessment field to provide measurements of students vocabulary choices and content relevancy . however , training reference vectors ( rv ) in a vsm requires a time-consuming and costly human scoring process . to address this limitation , we applied unsupervised learning methods to reduce or even eliminate the human scoring step required for training rvs . our experiments conducted on data from a non-native english speaking test suggest that the unsupervised topic clustering is better at selecting responses to train rvs than random selection . in addition , we conducted an experiment to totally eliminate the need of human scoring . instead of using human rated scores to train rvs , we used used the machine-predicted scores from an automated speaking assessment system for training rvs . we obtained vsm-derived features that show promisingly high correlations to human-holistic scores , indicating that the costly human scoring process can be eliminated . index terms : vector space model ( vsm ) , speech assessment , unsupervised learning , document clustering

data-driven dependency parsing of new languages using incomplete and noisy training data
we present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing , by greedily exploiting partial structures . we analyze our approach in an annotation projection framework for dependency trees , and show how dependency parsers from two different paradigms ( graph-based and transition-based ) can be trained on the resulting tree fragments . we train parsers for dutch to evaluate our method and to investigate to which degree graph-based and transitionbased parsers can benefit from incomplete training data . we find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets , and achieve unlabeled attachment scores that are only 5 % behind the average uas for dutch in the conll-x shared task on supervised parsing ( buchholz and marsi , 2006 ) .

dude : a dialogue and understanding development environment , mapping business process models to information state update dialogue
we demonstrate a new development environment1 information state update dialogue systems which allows non-expert developers to produce complete spoken dialogue systems based only on a business process model ( bpm ) describing their application ( e.g . banking , cinema booking , shopping , restaurant information ) . the environment includes automatic generation of grammatical framework ( gf ) grammars for robust interpretation of spontaneous speech , and uses application databases to generate lexical entries and grammar rules . the gf grammar is compiled to an atk or nuance language model for speech recognition . the demonstration system allows users to create and modify spoken dialogue systems , starting with a definition of a business processmodel and ending with a working system . this paper describes the environment , its main components , and some of the research issues involved in its development .

on dual decomposition and linear programming relaxations for natural language processing
this paper introduces dual decomposition as a framework for deriving inference algorithms for nlp problems . the approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems , together with a simple method for forcing agreement between the different oracles . the approach provably solves a linear programming ( lp ) relaxation of the global inference problem . it leads to algorithms that are simple , in that they use existing decoding algorithms ; efficient , in that they avoid exact algorithms for the full model ; and often exact , in that empirically they often recover the correct solution in spite of using an lp relaxation . we give experimental results on two problems : 1 ) the combination of two lexicalized parsing models ; and 2 ) the combination of a lexicalized parsing model and a trigram part-of-speech tagger .

improving chinese word segmentation by adopting self-organized maps of character n-gram
character-based tagging method has achieved great success in chinese word segmentation ( cws ) . this paper proposes a new approach to improve the cws tagging accuracy by combining self-organizing map ( som ) with structured support vector machine ( svm ) for utilization of enormous unlabeled text corpus . first , character n-grams are clustered and mapped into a low-dimensional space by adopting som algorithm . two different maps are built based on the n-grams preceding and succeeding context respectively . then new features are extracted from these maps and integrated into the structured svm methods for cws . experimental results on bakeoff-2005 database show that som-based features can contribute more than 7 % relative error reduction , and the structured svm method for cws proposed in this paper also outperforms traditional conditional random field ( crf ) method .

domain adaptation for medical text translation using web resources
this paper describes adapting statistical machine translation ( smt ) systems to medical domain using in-domain and general-domain data as well as webcrawled in-domain resources . in order to complement the limited in-domain corpora , we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the internet . the collected data is used for adapting the language model and translation model to boost the overall translation quality . besides , we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific smt system . we attend the medical summary sentence unconstrained translation task of the ninth workshop on statistical machine translation ( wmt2014 ) . our systems achieve the second best bleu scores for czech-english , fourth for french-english , english-french language pairs and the third best results for reminding pairs .

a large scale arabic sentiment lexicon for arabic opinion mining
most opinion mining methods in english rely successfully on sentiment lexicons , such as english sentiwordnet ( eswn ) . while there have been efforts towards building arabic sentiment lexicons , they suffer from many deficiencies : limited size , unclear usability plan given arabics rich morphology , or nonavailability publicly . in this paper , we address all of these issues and produce the first publicly available large scale standard arabic sentiment lexicon ( arsenl ) using a combination of existing resources : eswn , arabic wordnet , and the standard arabic morphological analyzer ( sama ) . we compare and combine two methods of constructing this lexicon with an eye on insights for arabic dialects and other low resource languages . we also present an extrinsic evaluation in terms of subjectivity and sentiment analysis .

integrating motion predicate classes with
we propose a spatio-temporal markup for the annotation of motion predicates in text , informed by a lexical semantic classification of these verbs . we incorporate this classification within a spatial event structure , based on generative lexicon theory . we discuss how the spatial event structure suggests changes to annotation systems designed solely for temporal or spatial phenomena , resulting in spatio-temporal annotation .

to compute distributional thesauri fg language technology
we introduce a new highly scalable approach for computing distributional thesauri ( dts ) . by employing pruning techniques and a distributed framework , we make the computation for very large corpora feasible on comparably small computational resources . we demonstrate this by releasing a dt for the whole vocabulary of google books syntactic n-grams . evaluating against lexical resources using two measures , we show that our approach produces higher quality dts than previous approaches , and is thus preferable in terms of speed and quality for large corpora .

maximal match chinese segmentation augmented by resources generated from a very large dictionary for post-processing
we used a production segmentation system , which draws heavily on a large dictionary derived from processing a large amount ( over 150 million chinese characters ) of synchronous textual data gathered from various chinese speech communities , including beijing , hong kong , taipei , and others . we run this system in two tracks in the second international chinese word segmentation bakeoff , with backward maximal matching ( right-to-left ) as the primary mechanism . we also explored the use of a number of supplementary features offered by the large dictionary in postprocessing , in an attempt to resolve ambiguities and detect unknown words . while the results might not have reached their fullest potential , they nevertheless reinforced the importance and usefulness of a large dictionary as a basis for segmentation , and the implication of following a uniform standard on the segmentation performance on data from various sources .

dependency annotation scheme for indian languages
the paper introduces a dependency annotation effort which aims to fully annotate a million word hindi corpus . it is the first attempt of its kind to develop a large scale tree-bank for an indian language . in this paper we provide the motivation for following the paninian framework as the annotation scheme and argue that the paninian framework is better suited to model the various linguistic phenomena manifest in indian languages . we present the basic annotation scheme . we also show how the scheme handles some phenomenon such as complex verbs , ellipses , etc . empirical results of some experiments done on the currently annotated sentences are also reported .

what is the jeopardy model a quasi-synchronous grammar for qa
this paper presents a syntax-driven approach to question answering , specifically the answer-sentence selection problem for short-answer questions . rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( correct ) answers relate to each other via loose but predictable syntactic transformations . we propose a probabilistic quasi-synchronous grammar , inspired by one proposed for machine translation ( d. smith and eisner , 2006 ) , and parameterized by mixtures of a robust nonlexical syntax/alignment model with a ( n optional ) lexical-semantics-driven log-linear model . our model learns soft alignments as a hidden variable in discriminative training . experimental results using the trec dataset are shown to significantly outperform strong state-of-the-art baselines .

identifying collocations to measure compositionality : shared task system description
this paper describes three systems from the university of minnesota , duluth that participated in the disco 2011 shared task that evaluated distributional methods of measuring semantic compositionality . all three systems approached this as a problem of collocation identification , where strong collocates are assumed to be minimally compositional . duluth1 relies on the t-score , whereas duluth-2 and duluth-3 rely on pointwise mutual information ( pmi ) . duluth-1 was the top ranked system overall in coarsegrained scoring , which was a 3-way category assignment where pairs were assigned values of high , medium , or low compositionality .

soft cross-lingual syntax projection for dependency parsing
this paper proposes a simple yet effective framework of soft cross-lingual syntax projection to transfer syntactic structures from source language to target language using monolingual treebanks and large-scale bilingual parallel text . here , soft means that we only project reliable dependencies to compose high-quality target structures . the projected instances are then used as additional training data to improve the performance of supervised parsers . the major issues for this idea are 1 ) errors from the source-language parser and unsupervised word aligner ; 2 ) intrinsic syntactic non-isomorphism between languages ; 3 ) incomplete parse trees after projection . to handle the first two issues , we propose to use a probabilistic dependency parser trained on the target-language treebank , and prune out unlikely projected dependencies that have low marginal probabilities . to make use of the incomplete projected syntactic structures , we adopt a new learning technique based on ambiguous labelings . for a word that has no head words after projection , we enrich the projected structure with all other words as its candidate heads as long as the newly-added dependency does not cross any projected dependencies . in this way , the syntactic structure of a sentence becomes a parse forest ( ambiguous labels ) instead of a single parse tree . during training , the objective is to maximize the mixed likelihood of manually labeled instances and projected instances with ambiguous labelings . experimental results on benchmark data show that our method significantly outperforms a strong baseline supervised parser and previous syntax projection methods .

automatic short answer marking
our aim is to investigate computational linguistics ( cl ) techniques in marking short free text responses automatically . successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding . however , recent advances in cl techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers . this paper describes some of the techniques we have tried so far vis -- vis this problem with results , discussion and description of the main issues encountered.1

what is at stake : a case study of russian expressions starting with a preposition
the paper describes an experiment in detecting a specific type of multiword expressions in russian , namely expressions starting with a preposition . this covers not only prepositional phrases proper , but also fixed syntactic constructions like v techenie ( in the course of ) . first , we collect lists of such constructions in a corpus of 50 mln words using a simple mechanism that combines statistical methods with knowledge about the structure of russian prepositional phrases . then we analyse the results of this data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus .

renew : a semi-supervised framework for generating domain-specific
the sentiment captured in opinionated text provides interesting and valuable information for social media services . however , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment . we propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level . our framework can greatly reduce the human effort for building a domainspecific sentiment lexicon with high quality . specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average fmeasure gains of 3 % . our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach based on elementary discourse units .

biomedical event extraction without training data andreas vlachos , paula buttery , diarmuid o seaghdha , ted briscoe
we describe our system for the bionlp 2009 event detection task . it is designed to be as domain-independent and unsupervised as possible . nevertheless , the precisions achieved for single theme event classes range from 75 % to 92 % , while maintaining reasonable recall . the overall f-scores achieved were 36.44 % and 30.80 % on the development and the test sets respectively .

the inside-outside recursive neural network model for
we propose the first implementation of an infinite-order generative dependency model . the model is based on a new recursive neural network architecture , the inside-outside recursive neural network . this architecture allows information to flow not only bottom-up , as in traditional recursive neural networks , but also topdown . this is achieved by computing content as well as context representations for any constituent , and letting these representations interact . experimental results on the english section of the universal dependency treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting , and tends to choose more accurate parses in k-best lists . in addition , reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores .

a conditional random field word segmenter
we present a chinese word segmentation system submitted to the closed track of sighan bakeoff 2005. our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity , morphological and character reduplication features . because our morphological features were extracted from the training corpora automatically , our system was not biased toward any particular variety of mandarin . thus , our system does not overfit the variety of mandarin most familiar to the system 's designers . our final system achieved a f-score of 0.947 ( as ) , 0.943 ( hk ) , 0.950 ( pk ) and 0.964 ( msr ) .

annotating discourse connectives and their arguments
this paper describes a new , large scale discourse-level annotation project the penn discourse treebank ( pdtb ) . we present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments . the pdtb is being built directly on top of the penn treebank and propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms . we provide a detailed preliminary analysis of inter-annotator agreement both the level of agreement and the types of inter-annotator variation .

context-dependent smt model using bilingual verb-noun collocation
in this paper , we propose a new contextdependent smt model that is tightly coupled with a language model . it is designed to decrease the translation ambiguities and efficiently search for an optimal hypothesis by reducing the hypothesis search space . it works through reciprocal incorporation between source and target context : a source word is determined by the context of previous and corresponding target words and the next target word is predicted by the pair consisting of the previous target word and its corresponding source word . in order to alleviate the data sparseness in chunk-based translation , we take a stepwise back-off translation strategy . moreover , in order to obtain more semantically plausible translation results , we use bilingual verb-noun collocations ; these are automatically extracted by using chunk alignment and a monolingual dependency parser . as a case study , we experimented on the language pair of japanese and korean . as a result , we could not only reduce the search space but also improve the performance .

exploring features for localized detection of speech recognition errors
we address the problem of localized error detection in automatic speech recognition ( asr ) output to support the generation of targeted clarifications in spoken dialogue systems . localized error detection finds specific mis-recognized words in a user utterance . targeted clarifications , in contrast with generic please repeat/rephrase clarifications , target a specific mis-recognized word in an utterance ( stoyanchev et al , 2012a ) and require accurate detection of such words . we extend and modify work presented in ( stoyanchev et al. , 2012b ) by experimenting with a new set of features for predicting the likelihood of a local error in an asr hypothesis on an unsifted version of the original dataset . we improve over baseline results , where only asrgenerated features are used , by constructing optimal feature sets for utterance and word mis-recognition prediction . the f-measure for identifying incorrect utterances improves by 2.2 % and by 3.9 % for identifiying incorrect words .

an annotation tool for multimodal dialogue corpora using global document annotation
this paper reports a tool which assists the user in annotating a video corpus and enables the user to search for a semantic or pragmatic structure in a gda tagged corpus . an xql format is allowed for search patterns as well as a plain phrase . this tool is capable of generating a gda timestamped corpus from a video file manually . it will be publicly available for academic purposes .

bridging smt and tm with translation recommendation
we propose a translation recommendation framework to integrate statistical machine translation ( smt ) output with translation memory ( tm ) systems . the framework recommends smt outputs to a tm user when it predicts that smt outputs are more suitable for post-editing than the hits provided by the tm . we describe an implementation of this framework using an svm binary classifier . we exploit methods to fine-tune the classifier and investigate a variety of features of different types . we rely on automatic mt evaluation metrics to approximate human judgements in our experiments . experimental results show that our system can achieve 0.85 precision at 0.89 recall , excluding exact matches . furthermore , it is possible for the end-user to achieve a desired balance between precision and recall by adjusting confidence levels .

co-regularizing character-based and word-based models for semi-supervised chinese word segmentation
this paper presents a semi-supervised chinese word segmentation ( cws ) approach that co-regularizes character-based and word-based models . similarly to multi-view learning , the segmentation agreements between the two different types of view are used to overcome the scarcity of the label information on unlabeled data . the proposed approach trains a character-based and word-based model on labeled data , respectively , as the initial models . then , the two models are constantly updated using unlabeled examples , where the learning objective is maximizing their segmentation agreements . the agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data . the segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models . the evaluation on the chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature .

language models and reranking for machine translation
complex language models can not be easily integrated in the first pass decoding of a statistical machine translation system the decoder queries the lm a very large number of times ; the search process in the decoding builds the hypotheses incrementally and can not make use of lms that analyze the whole sentence . we present in this paper the language computers system for wmt06 that employs lmpowered reranking on hypotheses generated by phrase-based smt systems

cityu-dac : disambiguating sentiment-ambiguous adjectives within
this paper describes our system participating in task 18 of semeval-2010 , i.e . disambiguating sentimentambiguous adjectives ( saas ) . to disambiguating saas , we compare the machine learning-based and lexiconbased methods in our submissions : 1 ) maximum entropy is used to train classifiers based on the annotated chinese data from the ntcir opinion analysis tasks , and the clause-level and sentence-level classifiers are compared ; 2 ) for the lexicon-based method , we first classify the adjectives into two classes : intensifiers ( i.e . adjectives intensifying the intensity of context ) and suppressors ( i.e . adjectives decreasing the intensity of context ) , and then use the polarity of context to get the saas contextual polarity based on a sentiment lexicon . the results show that the performance of maximum entropy is not quite high due to little training data ; on the other hand , the lexicon-based method could improve the precision by considering the polarity of context .

example selection for bootstrapping statistical parsers julia hockenmaier , paul ruhlen steven baker , jeremiah crim
this paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data . we consider both a mostly-unsupervised approach , co-training , in which two parsers are iteratively re-trained on each others output ; and a semi-supervised approach , corrected co-training , in which a human corrects each parsers output before adding it to the training data . the selection of labeled training examples is an integral part of both frameworks . we propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility . we show that incorporating the utility criterion into the selection method results in better parsers for both frameworks .

unknown word extraction for chinese documents
there is no blank to mark word boundaries in chinese text . as a result , identifying words is difficult , because of segmentation ambiguities and occurrences of unknown words . conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient . however the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall , since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods . in addition to statistical information , we try to use as much information as possible , such as morphology , syntax , semantics , and world knowledge . the identification system fully utilizes the context and content information of unknown words in the steps of detection process , extraction process , and verification process . a practical unknown word extraction system was implemented which online identifies new words , including low frequency new words , with high precision and high recall rates .

pycwn : a python module for chinese wordnet
this presentation introduces a python module ( pycwn ) for accessing and processing chinese lexical resources . in particular , our focus is put on the chinese wordnet ( cwn ) that has been developed and released by cwn group at academia sinica . pycwn provides the access to chinese wordnet ( sense and relation data ) under the python environment . the presenation further demonstrates how this module applies to a variety of lexical processing tasks as well as the potentials for multilingual lexical processing .

a method for unsupervised broad-coverage lexical error detection and correction
we describe and motivate an unsupervised lexical error detection and correction algorithm and its application in a tool called lexbar appearing as a query box on the web browser toolbar or as a search engine interface . lexbar accepts as user input candidate strings of english to be checked for acceptability and , where errors are detected , offers corrections . we introduce the notion of hybrid n-gram and extract these from bnc as the knowledgebase against which to compare user input . an extended notion of edit distance is used to identify most likely candidates for correcting detected errors . results are illustrated with four types of errors .

using information about multi-word expressions for the word-alignment task
it is well known that multi-word expressions are problematic in natural language processing . in previous literature , it has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically . in this paper , we propose a framework in which information about the multi-word expressions can be used in the word-alignment task . we have shown that even simple features like point-wise mutual information are useful for word-alignment task in english-hindi parallel corpora . the alignment error rate which we achieve ( aer = 0.5040 ) is significantly better ( about 10 % decrease in aer ) than the alignment error rates of the state-of-art models ( och and ney , 2003 ) ( best aer = 0.5518 ) on the english-hindi dataset .

semantic relations between nouns paul nulty fintan costello
this paper investigates methods for using lexical patterns in a corpus to deduce the semantic relation that holds between two nouns in a noun-noun compound phrase such as flu virus or morning exercise . much of the previous work in this area has used automated queries to commercial web search engines . in our experiments we use the google web 1t corpus . this corpus contains every 2,3 , 4 and 5 gram occurring more than 40 times in google 's index of the web , but has the advantage of being available to researchers directly rather than through a web interface . this paper evaluates the performance of the web 1t corpus on the task compared to similar systems in the literature , and also investigates what kind of lexical patterns are most informative when trying to identify a semantic relation between two nouns .

multilingual syntactic-semantic dependency parsing with three-stage approximate max-margin linear models
this paper describes a system for syntacticsemantic dependency parsing for multiple languages . the system consists of three parts : a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing , a predicate classifier , and an argument classifier for semantic dependency parsing . for semantic dependency parsing , we explore use of global features . all components are trained with an approximate max-margin learning algorithm . in the closed challenge of the conll-2009 shared task ( hajic et al , 2009 ) , our system achieved the 3rd best performances for english and czech , and the 4th best performance for japanese .

references to named entities : a corpus study
references included in multi-document summaries are often problematic . in this paper , we present a corpus study performed to derive a statistical model for the syntactic realization of referential expressions . the interpretation of the probabilistic data helps us gain insight on how extractive summaries can be rewritten in an efficient manner to produce more fluent and easy-to-read text .

a hybrid approach for extracting semantic relations from texts
we present an approach for extracting relations from texts that exploits linguistic and empirical strategies , by means of a pipeline method involving a parser , partof-speech tagger , named entity recognition system , pattern-based classification and word sense disambiguation models , and resources such as ontology , knowledge base and lexical databases . the relations extracted can be used for various tasks , including semantic web annotation and ontology learning . we suggest that the use of knowledge intensive strategies to process the input text and corpusbased techniques to deal with unpredicted cases and ambiguity problems allows to accurately discover the relevant relations between pairs of entities in that text .

improving alignment for smt by reordering and augmenting the training corpus
we describe the liu systems for englishgerman and german-english translation in the wmt09 shared task . we focus on two methods to improve the word alignment : ( i ) by applying giza++ in a second phase to a reordered training corpus , where reordering is based on the alignments from the first phase , and ( ii ) by adding lexical data obtained as highprecision alignments from a different word aligner . these methods were studied in the context of a system that uses compound processing , a morphological sequence model for german , and a partof-speech sequence model for english . both methods gave some improvements to translation quality as measured by bleu and meteor scores , though not consistently . all systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration .

adaptive dialogue systems - interaction with interact
technological development has made computer interaction more common and also commercially feasible , and the number of interactive systems has grown rapidly . at the same time , the systems should be able to adapt to various situations and various users , so as to provide the most efficient and helpful mode of interaction . the aim of the interact project is to explore natural human-computer interaction and to develop dialogue models which will allow users to interact with the computer in a natural and robust way . the paper describes the innovative goals of the project and presents ways that the interact system supports adaptivity on different system design and interaction management levels .

active learning for classifying phone sequences from unsupervised
this paper describes an application of active learning methods to the classification of phone strings recognized using unsupervised phonotactic models . the only training data required for classification using these recognition methods is assigning class labels to the audio files . the work described here demonstrates that substantial savings in this effort can be obtained by actively selecting examples to be labeled using confidence scores from the boostexter classifier . the saving in class labeling effort is evaluated on two different spoken language system domains in terms both of the number of utterances to be labeled and the length of the labeled utterances in phones . we show that savings in labeling effort of around 30 % can be obtained using active selection of examples .

semantic relatedness from automatically generated semantic
we introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated , large-scale semantic network . we present promising first results that indicate potential competitiveness with approaches based on manually created resources .

cocqa : co-training over questions and answers with an application to predicting question subjectivity orientation
an increasingly popular method for finding information online is via the community question answering ( cqa ) portals such as yahoo ! answers , naver , and baidu knows . searching the cqa archives , and ranking , filtering , and evaluating the submitted answers requires intelligent processing of the questions and answers posed by the users . one important task is automatically detecting the questions subjectivity orientation : namely , whether a user is searching for subjective or objective information . unfortunately , real user questions are often vague , ill-posed , poorly stated . furthermore , there has been little labeled training data available for real user questions . to address these problems , we present cocqa , a co-training system that exploits the association between the questions and contributed answers for question analysis tasks . the co-training approach allows cocqa to use the effectively unlimited amounts of unlabeled data readily available in cqa archives . in this paper we study the effectiveness of cocqa for the question subjectivity classification task by experimenting over thousands of real users questions .

ukp-biu : similarity and entailment metrics for student response analysis torsten zesch omer levy iryna gurevych ido dagan
our system combines text similarity measures with a textual entailment system . in the main task , we focused on the influence of lexicalized versus unlexicalized features , and how they affect performance on unseen questions and domains . we also participated in the pilot partial entailment task , where our system significantly outperforms a strong baseline .

syntactic and semantic dependencies in multiple languages
for the 11th straight year , the conference on computational natural language learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting . in 2009 , the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages . this shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task . in this paper , we define the shared task , describe how the data sets were created and show their quantitative properties , report the results and summarize the approaches of the participating systems .

an initial study of topical poetry segmentation
this work performs some basic research upon topical poetry segmentation in a pilot study designed to test some initial assumptions and methodologies . nine segmentations of the poem titled kubla khan ( coleridge , 1816 , pp . 55-58 ) are collected and analysed , producing low but comparable inter-coder agreement . analyses and discussions of these codings focus upon how to improve agreement and outline some initial results on the nature of topics in this poem .

a proposal for building highly knowledge bases from the web
this paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources . basically , the method uses a wide-coverage and accurate knowledgebased word sense disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web . knownet , the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . in fact , knownet is several times larger than any available knowledge resource encoding relations between synsets , and the knowledge that knownet contains outperform any other resource when empirically evaluated in a common multilingual framework . 71 72 cuadros and rigau

unsupervised ontology induction from text
extracting knowledge from unstructured text is a long-standing goal of nlp . although learning approaches to many of its subtasks have been developed ( e.g. , parsing , taxonomy induction , information extraction ) , all end-to-end solutions to date require heavy supervision and/or manual engineering , limiting their scope and scalability . we present ontousp , a system that induces and populates a probabilistic ontology using only dependency-parsed text as input . ontousp builds on the usp unsupervised semantic parser by jointly forming isa and is-part hierarchies of lambda-form clusters . the isa hierarchy allows more general knowledge to be learned , and the use of smoothing for parameter estimation . we evaluate ontousp by using it to extract a knowledge base from biomedical abstracts and answer questions . ontousp improves on the recall of usp by 47 % and greatly outperforms previous state-of-the-art approaches .

experiments with crowdsourced re-annotation of a pos tagging data set
crowdsourcing lets us collect multiple annotations for an item from several annotators . typically , these are annotations for non-sequential classification tasks . while there has been some work on crowdsourcing named entity annotations , researchers have largely assumed that syntactic tasks such as part-of-speech ( pos ) tagging can not be crowdsourced . this paper shows that workers can actually annotate sequential data almost as well as experts . further , we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks .

language-independent context aware query translation using wikipedia rohit bharadwaj g
cross lingual information access ( clia ) systems are required to access the large amounts of multilingual content generated on the world wide web in the form of blogs , news articles and documents . in this paper , we discuss our approach to query formation for clia systems where language resources are replaced by wikipedia . we claim that wikipedia , with its rich multilingual content and structure , forms an ideal platform to build a clia system . our approach is particularly useful for under-resourced languages , as all the languages dont have the resources ( tools ) with sufficient accuracies . we propose a context aware language-independent query formation method which , with the help of bilingual dictionaries , forms queries in the target language . results are encouraging with a precision of 69.75 % and thus endorse our claim on using wikipedia for building clia systems .

towards free-text semantic parsing : a unified framework based on
this article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : framenet , verbnet and propbank . the framenet corpus contains the examples annotated with semantic roles whereas the verbnet lexicon provides the knowledge about the syntactic behavior of the verbs . we connect verbnet and framenet by mapping the framenet frames to the verbnet intersective levin classes . the propbank corpus , which is tightly connected to the verbnet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach . the results indicate that our model is an interesting step towards the design of free-text semantic parsers .

exploring vector space models to predict the compositionality of german noun-noun compounds
this paper explores two hypotheses regarding vector space models that predict the compositionality of german noun-noun compounds : ( 1 ) against our intuition , we demonstrate that window-based rather than syntax-based distributional features perform better predictions , and that not adjectives or verbs but nouns represent the most salient part-of-speech . our overall best result is state-of-the-art , reaching spearmans = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus . ( 2 ) while there are no significant differences in predicting compoundmodifier vs. compoundhead ratings on compositionality , we show that the modifier ( rather than the head ) properties predominantly influence the degree of compositionality of the compound .

designing agreement features for realization ranking
this paper shows that incorporating linguistically motivated features to ensure correct animacy and number agreement in an averaged perceptron ranking model for ccg realization helps improve a state-ofthe-art baseline even further . traditionally , these features have been modelled using hard constraints in the grammar . however , given the graded nature of grammaticality judgements in the case of animacy we argue a case for the use of a statistical model to rank competing preferences . though subject-verb agreement is generally viewed to be syntactic in nature , a perusal of relevant examples discussed in the theoretical linguistics literature ( kathol , 1999 ; pollard and sag , 1994 ) points toward the heterogeneous nature of english agreement . compared to writing grammar rules , our method is more robust and allows incorporating information from diverse sources in realization . we also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter . the full model yields significant improvements in bleu scores on section 23 of the ccgbank and makes many fewer agreement errors .

morphological analysis of the spontaneous speech corpus
this paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech . we use a morphological analysis system based on a maximum entropy model , which is independent of the domain of corpora . in this paper we show the tagging accuracy achieved by using the model and discuss problems in tagging the spontaneous speech corpus . we also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain .

unsupervised parse selection for hpsg
parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model . in the standard process , the parse selection model is trained over a hand-disambiguated treebank , meaning that without a significant investment of effort to produce the treebank , parse selection is not possible . furthermore , as treebanking is generally streamlined with parse selection models , creating the initial treebank without a model requires more resources than subsequent treebanks . in this work , we show that , by taking advantage of the constrained nature of these hpsg grammars , we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion . this allows us to bootstrap the treebanking process and provide better parsers faster , and with less resources .

whats in a message
in this paper we present the first step in a larger series of experiments for the induction of predicate/argument structures . the structures that we are inducing are very similar to the conceptual structures that are used in frame semantics ( such as framenet ) . those structures are called messages and they were previously used in the context of a multi-document summarization system of evolving events . the series of experiments that we are proposing are essentially composed from two stages . in the first stage we are trying to extract a representative vocabulary of words . this vocabulary is later used in the second stage , during which we apply to it various clustering approaches in order to identify the clusters of predicates and argumentsor frames and semantic roles , to use the jargon of frame semantics . this paper presents in detail and evaluates the first stage .

extracting sense trees from the romanian thesaurus by sense segmentation & dependency parsing
this paper aims to introduce a new parsing strategy for large dictionary ( thesauri ) parsing , called dictionary sense segmentation & dependency ( dssd ) , devoted to obtain the sense tree , i.e . the hierarchy of the defined meanings , for a dictionary entry . the real novelty of the proposed approach is that , contrary to dictionary standard parsing , dssd looks for and succeeds to separate the two essential processes within a dictionary entry parsing : sense tree construction and sense definition parsing . the key tools to accomplish the task of ( autonomous ) sense tree building consist in defining the dictionary sense marker classes , establishing a tree-like hierarchy of these classes , and using a proper searching procedure of sense markers within the dssd parsing algorithm . a similar but more general approach , using the same techniques and data structures for ( romanian ) free text parsing is scd ( segmentation-cohesiondependency ) ( curteanu ; 1988 , 2006 ) , which dssd is inspired from . a dssdbased parser is implemented in java , building currently 91 % correct sense trees from dtlr ( dicionarul tezaur al 2008. licensed under the creative commons attribution-noncommercial-share alike 3.0 unported license . some rights reserved . limbii romne romanian language thesaurus ) entries , with significant resources to improve and enlarge the dtlr lexical semantics analysis .

active learning in noisy conditions for spoken language
active learning has proved effective in many fields of natural language processing . however , in the field of spoken language understanding which is always dealing with noise , no complete comparison between different active learning methods has been done . this paper compares the best known active learning methods in noisy conditions for spoken language understanding . additionally a new method based on fisher information named as weighted gradient uncertainty ( wgu ) is proposed . furthermore , strict local density ( sld ) method is proposed based on a new concept of local density and a new technique of utilizing information density measures . results demonstrate that both proposed methods outperform the best performance of the previous methods in noisy and noise-free conditions with sld being superior to wgu slightly .

a fast finite-state relaxation method for enforcing global constraints on sequence decoding
we describe finite-state constraint relaxation , a method for applying global constraints , expressed as automata , to sequence model decoding . we present algorithms for both hard constraints and binary soft constraints . on the conll-2004 semantic role labeling task , we report a speedup of at least 16x over a previous method that used integer linear programming .

sense information for disambiguation : confluence of supervised and unsupervised methods
for senseval-2 , we disambiguated the lexical sample using two different sense inventories . official senseval-2 results were generated using wordnet , and separately using the new oxford dictionary of english ( node ) . since our initial submission , we have implemented additional routines and have now examined the differences in the features used for making sense selections . we report here the contribution of default sense selection , idiomatic usage , syntactic and semantic clues , subcategorization patterns , word forms , syntactic usage , context , selectional preferences , and topics or subject fields . we also compare the differences between wordnet and node . finally , we compare these features to those identified as significant in supervised learning approaches .

word sense disambiguation using a dictionary for sense similarity measure
this paper presents a disambiguation method in which word senses are determined using a dictionary . we use a semantic proximity measure between words in the dictionary , taking into account the whole topology of the dictionary , seen as a graph on its entries . we have tested the method on the problem of disambiguation of the dictionary entries themselves , with promising results considering we do not use any prior annotated data .

crf tagging for head recognition based on stanford parser
chinese parsing has received more and more attention , and in this paper , we use toolkit to perform parsing on the data of tsinghua chinese treebank ( tct ) used in cips , and we use conditional random fields ( crfs ) to train specific model for the head recognition . at last , we compare different results on different pos results .

multilingual speech recognition for information retrieval in indian context
this paper analyzes various issues in building a hmm based multilingual speech recognizer for indian languages . the system is originally designed for hindi and tamil languages and adapted to incorporate indian accented english . language-specific characteristics in speech recognition framework are highlighted . the recognizer is embedded in information retrieval applications and hence several issues like handling spontaneous telephony speech in real-time , integrated language identification for interactive response and automatic grapheme to phoneme conversion to handle out of vocabulary words are addressed . experiments to study relative effectiveness of different algorithms have been performed and the results are investigated .

wide-coverage parsing of speech transcripts
this paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts . two parsers ( c & c ccg and rasp ) are tested on the speech transcripts of two different domains ( parent-child language , and picture descriptions ) . the performance difference between the domain-independent parsers and two domain-trained parsers ( mstparser and megrasp ) is substantial , with a difference of at least 30 percent point in accuracy . despite this gap , some of the grammatical relations can still be recovered reliably .

lexicalized markov grammars for sentence compression
we present a sentence compression system based on synchronous context-free grammars ( scfg ) , following the successful noisy-channel approach of ( knight and marcu , 2000 ) . we define a headdriven markovization formulation of scfg deletion rules , which allows us to lexicalize probabilities of constituent deletions . we also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora , which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora . finally , we evaluate different markovized models , and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements , and that produces sentences that were judged more grammatical than those generated by previous work .

empirical analysis of aggregation methods for collective annotation
we investigate methods for aggregating the judgements of multiple individuals in a linguistic annotation task into a collective judgement . we define several aggregators that take the reliability of annotators into account and thus go beyond the commonly used majority vote , and we empirically analyse their performance on new datasets of crowdsourced data .

a smorgasbord of features for automatic mt evaluation
this document describes the approach by the nlp group at the technical university of catalonia ( upc-lsi ) , for the shared task on automatic evaluation of machine translation at the acl 2008 third smt workshop .

supporting annotation layers for natural language processing
we demonstrate a system for flexible querying against text that has been annotated with the results of nlp processing . the system supports self-overlapping and parallel layers , integration of syntactic and ontological hierarchies , flexibility in the format of returned results , and tight integration with sql . we present a query language and its use on examples taken from the nlp literature .

referring expression generation
in this paper we present research in which we apply ( i ) the kind of intrinsic evaluation metrics that are characteristic of current comparative hlt evaluation , and ( ii ) extrinsic , human task-performance evaluations more in keeping with nlg traditions , to 15 systems implementing a language generation task . we analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task .

supersense tagging for arabic : the mt-in-the-middle attack
we consider the task of tagging arabic nouns with wordnet supersenses . three approaches are evaluated . the first uses an expertcrafted but limited-coverage lexicon , arabic wordnet , and heuristics . the second uses unsupervised sequence modeling . the third and most successful approach uses machine translation to translate the arabic into english , which is automatically tagged with english supersenses , the results of which are then projected back into arabic . analysis shows gains and remaining obstacles in four wikipedia topical domains .

detecting errors in discontinuous structural annotation
consistency of corpus annotation is an essential property for the many uses of annotated corpora in computational and theoretical linguistics . while some research addresses the detection of inconsistencies in positional annotation ( e.g. , partof-speech ) and continuous structural annotation ( e.g. , syntactic constituency ) , no approach has yet been developed for automatically detecting annotation errors in discontinuous structural annotation . this is significant since the annotation of potentially discontinuous stretches of material is increasingly relevant , from treebanks for free-word order languages to semantic and discourse annotation . in this paper we discuss how the variation n-gram error detection approach ( dickinson and meurers , 2003a ) can be extended to discontinuous structural annotation . we exemplify the approach by showing how it successfully detects errors in the syntactic annotation of the german tiger corpus ( brants et al , 2002 ) .

spoken language dialogue systems laila dybkjr and niels ole bernsen
the paper first addresses a series of issues basic to evaluating the usability of spoken language dialogue systems , including types and purpose of evaluation , when to evaluate and which methods to use , user involvement , how to evaluate and what to evaluate . we then go on to present and discuss a comprehensive set of usability evaluation criteria for spoken language dialogue systems .

extensible multimodal annotation markup language ( emma )
this talk will introduce the w3c multimodal interaction activity , whose goal is to design a framework of specifications to enable access to the web using multi-modal interaction . in particular we will introduce the extensible multimodal annotation ( emma ) language specification . emma is an xml language for describing the interpretation of user input , combining transcriptions of raw signals into words with metadata to help applications resolve uncertainties and contradictions in interpretations . we will also discuss the issues encountered by the working group as the language is being developed , such as whether to use rdf or not , how best to combine interpretations , or what metadata properties to represent .

undirected machine translation with discriminative reinforcement learning
we present a novel undirected machine translation model of hierarchical mt that

domain kernels for word sense disambiguation
in this paper we present a supervised word sense disambiguation methodology , that exploits kernel methods to model sense distinctions . in particular a combination of kernel functions is adopted to estimate independently both syntagmatic and domain similarity . we defined a kernel function , namely the domain kernel , that allowed us to plug external knowledge into the supervised learning process . external knowledge is acquired from unlabeled data in a totally unsupervised way , and it is represented by means of domain models . we evaluated our methodology on several lexical sample tasks in different languages , outperforming significantly the state-of-the-art for each of them , while reducing the amount of labeled training data required for learning .

french timebank : an iso-timeml annotated reference corpus
this article presents the main points in the creation of the french timebank ( bittar , 2010 ) , a reference corpus annotated according to the iso-timeml standard for temporal annotation . a number of improvements were made to the markup language to deal with linguistic phenomena not yet covered by iso-timeml , including cross-language modifications and others specific to french . an automatic preannotation system was used to speed up the annotation process . a preliminary evaluation of the methodology adopted for this project yields positive results in terms of data quality and annotation time .

probabilistic frame-semantic parsing
this paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an english sentence into a frame-semantic representation . it finds words that evoke framenet frames , selects frames for them , and locates the arguments for each frame . the system uses two featurebased , discriminative probabilistic ( log-linear ) models , one with latent variables to permit disambiguation of new predicate words . the parser is demonstrated to significantly outperform previously published results .

a decision-theoretic approach to natural language generation
we study the problem of generating an english sentence given an underlying probabilistic grammar , a world and a communicative goal . we model the generation problem as a markov decision process with a suitably defined reward function that reflects the communicative goal . we then use probabilistic planning to solve the mdp and generate a sentence that , with high probability , accomplishes the communicative goal . we show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art . further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .

weblog classification for fast splog filtering : a url language model segmentation approach
this paper shows that in the context of statistical weblog classification for splog filtering based on n-grams of tokens in the url , further segmenting the urls beyond the standard punctuation is helpful . many splog urls contain phrases in which the words are glued together in order to avoid splog filtering techniques based on punctuation segmentation and unigrams . a technique which segments long tokens into the words forming the phrase is proposed and evaluated . the resulting tokens are used as features for a weblog classifier whose accuracy is similar to that of humans ( 78 % vs. 76 % ) and reaches 93.3 % of precision in identifying splogs with recall of 50.9 % .

construction of domain dictionary for fundamental vocabulary
for natural language understanding , it is essential to reveal semantic relations between words . to date , only the is-a relation has been publicly available . toward deeper natural language understanding , we semiautomatically constructed the domain dictionary that represents the domain relation between japanese fundamental words . this is the first japanese domain resource that is fully available . besides , our method does not require a document collection , which is indispensable for keyword extraction techniques but is hard to obtain . as a task-based evaluation , we performed blog categorization . also , we developed a technique for estimating the domain of unknown words .

bart : a modular toolkit for coreference resolution simone paolo ponzetto
developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort , yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference , or for researchers who want to concentrate on a specific aspect of the problem . we present bart , a highly modular toolkit for developing coreference applications . in the johns hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation , the toolkit was used to extend a reimplementation of the soon et al ( 2001 ) proposal with a variety of additional syntactic and knowledge-based features , and experiment with alternative resolution processes , preprocessing tools , and classifiers .

the types and distributions of errors in a wide coverage surface realizer evaluation
recent empirical experiments on surface realizers have shown that grammars for generation can be effectively evaluated using large corpora . evaluation metrics are usually reported as single averages across all possible types of errors and syntactic forms . but the causes of these errors are diverse , and the extent to which the accuracy of generation over individual syntactic phenomena is unknown . this article explores the types of errors , both computational and linguistic , inherent in the evaluation of a surface realizer when using large corpora . we analyze data from an earlier wide coverage experiment on the fuf/surge surface realizer with the penn treebank in order to empirically classify the sources of errors and describe their frequency and distribution . this both provides a baseline for future evaluations and allows designers of nlg applications needing off-the-shelf surface realizers to choose on a quantitative basis .

eliciting subjectivity and polarity judgements on word senses
there has been extensive work on eliciting human judgements on the sentiment of words and the resulting annotated word lists have frequently been used for opinion mining applications in natural language processing ( nlp ) . however , this word-based approach does not take different senses of a word into account , which might differ in whether and what kind of sentiment they evoke . in this paper , we therefore introduce a human annotation scheme for judging both the subjectivity and polarity of word senses . we show that the scheme is overall reliable , making this a well-defined task for automatic processing . we also discuss three issues that surfaced during annotation : the role of annotation bias , hierarchical annotation ( or underspecification ) and bias in the sense inventory used .

vector space semantics with frequency-driven motifs
traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items . in this work , we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs . the framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design . we design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable , and linguistically disambiguated . hellinger pca embeddings learnt using the framework show competitive results on empirical tasks .

extensible dependency grammar : a new methodology
this paper introduces the new grammar formalism of extensible dependency grammar ( xdg ) , and emphasizes the benefits of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description . this has the potential to increase modularity with respect to linguistic description and grammar engineering , and to facilitate concurrent processing and the treatment of ambiguity .

semi-automatic construction of cross-period thesaurus
cross-period ( diachronic ) thesaurus construction aims to enable potential users to search for modern terms and obtain semantically related terms from earlier periods in history . this is a complex task not previously addressed computationally . in this paper we introduce a semi-automatic iterative query expansion ( qe ) scheme for supporting cross-period thesaurus construction . we demonstrate the empirical benefit of our scheme for a jewish crossperiod thesaurus and evaluate its impact on recall and on the effectiveness of lexicographer manual effort .

evaluating cetempblico , a free resource for portuguese
in this paper we present a thorough evaluation of a corpus resource for portuguese , cetempblico , a 180million word newspaper corpus free for r & d in portuguese processing . we provide information that should be useful to those using the resource , and to considerable improvement for later versions . in addition , we think that the procedures presented can be of interest for the larger nlp community , since corpus evaluation and description is unfortunately not a common exercise .

what good are nominalkomposita for noun compounds : of nominal compositions using linguistic restrictors patrick ziering lonneke van der plas
finding a definition of compoundhood that is cross-lingually valid is a non-trivial task as shown by linguistic literature . we present an iterative method for defining and extracting english noun compounds in a multilingual setting . we show how linguistic criteria can be used to extract compounds automatically and vice versa how the results of this extraction can shed new lights on linguistic theories about compounding . the extracted compound nouns and their multilingual contexts are a rich source that serves several purposes . in an additional case study we show how the database serves to predict the internal structure of tripartite noun compounds using spelling variations across languages , which leads to a precision of over 91 % .

a semantic approach to ie pattern induction
this paper presents a novel algorithm for the acquisition of information extraction patterns . the approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant . patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity . evaluation shows this algorithm performs well when compared with a previously reported document-centric approach .

hybrid models for semantic classification of chinese unknown words
this paper addresses the problem of classifying chinese unknown words into fine-grained semantic categories defined in a chinese thesaurus . we describe three novel knowledge-based models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways . we then combine two of the knowledge-based models with a corpus-based model which classifies unknown words using contextual information . experiments show that the knowledge-based models outperform previous methods on the same task , but the use of contextual information does not further improve performance .

subsegmental language detection in celtic language text
this paper describes an experiment to perform language identification on a sub-sentence basis . the typical case of language identification is to detect the language of documents or sentences . however , it may be the case that a single sentence or segment contains more than one language . this is especially the case in texts where code switching occurs .

name translation in statistical machine translation learning when to transliterate
we present a method to transliterate names in the framework of end-to-end statistical machine translation . the system is trained to learn when to transliterate . for arabic to english mt , we developed and trained a transliterator on a bitext of 7 million sentences and googles english terabyte ngrams and achieved better name translation accuracy than 3 out of 4 professional translators . the paper also includes a discussion of challenges in name translation evaluation .

a cross-task flexible transition model for arabic tokenization , affix
this paper describes cross-task flexible transition models ( ctf-tms ) and demonstrates their effectiveness for arabic natural language processing ( nlp ) . nlp pipelines often suffer from error propagation , as errors committed in lower-level tasks cascade through the remainder of the processing pipeline . by allowing a flexible order of operations across and within multiple nlp tasks , a ctf-tm can mitigate both cross-task and within-task error propagation . our arabic ctf-tm models tokenization , affix detection , affix labeling , partof-speech tagging , and dependency parsing , achieving state-of-the-art results . we present the details of our general framework , our arabic ctf-tm , and the setup and results of our experiments .

sw-ag : local context matching for english lexical substitution
we present two systems that pick the ten most appropriate substitutes for a marked word in a test sentence . the first system scores candidates based on how frequently their local contexts match that of the marked word . the second system , an enhancement to the first , incorporates cosine similarity using unigram features . the core of both systems bypasses intermediate sense selection . our results show that a knowledge-light , direct method for scoring potential replacements is viable .

multilingual single-document summarization with muse
multilingual sentence extractor ( muse ) is aimed at multilingual single-document summarization . muse implements a supervised language-independent summarization approach based on optimization of multiple sentence ranking methods using a genetic algorithm . the main advantage of muse is its language-independency it is using statistical sentence features , which can be calculated for sentences in any language . in our previous work , the performance of muse was found to be significantly better than the best known state-of-the-art extractive summarization approaches and tools in three different languages : english , hebrew , and arabic . moreover , our experimental results in the cross-lingual domain suggest that muse does not need to be retrained on a summarization corpus in each new language , and the same weighting model can be used across several languages ( last and litvak , 2012 ) . muse participated in the multiling 2013 single document summarization task on three languages : english , hebrew and arabic . due to a very limited time that was given to the participants to run their systems on the multiling 2013 data , the results submitted to evaluation were obtained by summarizing the documents using models pre-trained on different corpora . as such , no training has been performed on the multiling 2013 corpus .

simplifica : a tool for authoring simplified texts in brazilian portuguese guided by readability assessments caroline gasperin and sandra maria alusio
simplifica is an authoring tool for producing simplified texts in portuguese . it provides functionalities for lexical and syntactic simplification and for readability assessment . this tool is the first of its kind for portuguese ; it brings innovative aspects for simplification tools in general , since the authoring process is guided by readability assessment based on the levels of literacy of the brazilian population .

a transition-based dependency parser using a dynamic parsing strategy
we present a novel transition-based , greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies . the new strategy allows the parser to postpone difficult decisions until the relevant information becomes available . the novel parser has a 12 % error reduction in unlabeled attachment score over an arc-eager parser , with a slow-down factor of 2.8 .

enhanced search with wildcards and morphological inflections in the google books ngram viewer
we present a new version of the google books ngram viewer , which plots the frequency of words and phrases over the last five centuries ; its data encompasses 6 % of the worlds published books . the new viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization . these additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .

levels of certainty in knowledge-intensive corpora : an initial annotation study
in this initial annotation study , we suggest an appropriate approach for determining the level of certainty in text , including classification into multiple levels of certainty , types of statement and indicators of amplified certainty . a primary evaluation , based on pairwise inter-annotator agreement ( iaa ) using f 1 -score , is performed on a small corpus comprising documents from the world bank . while iaa results are low , the analysis will allow further refinement of the created guidelines .

bilingual lexicon extraction from comparable corpora using
many existing methods for bilingual lexicon learning from comparable corpora are based on similarity of context vectors . these methods suffer from noisy vectors that greatly affect their accuracy . we introduce a method for filtering this noise allowing highly accurate learning of bilingual lexicons . our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words . we provide a method for identifying such terms . our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary . in addition , we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary .

grammar error detection with best approximated parse
in this paper , we propose that grammar error detection be disambiguated in generating the connected parse ( s ) of optimal merit for the full input utterance , in overcoming the cheapest error . the detected error ( s ) are described as violated grammatical constraints in a framework for modeltheoretic syntax ( mts ) . we present a parsing algorithm for mts , which only relies on a grammar of well-formedness , in that the process does not require any extragrammatical resources , additional rules for constraint relaxation or error handling , or any recovery process .

an all-subtrees approach to unsupervised parsing
we investigate generalizations of the allsubtrees `` dop '' approach to unsupervised parsing . unsupervised dop models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees . we will test both a relative frequency estimator for unsupervised dop and a maximum likelihood estimator which is known to be statistically consistent . we report state-ofthe-art results on english ( wsj ) , german ( negra ) and chinese ( ctb ) data . to the best of our knowledge this is the first paper which tests a maximum likelihood estimator for dop on the wall street journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank pcfg ) .

advanced dynamic programming in
dynamic programming ( dp ) is an important class of algorithms widely used in many areas of speech and language processing . recently there have been a series of work trying to formalize many instances of dp algorithms under algebraic and graph-theoretic frameworks . this tutorial surveys two such frameworks , namely semirings and directed hypergraphs , and draws connections between them . we formalize two particular types of dp algorithms under each of these frameworks : the viterbi-style topological algorithms and the dijkstra-style best-first algorithms . wherever relevant , we also discuss typical applications of these algorithms in natural language processing .

revisiting context-based projection methods for term-translation spotting in comparable corpora rali diro
context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g . ( fung , 1998 ; rapp , 1999 ) . surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach . the present study aims at doing just this . as a testcase , we address the task of translating terms of the medical domain by exploiting pages mined from wikipedia . one interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice .

quantitative modeling of the neural representation of adjective-noun phrases to account for fmri activation
recent advances in functional magnetic resonance imaging ( fmri ) offer a significant new approach to studying semantic representations in humans by making it possible to directly observe brain activity while people comprehend words and sentences . in this study , we investigate how humans comprehend adjective-noun phrases ( e.g . strong dog ) while their neural activity is recorded . classification analysis shows that the distributed pattern of neural activity contains sufficient signal to decode differences among phrases . furthermore , vector-based semantic models can explain a significant portion of systematic variance in the observed neural activity . multiplicative composition models of the two-word phrase outperform additive models , consistent with the assumption that people use adjectives to modify the meaning of the noun , rather than conjoining the meaning of the adjective and noun .

a latent variable model for
this paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to wikipedia . our novel approach models the relationship between disambiguation and aspects of cohesion using markov logic networks with latent variables . considering cohesive aspects consistently improves the disambiguation results on various commonly used data sets .

cedit semantic networks manual annotation tool
we present a demonstration of an annotation tool designed to annotate texts into a semantic network formalism called multinet . the tool is based on a java swing gui and allows the annotators to edit nodes and relations in the network , as well as links between the nodes in the network and the nodes from the previous layer of annotation . the data processed by the tool in this presentation are from the english version of the wall street journal .

finding good enough : a task-based evaluation of query biased summarization for cross language information retrieval
in this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval ( clir ) using relevance prediction . we describe our 13 summarization methods each from one of four summarization strategies . we show how well our methods perform using farsi text from the clef 2008 shared-task , which we translated to english automtatically . we report precision/recall/f1 , accuracy and time-on-task . we found that different summarization methods perform optimally for different evaluation metrics , but overall query biased word clouds are the best summarization strategy . in our analysis , we demonstrate that using the rouge metric on our sentence-based summaries can not make the same kinds of distinctions as our evaluation framework does . finally , we present our recommendations for creating muchneeded evaluation standards and datasets .

unsupervised information extraction approach using graph mutual hany hassan ahmed hassan ossama emam
information extraction ( ie ) is the task of extracting knowledge from unstructured text . we present a novel unsupervised approach for information extraction based on graph mutual reinforcement . the proposed approach does not require any seed patterns or examples . instead , it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized extraction patterns . the proposed approach has been used to acquire extraction patterns for the ace ( automatic content extraction ) relation detection and characterization ( rdc ) task . ace rdc is considered a hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data . the proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data .

models for inuktitut-english word alignment
this paper presents a set of techniques for bitext word alignment , optimized for a language pair with the characteristics of inuktitut-english . the resulting systems exploit cross-lingual affinities at the sublexical level of syllables and substrings , as well as regular patterns of transliteration and the tendency towards monotonicity of alignment . our most successful systems were based on classifier combination , and we found different combination methods performed best under the target evaluation metrics of f-measure and alignment error rate .

integrating linguistic knowledge in passage retrieval for question answering
in this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system . we use annotation produced by a deep syntactic dependency parser for dutch , alpino , to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index . similar annotation is produced for natural language questions to be answered by the system . from this we extract query terms to be sent to the enriched retrieval index . we use a genetic algorithm to optimize the selection of features and syntactic units to be included in a query . this algorithm is also used to optimize further parameters such as keyword weights . the system is trained on questions from the competition on dutch question answering within the cross-language evaluation forum ( clef ) . we could show an improvement of about 15 % in mean total reciprocal rank compared to traditional information retrieval using plain text keywords ( including stemming and stop word removal ) .

modeling of long distance context dependency
ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks . however , they can only capture the short distance context dependency within an n-words window where currently the largest practical n for a natural language is three while much of the context dependency in a natural language occurs beyond a three words window . in order to incorporate this kind of long distance context dependency in the ngram model of our mandarin speech recognition system , this paper proposes a novel mi-ngram modeling approach . this new mi-ngram model consists of two components : a normal ngram model and a novel mi model . the ngram model captures the short distance context dependency within an n-words window while the mi model captures the context dependency between the word pairs over a long distance by using the concept of mutual information . that is , the mi-ngram model incorporates the word occurrences beyond the scope of the normal ngram model . it is found that mingram modeling has much better performance than the normal word ngram modeling . experimentation shows that about 20 % of errors can be corrected by using a mi-trigram model compared with the pure word trigram model .

does korean defeat phonotactic word segmentation
computational models of infant word segmentation have not been tested on a wide range of languages . this paper applies a phonotactic segmentation model to korean . in contrast to the undersegmentation pattern previously found in english and russian , the model exhibited more oversegmentation errors and more errors overall . despite the high error rate , analysis suggested that lexical acquisition might not be problematic , provided that infants attend only to frequently segmented items .

trained named entity recognition using distributional clusters
this work applies boosted wrapper induction ( bwi ) , a machine learning algorithm for information extraction from semi-structured documents , to the problem of named entity recognition . the default feature set of bwi is augmented with features based on distributional term clusters induced from a large unlabeled text corpus . using no traditional linguistic resources , such as syntactic tags or specialpurpose gazetteers , this approach yields results near the state of the art in the muc 6 named entity domain . supervised learning using features derived through unsupervised corpus analysis may be regarded as an alternative to bootstrapping methods .

the effects of human variation in duc summarization evaluation
there is a long history of research in automatic text summarization systems by both the text retrieval and the natural language processing communities , but evaluation of such systems output has always presented problems . one critical problem remains how to handle the unavoidable variability in human judgments at the core of all the evaluations . sponsored by the darpa tides project , nist launched a new text summarization evaluation effort , called duc , in 2001 with follow-on workshops in 2002 and 2003. human judgments provided the foundation for all three evaluations and this paper examines how the variation in those judgments does and does not affect the results and their interpretation .

automatic story segmentation using a bayesian decision framework for statistical models of lexical chain features wai-kit
this paper presents a bayesian decision framework that performs automatic story segmentation based on statistical modeling of one or more lexical chain features . automatic story segmentation aims to locate the instances in time where a story ends and another begins . a lexical chain is formed by linking coherent lexical items chronologically . a story boundary is often associated with a significant number of lexical chains ending before it , starting after it , as well as a low count of chains continuing through it . we devise a bayesian framework to capture such behavior , using the lexical chain features of start , continuation and end . in the scoring criteria , lexical chain starts/ends are modeled statistically with the weibull and uniform distributions at story boundaries and non-boundaries respectively . the normal distribution is used for lexical chain continuations . full combination of all lexical chain features gave the best performance ( f1=0.6356 ) . we found that modeling chain continuations contributes significantly towards segmentation performance .

multilingual dependency parsing using bayes point machines
we develop dependency parsers for arabic , english , chinese , and czech using bayes point machines , a training algorithm which is as easy to implement as the perceptron yet competitive with large margin methods . we achieve results comparable to state-of-the-art in english and czech , and report the first directed dependency parsing accuracies for arabic and chinese . given the multilingual nature of our experiments , we discuss some issues regarding the comparison of dependency parsers for different languages .

robust vpe detection using automatically parsed text leif arda nielsen
this paper describes a verb phrase ellipsis ( vpe ) detection system , built for robustness , accuracy and domain independence . the system is corpus-based , and uses machine learning techniques on free text that has been automatically parsed . tested on a mixed corpus comprising a range of genres , the system achieves a 70 % f1-score . this system is designed as the first stage of a complete vpe resolution system that is input free text , detects vpes , and proceeds to find the antecedents and resolve them .

shallow parsing with conditional random fields
conditional random fields for sequence labeling offer advantages over both generative models like hmms and classifiers applied at each sequence position . among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods . we show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the conll task , and better than any reported single model . improved training methods based on modern optimization algorithms were critical in achieving these results . we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models .

influence of parser choice on dependency-based mt
accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation . this paper deals with the influence of various dependency parsing approaches ( and also different training data size ) on the overall performance of an english-to-czech dependency-based statistical translation system implemented in the treex framework . we also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of bleu .

two tools for creating and visualizing sub-sentential alignments of parallel text
we present two web-based , interactive tools for creating and visualizing sub-sentential alignments of parallel text . yawat is a tool to support distributed , manual word- and phrase-alignment of parallel text through an intuitive , web-based interface . kwipc is an interface for displaying words or bilingual word pairs in parallel , word-aligned context . a key element of the tools presented here is the interactive visualization : alignment information is shown only for one pair of aligned words or phrases at a time . this allows users to explore the alignment space interactively without being overwhelmed by the amount of information available .

mining medline : postulating a beneficial role for curcumin longa in & information science national library of medicine aditya kumar sehgal
text mining tools are designed to assist users with the important step of hypothesis generation . in this research we apply an open discovery process to the problem of identifying novel disease or problem contexts in which a substance may have therapeutic potential . we illustrate this discovery process by executing our open discovery algorithm with turmeric ( curcumin longa ) as the substance being investigated . the top ranking entry suggested by the algorithm is retinal diseases . further analysis of the literature yields evidence supporting the suggested connection between curcumin and retinal diseases . in particular , curcumin influences the activation of genes such as cox-2 , tnf-alpha , jnk , erk and nf-kappab . these genes are in turn involved in retinal diseases such as diabetic retinopathies , ocular inflammation and glaucoma . moreover , the evidence suggests that curcumin may have a beneficial and therapeutic role in the context of these diseases .

hybrid approach to user intention modeling for dialog simulation
this paper proposes a novel user intention simulation method which is a data-driven approach but able to integrate diverse user discourse knowledge together to simulate various type of users . in markov logic framework , logistic regression based data-driven user intention modeling is introduced , and human dialog knowledge are designed into two layers such as domain and discourse knowledge , then it is integrated with the data-driven model in generation time . cooperative , corrective and selfdirecting discourse knowledge are designed and integrated to mimic such type of users . experiments were carried out to investigate the patterns of simulated users , and it turned out that our approach was successful to generate user intention patterns which are not only unseen in the training corpus and but also personalized in the designed direction .

resolving speculation : maxent cue classification and dependency-based scope rules
this paper describes a hybrid , two-level approach for resolving hedge cues , the problem of the conll-2010 shared task . first , a maximum entropy classifier is applied to identify cue words , using both syntactic- and surface-oriented features . second , a set of manually crafted rules , operating on dependency representations and the output of the classifier , is applied to resolve the scope of the hedge cues within the sentence .

a review selection approach for accurate feature rating estimation
in this paper , we propose a review selection approach towards accurate estimation of feature ratings for services on participatory websites where users write textual reviews for these services . our approach selects reviews that comprehensively talk about a feature of a service by using information distance of the reviews on the feature . the rating estimation of the feature for these selected reviews using machine learning techniques provides more accurate results than that for other reviews . the average of these estimated feature ratings also better represents an accurate overall rating for the feature of the service , which provides useful feedback for other users to choose their satisfactory services .

statistical transliteration for cross langauge information retrieval using
in this paper we present a statistical transliteration technique that is language independent . this technique uses hidden markov model ( hmm ) alignment and conditional random fields ( crf ) , a discriminative model . hmm alignment maximizes the probability of the observed ( source , target ) word pairs using the expectation maximization algorithm and then the character level alignments ( n-gram ) are set to maximum posterior predictions of the model . crf has efficient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solutions . we apply this technique for hindi-english transliteration task . the results show that our technique perfoms better than the existing transliteration system which uses hmm alignment and conditional probabilities derived from counting the alignments .

efficient inference through cascades of weighted tree transducers
weighted tree transducers have been proposed as useful formal models for representing syntactic natural language processing applications , but there has been little description of inference algorithms for these automata beyond formal foundations . we give a detailed description of algorithms for application of cascades of weighted tree transducers to weighted tree acceptors , connecting formal theory with actual practice . additionally , we present novel on-the-fly variants of these algorithms , and compare their performance on a syntax machine translation cascade based on ( yamada and knight , 2001 ) .

identifying parallel documents from a large bilingual collection of texts :
while several recent works on dealing with large bilingual collections of texts , e.g . ( smith et al , 2010 ) , seek for extracting parallel sentences from comparable corpora , we present paradocs , a system designed to recognize pairs of parallel documents in a ( large ) bilingual collection of texts . we show that this system outperforms a fair baseline ( enright and kondrak , 2007 ) in a number of controlled tasks . we applied it on the frenchenglish cross-language linked article pairs of wikipedia in order see whether parallel articles in this resource are available , and if our system is able to locate them . according to some manual evaluation we conducted , a fourth of the article pairs in wikipedia are indeed in translation relation , and paradocs identifies parallel or noisy parallel article pairs with a precision of 80 % .

machine translation and its philosophical accounts
this paper attempts to explore the interrelation between philosophical accounts of language and respective technological developments in the field of human language technologies . in doing so , it focuses on the interaction between analytical philosophy and machine translation development , trying to draw the emerging methodological analogies .

applying spectral clustering for chinese word sense induction ministry of education , china
sense induction is the process of identifying the word sense given its context , often treated as a clustering task . this paper explores the use of spectral cluster method which incorporates word features and ngram features to determine which cluster the word belongs to , each cluster represents one sense in the given document set .

combining outputs of multiple japanese named entity chunkers and computer sciences ,
in this paper , we propose a method for learning a classifier which combines outputs of more than one japanese named entity extractors . the proposed combination method belongs to the family of stacked generalizers , which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage . individual models to be combined are based on maximum entropy models , one of which always considers surrounding contexts of a fixed length , while the other considers those of variable lengths according to the number of constituent morphemes of named entities . as an algorithm for learning the second stage classifier , we employ a decision list learning method . experimental evaluation shows that the proposed method achieves improvement over the best known results with japanese named entity extractors based on maximum entropy models .

notes on the evaluation of dependency parsers obtained through cross-lingual projection
in this paper we address methodological issues in the evaluation of a projectionbased framework for dependency parsing in which annotations for a source language are transfered to a target language using word alignments in a parallel corpus . the projected trees then constitute the training data for a data-driven parser in the target language . we discuss two problems that arise in the evaluation of such cross-lingual approaches . first , the annotation scheme underlying the source language annotations and hence the projected target annotations and predictions of the parser derived from them is likely to differ from previously existing gold standard test sets devised specifically for the target language . second , the standard procedure of cross-validation can not be performed in the absence of parallel gold standard annotations , so an alternative method has to be used to assess the generalization capabilities of the projected parsers .

think positive : towards twitter sentiment analysis from scratch
in this paper we describe a deep convolutional neural network ( dnn ) approach to perform two sentiment detection tasks : message polarity classification and contextual polarity disambiguation . we apply the proposed approach for the semeval2014 task 9 : sentiment analysis in twitter . despite not using any handcrafted feature or sentiment lexicons , our system achieves very competitive results for twitter data .

automatic generation of translation dictionaries using intermediary
we describe a method which uses one or more intermediary languages in order to automatically generate translation dictionaries . such a method could potentially be used to efficiently create translation dictionaries for language groups which have as yet had little interaction . for any given word in the source language , our method involves first translating into the intermediary language ( s ) , then into the target language , back into the intermediary language ( s ) and finally back into the source language . the relationship between a word and the number of possible translations in another language is most often 1-to-many , and so at each stage , the number of possible translations grows exponentially . if we arrive back at the same starting point i.e . the same word in the source language , then we hypothesise that the meanings of the words in the chain have not diverged significantly . hence we backtrack through the link structure to the target language word and accept this as a suitable translation . we have tested our method by using english as an intermediary language to automatically generate a spanish-to-germandictionary , and the results are encouraging .

object search : supporting structured queries in web search engines
as the web evolves , increasing quantities of structured information is embedded in web pages in disparate formats . for example , a digital cameras description may include its price and megapixels whereas a professors description may include her name , university , and research interests . both types of pages may include additional ambiguous information . general search engines ( gses ) do not support queries over these types of data because they ignore the web document semantics . conversely , describing requisite semantics through structured queries into databases populated by information extraction ( ie ) techniques are expensive and not easily adaptable to new domains . this paper describes a methodology for rapidly developing search engines capable of answering structured queries over unstructured corpora by utilizing machine learning to avoid explicit ie . we empirically show that with minimum additional human effort , our system outperforms a gse with respect to structured queries with clear object semantics .

exploiting structured ontology to organize scattered online opinions
we study the problem of integrating scattered online opinions . for this purpose , we propose to exploit structured ontology to obtain well-formed relevant aspects to a topic and use them to organize scattered opinions to generate a structured summary . particularly , we focus on two main challenges in implementing this idea , ( 1 ) how to select the most useful aspects from a large number of aspects in the ontology and ( 2 ) how to order the selected aspects to optimize the readability of the structured summary . we propose and explore several methods for solving these challenges . experimental results on two different data sets ( us presidents and digital cameras ) show that the proposed methods are effective for selecting aspects that can represent the major opinions and for generating coherent ordering of aspects .

understanding complex natural language explanations in tutorial
we describe the why2-atlas intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue . we focus on the issue of analyzing and responding to multisentential explanations . we explore an approach that combines a statistical classifier , multiple semantic parsers and a formal reasoner for achieving a deeper understanding of these explanations in order to provide appropriate feedback on them .

word sense disambiguation with multilingual features
this paper explores the role played by a multilingual feature representation for the task of word sense disambiguation . we translate the context of an ambiguous word in multiple languages , and show through experiments on standard datasets that by using a multilingual vector space we can obtain error rate reductions of up to 25 % , as compared to a monolingual classifier .

chinese word segmentation in ict-nlp
chinese word segmentation is always much accounted of in ict-nlp . in this bakeoff , two different systems in ictnlp participated . the one is system_ # 1 evaluated in three tracks -- pk-close , msr-close and msr-open , and system_ # 2 pk-open . through this bakeoff , the development of chinese segmentation is learned and the problems are found in our systems .

ucd-fc : deducing semantic relations using wordnet senses that occur frequently in a database of noun-noun compounds
this paper describes a system for classifying semantic relations among nominals , as in semeval task 4. this system uses a corpus of 2,500 compounds annotated with wordnet senses and covering 139 different semantic relations . given a set of nominal pairs for training , as provided in the semeval task 4 training data , this system constructs for each training pair a set of features made up of relations and wordnet sense pairs which occurred with those nominals in the corpus . a naive bayes learning algorithm learns associations between these features and relation membership categories . the identification of relations among nominals in test items takes place on the basis of these associations .

an integrated multi-document summarization approach based on word hierarchical representation
this paper introduces a novel hierarchical summarization approach for automatic multidocument summarization . by creating a hierarchical representation of the words in the input document set , the proposed approach is able to incorporate various objectives of multidocument summarization through an integrated framework . the evaluation is conducted on the duc 2007 data set .

bayesian inference for pcfgs via markov chain monte carlo
this paper presents two markov chain monte carlo ( mcmc ) algorithms for bayesian inference of probabilistic context free grammars ( pcfgs ) from terminal strings , providing an alternative to maximum-likelihood estimation using the inside-outside algorithm . we illustrate these methods by estimating a sparse grammar describing the morphology of the bantu language sesotho , demonstrating that with suitable priors bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the inside-outside algorithm only produce a trivial grammar .

a connectionist approach
this study presents a novel computational approach to the analysis of unaccusative/unergative distinction in turkish by employing feed-forward artificial neural networks with a backpropagation algorithm . the findings of the study reveal correspondences between semantic notions and syntactic manifestations of unaccusative/unergative distinction in this language , thus presenting a computational analysis of the distinction at the syntax/semantics interface . the approach is applicable to other languages , particularly the ones which lack an explicit diagnostic such as auxiliary selection but has a number of diagnostics instead .

text chunking by combining hand-crafted rules and memory-based seong-bae park byoung-tak
this paper proposes a hybrid of handcrafted rules and a machine learning method for chunking korean . in the partially free word-order languages such as korean and japanese , a small number of rules dominate the performance due to their well-developed postpositions and endings . thus , the proposed method is primarily based on the rules , and then the residual errors are corrected by adopting a memory-based machine learning method . since the memory-based learning is an efficient method to handle exceptions in natural language processing , it is good at checking whether the estimates are exceptional cases of the rules and revising them . an evaluation of the method yields the improvement in f-score over the rules or various machine learning methods alone .

a composite kernel to extract relations between entities with min
this paper proposes a novel composite kernel for relation extraction . the composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples . the motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction . our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features . evaluation on the ace corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .

yifan he jinhua du andy way josef van genabith
we describe dcus lfg dependencybased metric submitted to the shared evaluation task of wmt-metricsmatr 2010. the metric is built on the lfg f-structurebased approach presented in ( owczarzak et al , 2007 ) . we explore the following improvements on the original metric : 1 ) we replace the in-house lfg parser with an open source dependency parser that directly parses strings into lfg dependencies ; 2 ) we add a stemming module and unigram paraphrases to strengthen the aligner ; 3 ) we introduce a chunk penalty following the practice of meteor to reward continuous matches ; and 4 ) we introduce and tune parameters to maximize the correlation with human judgement . experiments show that these enhancements improve the dependency-based metrics correlation with human judgement .

an overview of the craft concept annotation guidelines anschutz medical campus
we present our concept-annotation guidelines for an large multi-institutional effort to create a gold-standard manually annotated corpus of full-text biomedical journal articles . we are semantically annotating these documents with the full term sets of eight large biomedical ontologies and controlled terminologies ranging from approximately 1,000 to millions of terms , and , using these guidelines , we have been able to perform this extremely challenging task with a high degree of interannotator agreement . the guidelines have been designed to be able to be used with any terminology employed to semantically annotate concept mentions in text and are available for external use .

total rank distance and scaled total rank distance : two alternative metrics in computational linguistics
in this paper we propose two metrics to be used in various fields of computational linguistics area . our construction is based on the supposition that in most of the natural languages the most important information is carried by the first part of the unit . we introduce total rank distance and scaled total rank distance , we prove that they are metrics and investigate their max and expected values . finally , a short application is presented : we investigate the similarity of romance languages by computing the scaled total rank distance between the digram rankings of each language .

a comparative study on reordering constraints in statistical machine chair of computer science vi
in statistical machine translation , the generation of a translation hypothesis is computationally expensive . if arbitrary wordreorderings are permitted , the search problem is np-hard . on the other hand , if we restrict the possible word-reorderings in an appropriate way , we obtain a polynomial-time search algorithm . in this paper , we compare two different reordering constraints , namely the itg constraints and the ibm constraints . this comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints . we show a connection between the itg constraints and the since 1870 known schroder numbers . we evaluate these constraints on two tasks : the verbmobil task and the canadian hansards task . the evaluation consists of two parts : first , we check how many of the viterbi alignments of the training corpus satisfy each of these constraints . second , we restrict the search to each of these constraints and compare the resulting translation hypotheses . the experiments will show that the baseline itg constraints are not sufficient on the canadian hansards task .

simcompass : using deep learning word embeddings to assess cross-level similarity
this article presents our teams participating system at semeval-2014 task 3. using a meta-learning framework , we experiment with traditional knowledgebased metrics , as well as novel corpusbased measures based on deep learning paradigms , paired with varying degrees of context expansion . the framework enabled us to reach the highest overall performance among all competing systems .

bucking the trend : large-scale cost-focused active learning for statistical machine translation human language technology
we explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources . the main challenge is how to buck the trend of diminishing returns that is commonly encountered . we present an active learning-style data solicitation algorithm to meet this challenge . we test it , gathering annotations via amazon mechanical turk , and find that we get an order of magnitude increase in performance rates of improvement .

dependency parsing with latent refinements of part-of-speech tags
in this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( pos ) tags . we perform experiments on english and german and show significant improvements for both languages . the refinement is based on generative split-merge training for hidden markov models ( hmms ) .

tailoring word alignments to syntactic machine translation
extracting tree transducer rules for syntactic mt systems can be hindered by word alignment errors that violate syntactic correspondences . we propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure , while retaining the robustness and efficiency of the hmm alignment model . our models predictions improve the yield of a tree transducer extraction system , without sacrificing alignment quality . we also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments .

text categorization from category name via lexical reference
requiring only category names as user input is a highly attractive , yet hardly explored , setting for text categorization . earlier bootstrapping results relied on similarity in lsa space , which captures rather coarse contextual similarity . we suggest improving this scheme by identifying concrete references to the category names meaning , obtaining a special variant of lexical expansion .

spell checking techniques for replacement of unknown words and data cleaning for haitian creole sms translation
we report results on translation of sms messages from haitian creole to english . we show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents . we also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora .

multi-resolution language grounding with weak supervision
language is given meaning through its correspondence with a world representation . this correspondence can be at multiple levels of granularity or resolutions . in this paper , we introduce an approach to multi-resolution language grounding in the extremely challenging domain of professional soccer commentaries . we define and optimize a factored objective function that allows us to leverage discourse structure and the compositional nature of both language and game events . we show that finer resolution grounding helps coarser resolution grounding , and vice versa . our method results in an f1 improvement of more than 48 % versus the previous state of the art for fine-resolution grounding 1 .

selecting the most highly correlated pairs within a large vocabulary
occurence patterns of words in documents can be expressed as binary vectors . when two vectors are similar , the two words corresponding to the vectors may have some implicit relationship with each other . we call these two words a correlated pair . this report describes a method for obtaining the most highly correlated pairs of a given size . in practice , the method requires computation time , and memory space , where is the number of documents or records . since this does not depend on the size of the vocabulary under analysis , it is possible to compute correlations between all the words in a corpus .

building a japanese-chinese dictionary using
bilingual dictionaries , such as japanese-english and english-chinese to build japanese-chinese dictionary . however , japanese and chinese are nearer languages than english , there should be a more direct way of doing this . since a lot of japanese words are composed of kanji , which are similar to hanzi in chinese , we attempt to build a dictionary for kanji words by simple conversion from kanji to hanzi . our survey shows that around 2/3 of the nouns and verbal nouns in japanese are kanji words , and more than 1/3 of them can be translated into chinese directly . the accuracy of conversion is 97 % . besides , we obtain translation candidates for 24 % of the japanese words using english as a pivot language with 77 % accuracy . by adding the kanji/hanzi conversion method , we increase the candidates by 9 % , to 33 % , with better quality candidates .

a two level model for context sensitive inference rules
automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words , operating at the word space level . a recent line of work , which addresses context sensitivity of rules , represented contexts in a latent topic space and computed similarity over topic vectors . we propose a novel two-level model , which computes similarities between word-level vectors that are biased by topic-level context representations . evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models . we also release a first context-sensitive inference rule set .

a character n-gram based approach for improved recall in indian language ner praneeth m shishtla
named entity recognition ( ner ) is the task of identifying and classifying all proper nouns in a document as person names , organization names , location names , date & time expressions and miscellaneous . previous work ( cucerzan and yarowsky , 1999 ) was done using the complete words as features which suffers from a low recall problem . character n-gram based approach ( klein et al , 2003 ) using generative models , was experimented on english language and it proved to be useful over the word based models . applying the same technique on indian languages , we experimented with conditional random fields ( crfs ) , a discriminative model , and evaluated our system on two indian languages telugu and hindi . the character n-gram based models showed considerable improvement over the word based models . this paper describes the features used and experiments to increase the recall of named entity recognition systems which is also language independent .

segmentation of chinese long sentences using commas
the comma is the most common form of punctuation . as such , it may have the greatest effect on the syntactic analysis of a sentence . as an isolate language , chinese sentences have fewer cues for parsing . the clues for segmentation of a long chinese sentence are even fewer . however , the average frequency of comma usage in chinese is higher than other languages . the comma plays an important role in long chinese sentence segmentation . this paper proposes a method for classifying commas in chinese sentences by their context , then segments a long sentence according to the classification results . experimental results show that accuracy for the comma classification reaches 87.1 percent , and with our segmentation model , our parsers dependency parsing accuracy improves by 9.6 percent .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

modeling letter - to - phoneme conversion as a phrase based statistical machine translation problem with minimum error rate training
letter - to - phoneme conversion plays an important role in several applications. it can be a difficult task because the mapping from letters to phonemes can be many - to - many . we present a language independent letter - to - phoneme conversion approach which is based on the popular phrase based statistical machine translation techniques . the results of our experiments clearly demonstrate that such techniques can be used effectively for letter - tophoneme conversion . our results show an overall improvement of 5.8 % over the baseline and are comparable to the state of the art. we also propose a measure to estimate the difficulty level of l2p task for a language.

enhancing machine learning with linguistics
topic modelling is a popular approach to joint clustering of documents and terms , e.g . via latent dirichlet allocation . the standard document representation in topic modelling is a bag of unigrams , ignoring both macro-level document structure and micro-level constituent structure . in this talk , i will discuss recent work on consolidating the microlevel document representation with multiword expressions , and present experimental results which demonstrate that linguistically-richer document representations enhance topic modelling . biography tim baldwin is an associate professor and deputy head of the department of computer science and software engineering , university of melbourne and a contributed research staff member of the nicta victoria research laboratories . he has previously held visiting positions at the university of washington , university of tokyo , university of saarland , and ntt communication science laboratories . his research interests cover topics including deep linguistic processing , multiword expressions , deep lexical acquisition , computer-assisted language learning , information extraction and web mining , with a particular interest in the interface between computational and theoretical linguistics . current projects include web user forum mining , information personalisation in museum contexts , biomedical text mining , online linguistic exploration , and intelligent interfaces for japanese language learners . he is president of the australasian language technology association in 2011-2012. tim completed a bsc ( cs/maths ) and ba ( linguistics/japanese ) at the university of melbourne in 1995 , and an meng ( cs ) and phd ( cs ) at the tokyo institute of technology in 1998 and 2001 , respectively . prior to commencing his current position at the university of melbourne , he was a senior research engineer at the center for the study of language and information , stanford university ( 2001-2004 ) .

an unsupervised alignment algorithm for text simplification corpus
we present a method for the sentence-level alignment of short simplified text to the original text from which they were adapted . our goal is to align a medium-sized corpus of parallel text , consisting of short news texts in spanish with their simplified counterpart . no training data is available for this task , so we have to rely on unsupervised learning . in contrast to bilingual sentence alignment , in this task we can exploit the fact that the probability of sentence correspondence can be estimated from lexical similarity between sentences . we show that the algoithm employed performs better than a baseline which approaches the problem with a tf*idf sentence similarity metric . the alignment algorithm is being used for the creation of a corpus for the study of text simplification in the spanish language .

improving statistical machine translation performance by training data selection and optimization
parallel corpus is an indispensable resource for translation model training in statistical machine translation ( smt ) . instead of collecting more and more parallel training corpora , this paper aims to improve smt performance by exploiting full potential of the existing parallel corpora . two kinds of methods are proposed : offline data optimization and online model optimization . the offline method adapts the training data by redistributing the weight of each training sentence pairs . the online method adapts the translation model by redistributing the weight of each predefined submodels . information retrieval model is used for the weighting scheme in both methods . experimental results show that without using any additional resource , both methods can improve smt performance significantly .

polynomial time joint structural inference for sentence compression
we propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives . the first algorithm is exact and requires o ( n 6 ) running time . it extends eisners cubic time parsing algorithm by using virtual dependency arcs to link deleted words . two signatures are added to each span , indicating the number of deleted words and the rightmost kept word within the span . the second algorithm is a fast approximation of the first one . it relaxes the compression ratio constraint using lagrangian relaxation , and thereby requires o ( n 4 ) running time . experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

on the robustness of syntactic and semantic features for automatic mt evaluation
linguistic metrics based on syntactic and semantic information have proven very effective for automatic mt evaluation . however , no results have been presented so far on their performance when applied to heavily ill-formed low quality translations . in order to glean some light into this issue , in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages . corroborating previous findings , we have verified that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level . however , these metrics suffer a significant decrease at the sentence level . this is in many cases attributable to a loss of recall , due to parsing errors or to a lack of parsing at all , which may be partially ameliorated by backing off to lexical similarity .

exploiting paraphrases in a question answering system
we present a question answering system for technical domains which makes an intelligent use of paraphrases to increase the likelihood of finding the answer to the users question . the system implements a simple and efficient logic representation of questions and answers that maps paraphrases to the same underlying semantic representation . further , paraphrases of technical terminology are dealt with by a separate process that detects surface variants .

probabilistic dialogue modelling
we show how bayesian networks and related probabilistic methods provide an efficient way of capturing the complex balancing of different factors that determine interpretation and generation in dialogue . as a case study , we show how a probabilistic approach can be used to model anaphora resolution in dialogue1 .

using gene expression programming to construct sentence ranking functions for text summarization
in this paper , we consider the automatic text summarization as a challenging task of machine learning . we proposed a novel summarization system architecture which employs gene expression programming technique as its learning mechanism . the preliminary experimental results have shown that our prototype system outperforms the baseline systems .

towards a formal view of corrective feedback
this paper introduces a formal view of the semantics and pragmatics of corrective feedback in dialogues between adults and children . the goal of this research is to give a formal account of language coordination in dialogue , and semantic coordination in particular . accounting for semantic coordination requires ( 1 ) a semantics , i.e . an architecture allowing for dynamic meanings and meaning updates as results of dialogue moves , and ( 2 ) a pragmatics , describing the dialogue moves involved in semantic coordination . we illustrate the general approach by applying it to some examples from the literature on corrective feedback , and provide a fairly detailed discussion of one example using ttr ( type theory with records ) to formalize concepts . ttr provides an analysis of linguistic content which is structured in order to allow modification and similarity metrics , and a framework for describing dialogue moves and resulting updates to linguistic resources .

translation system for healthcare mark
we describe a highly interactive system for bidirectional , broad-coverage spoken language communication in the healthcare area . the paper briefly reviews the system 's interactive foundations , and then goes on to discuss in greater depth issues of practical usability . we present our translation shortcuts facility , which minimizes the need for interactive verification of sentences after they have been vetted once , considerably speeds throughput while maintaining accuracy , and allows use by minimally literate patients for whom any mode of text entry might be difficult . we also discuss facilities for multimodal input , in which handwriting , touch screen , and keyboard interfaces are offered as alternatives to speech input when appropriate . in order to deal with issues related to sheer physical awkwardness , we briefly mention facilities for hands-free or eyes-free operation of the system . finally , we point toward several directions for future improvement of the system .

gene name extraction using flybase resources the mitre corporation
machine-learning based entity extraction requires a large corpus of annotated training to achieve acceptable results . however , the cost of expert annotation of relevant data , coupled with issues of inter-annotator variability , makes it expensive and time-consuming to create the necessary corpora . we report here on a simple method for the automatic creation of large quantities of imperfect training data for a biological entity ( gene or protein ) extraction system . we used resources available in the flybase model organism database ; these resources include a curated lists of genes and the articles from which the entries were drawn , together a synonym lexicon . we applied simple pattern matching to identify gene names in the associated abstracts and filtered these entities using the list of curated entries for the article . this process created a data set that could be used to train a simple hidden markov model ( hmm ) entity tagger . the results from the hmm tagger were comparable to those reported by other groups ( f-measure of 0.75 ) . this method has the advantage of being rapidly transferable to new domains that have similar existing resources .

task-based evaluation of nlg systems : control vs real-world context
currently there is little agreement about , or even discussion of , methodologies for taskbased evaluation of nlg systems . i discuss one specific issue in this area , namely the importance of control vs the importance of ecological validity ( real-world context ) , and suggest that perhaps we need to put more emphasis on ecological validity in nlg evaluations .

knownet : building a large net of knowledge from the web
this paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources . basically , the method uses a wide-coverage and accurate knowledge-based word sense disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web . knownet , the resulting knowledge-base which connects large sets of semanticallyrelated concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . in fact , knownet is several times larger than any available knowledge resource encoding relations between synsets , and the knowledge knownet contains outperform any other resource when is empirically evaluated in a common framework .

learning semantic network patterns for hypernymy
current approaches of hypernymy acquisition are mostly based on syntactic or surface representations and extract hypernymy relations between surface word forms and not word readings . in this paper we present a purely semantic approach for hypernymy extraction based on semantic networks ( sns ) . this approach employs a set of patterns sub0 ( a1 , a2 ) premise where the premise part of a pattern is given by a sn . furthermore this paper describes how the patterns can be derived by relational statistical learning following the minimum description length principle ( mdl ) . the evaluation demonstrates the usefulness of the learned patterns and also of the entire hypernymy extraction system .

htm : a topic model for hypertexts
previously topic models such as plsi ( probabilistic latent semantic indexing ) and lda ( latent dirichlet allocation ) were developed for modeling the contents of plain texts . recently , topic models for processing hypertexts such as web pages were also proposed . the proposed hypertext models are generative models giving rise to both words and hyperlinks . this paper points out that to better represent the contents of hypertexts it is more essential to assume that the hyperlinks are fixed and to define the topic model as that of generating words only . the paper then proposes a new topic model for hypertext processing , referred to as hypertext topic model ( htm ) . htm defines the distribution of words in a document ( i.e. , the content of the document ) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites . the topics are further characterized as distributions of words , as in the conventional topic models . this paper further proposes a method for learning the htm model . experimental results show that htm outperforms the baselines on topic discovery and document classification in three datasets .

automated collocation suggestion for japanese second language
this study addresses issues of japanese language learning concerning word combinations ( collocations ) . japanese learners may be able to construct grammatically correct sentences , however , these may sound unnatural . in this work , we analyze correct word combinations using different collocation measures and word similarity methods . while other methods use well-formed text , our approach makes use of a large japanese language learner corpus for generating collocation candidates , in order to build a system that is more sensitive to constructions that are difficult for learners . our results show that we get better results compared to other methods that use only wellformed text .

a memorybased learning approach to event extraction in biomedical texts
in this paper we describe the memory-based machine learning system that we submitted to the bionlp shared task on event extraction . we modeled the event extraction task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding . the results obtained by our system ( 30.58 f-score in task 1 and 29.27 in task 2 ) suggest that the approach and the system need further adaptation to the complexity involved in extracting biomedical events .

automatic collection of related terms from the web
this paper proposes a method of collecting a dozen terms that are closely related to a given seed term . the proposed method consists of three steps . the first step , compiling corpus step , collects texts that contain the given seed term by using search engines . the second step , automatic term recognition , extracts important terms from the corpus by using nakagawas method . these extracted terms become the candidates for the final step . the final step , filtering step , removes inappropriate terms from the candidates based on search engine hits . an evaluation result shows that the precision of the method is 85 % .

exploring topic continuation follow-up questions using machine learning
some of the follow-up questions ( fu q ) that an interactive question answering ( iqa ) system receives are not topic shifts , but rather continuations of the previous topic . in this paper , we propose an empirical framework to explore such questions , with two related goals in mind : ( 1 ) modeling the different relations that hold between the fu qs answer and either the fu q or the preceding dialogue , and ( 2 ) showing how this model can be used to identify the correct answer among several answer candidates . for both cases , we use logistic regression models that we learn from real iqa data collected through a live system . we show that by adding dialogue context features and features based on sequences of domain-specific actions that represent the questions and answers , we obtain important additional predictors for the model , and improve the accuracy with which our system finds correct answers .

high-performance tagging on medical texts udo hahn joachim wermter
we ran both brills rule-based tagger and tnt , a statistical tagger , with a default german newspaper-language model on a medical text corpus . supplied with limited lexicon resources , tnt outperforms the brill tagger with state-of-the-art performance figures ( close to 97 % accuracy ) . we then trained tnt on a large annotated medical text corpus , with a slightly extended tagset that captures certain medical language particularities , and achieved 98 % tagging accuracy . hence , statistical off-the-shelf pos taggers can not only be immediately reused for medical nlp , but they also when trained on medical corpora achieve a higher performance level than for the newspaper genre .

edinburghs machine translation systems for european language pairs
we validated various novel and recently proposed methods for statistical machine translation on 10 language pairs , using large data resources . we saw gains from optimizing parameters , training with sparse features , the operation sequence model , and domain adaptation techniques . we also report on utilizing a huge language model trained on 126 billion tokens .

identifying important features for graph retrieval
infographics , such as bar charts and line graphs , occur often in popular media and are a rich knowledge source that should be accessible to users . unfortunately , information retrieval research has focused on the retrieval of text documents and images , with almost no attention specifically directed toward the retrieval of information graphics . our work is the first to directly tackle the retrieval of infographics and to design a system that takes into account their unique characteristics . learning-to-rank algorithms are applied on a large set of features to develop several models for infographics retrieval . evaluation of the models shows that features pertaining to the structure and the content of graphics should be taken into account when retrieving graphics and that doing so results in a model with better performance than a baseline model that relies on matching query words with words in the graphic .

surfaces and depths in text understanding : the case of newspaper commentary applied computational linguistics
using a specific example of a newspaper commentary , the paper explores the relationship between surface-oriented and deep analysis for purposes such as text summarization . the discussion is followed by a description of our ongoing work on automatic commentary understanding and the current state of the implementation .

generic sentence fusion is an ill-defined summarization task
we report on a series of human evaluations of the task of sentence fusion . in this task , a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two . thus , this is a highly constrained summarization task . our investigations show that even at this restricted level , there is no measurable agreement between humans regarding what information should be considered important . we further investigate the ability of separate evaluators to assess summaries , and find similarly disturbing lack of agreement .

retrieving bilingual verbnoun collocations by integrating cross-language category hierarchies fumiyo fukumoto yoshimi suzuki kazuyuki yamashita
this paper presents a method of retrieving bilingual collocations of a verb and its objective noun from cross-lingual documents with similar contents . relevant documents are obtained by integrating crosslanguage hierarchies . the results showed a 15.1 % improvement over the baseline nonhierarchy model , and a 6.0 % improvement over use of relevant documents retrieved from a single hierarchy . moreover , we found that some of the retrieved collocations were domain-specific .

adaptive models based on words in recommender systems
recommendation systems ( rs ) take advantage of products and users information in order to propose items to consumers . collaborative , content-based and a few hybrid rs have been developed in the past . in contrast , we propose a new domain-independent semantic rs . by providing textually well-argued recommendations , we aim to give more responsibility to the end user in his decision . the system includes a new similarity measure keeping up both the accuracy of rating predictions and coverage . we propose an innovative way to apply a fast adaptation scheme at a semantic level , providing recommendations and arguments in phase with the very recent past . we have performed several experiments on films data , providing textually well-argued recommendations .

understanding the thematic structure of the quran : an exploratory
in this paper , we develop a methodology for discovering the thematic structure of the quran based on a fundamental idea in data mining and related disciplines : that , with respect to some collection of texts , the lexical frequency profiles of the individual texts are a good indicator of their conceptual content , and thus provide a reliable criterion for their classification relative to one another . this idea is applied to the discovery of thematic interrelationships among the suras ( chapters ) of the quran by abstracting lexical frequency data from them and then applying hierarchical cluster analysis to that data . the results reported here indicate that the proposed methodology yields usable results in understanding the quran on the basis of its lexical semantics .

probabilistic human-computer trust handling and wolfgang minker
human-computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems . particularly incomprehensible situations in human-computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction . analogous to human-human interaction , explaining these situations can help to remedy negative effects . in this paper we present our approach of augmenting task-oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations . we have conducted a webbased study testing the effects of different goals of explanations on the components of human-computer trust . subsequently , we show how these results can be used in our probabilistic trust handling architecture to augment pre-defined task-oriented dialogs .

cognate identification and alignment using practical orthographies
we use an iterative process of multi-gram alignment between associated words in different languages in an attempt to identify cognates . to maximise the amount of data , we use practical orthographies instead of consistently coded phonetic transcriptions . first results indicate that using practical orthographies can be useful , the more so when dealing with large amounts of data .

and deployment of human language technologies
ibm research has over 200 people working on unstructured information management ( uim ) technologies with a strong focus on hlt . spread out over the globe they are engaged in activities ranging from natural language dialog to machine translation to bioinformatics to open-domain question answering . an analysis of these activities strongly suggested that improving the organizations ability to quickly discover each other 's results and rapidly combine different technologies and approaches would accelerate scientific advance . furthermore , the ability to reuse and combine results through a common architecture and a robust software framework would accelerate the transfer of research results in hlt into ibms product platforms . market analyses indicating a growing need to process unstructured information , specifically multi-lingual , natural language text , coupled with ibm researchs investment in hlt , led to the development of middleware architecture for processing unstructured information dubbed uima . at the heart of uima are powerful search capabilities and a data-driven framework for the development , composition

detection of non-native sentences using machine-translated training data spoken language systems
training statistical models to detect nonnative sentences requires a large corpus of non-native writing samples , which is often not readily available . this paper examines the extent to which machinetranslated ( mt ) sentences can substitute as training data . two tasks are examined . for the native vs non-native classication task , nonnative training data yields better performance ; for the ranking task , however , models trained with a large , publicly available set of mt data perform as well as those trained with non-native data .

towards robust context-sensitive sentence alignment for monolingual division of engineering and applied sciences
aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms , for tasks such as summarization or paraphrasing . we present here a new monolingual sentence alignment algorithm , combining a sentence-based tf*idf score , turned into a probability distribution using logistic regression , with a global alignment dynamic programming algorithm . our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems .

using grammar rule clusters for semantic relation classification
automatically-derived grammars , such as the split-and-merge model , have proven helpful in parsing ( petrov et al , 2006 ) . as such grammars are refined , latent information is recovered which may be usable for linguistic tasks besides parsing . in this paper , we present and examine a new method of semantic relation classification : using automaticallyderived grammar rule clusters as a robust knowledge source for semantic relation classification . we examine performance of this feature group on the semeval 2010 relation classification corpus , and find that it improves performance over both more coarse-grained and more fine-grained syntactic and collocational features in semantic relation classification .

joint inference for heterogeneous dependency parsing
this paper is concerned with the problem of heterogeneous dependency parsing . in this paper , we present a novel joint inference scheme , which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase . different from stacked learning methods ( nivre and mcdonald , 2008 ; martins et al , 2008 ) , which process the dependency parsing in a pipelined way ( e.g. , a second level uses the first level outputs ) , in our method , multiple dependency parsing models are coordinated to exchange consensus information . we conduct experiments on chinese dependency treebank ( cdt ) and penn chinese treebank ( ctb ) , experimental results show that joint inference can bring significant improvements to all state-of-the-art dependency parsers .

recognizing nested named entities in genia corpus
nested named entities ( nested nes ) , one containing another , are commonly seen in biomedical text , e.g. , accounting for 16.7 % of all named entities in genia corpus . while many works have been done in recognizing non-nested nes , nested nes have been largely neglected . in this work , we treat the task as a binary classification problem and solve it using support vector machines . for each token in nested nes , we use two schemes to set its class label : labeling as the outmost entity or the inner entity . our preliminary results show that while the outmost labeling tends to work better in recognizing the outmost entities , the inner labeling recognizes the inner nes better . this result should be useful for recognition of nested nes .

towards automatic building of document keywords
document keywords are associated to documents as summarized versions of the documents content . considering that the number of documents is quickly growing every day , the availability of these keywords is very important . although , usually keywords are manually written . this motivated us to work on an approach to change this manual procedure for an automatic one . this paper presents a language independent approach that extracts the most relevant multiword expressions and single words from documents and propose them to describe the core content of each document .

polyglot : distributed word representations for multilingual nlp rami al-rfou bryan perozzi
distributed word representations ( word embeddings ) have recently contributed to competitive performance in language modeling and several nlp tasks . in this work , we train word embeddings for more than 100 languages using their corresponding wikipedias . we quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages . we find their performance to be competitive with near state-of-art methods in english , danish and swedish . moreover , we investigate the semantic features captured by these embeddings through the proximity of word groupings . we will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications .

stacking dependency parsers
we explore a stacked framework for learning to predict dependency structures for natural language sentences . a typical approach in graph-based dependency parsing has been to assume a factorized model , where local features are used but a global function is optimized ( mcdonald et al , 2005b ) . recently nivre and mcdonald ( 2008 ) used the output of one dependency parser to provide features for another . we show that this is an example of stacked learning , in which a second predictor is trained to improve the performance of the first . further , we argue that this technique is a novel way of approximating rich non-local features in the second parser , without sacrificing efficient , model-optimal prediction . experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers .

uses of monolingual in-domain corpora for cross-domain adaptation with hybrid mt approaches
resource limitation is challenging for crossdomain adaption . this paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results , and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline mt system . the adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful , and the effect of using the selected pseudo bilingual corpus is significant .

identifying semantic relations and functional properties of human verb associations
this paper uses human verb associations as the basis for an investigation of verb properties , focusing on semantic verb relations and prominent nominal features . first , the lexical semantic taxonymy germanet is checked on the types of classic semantic relations in our data ; verbverb pairs not covered by germanet can help to detect missing links in the taxonomy , and provide a useful basis for defining non-classical relations . second , a statistical grammar is used for determining the conceptual roles of the noun responses . we present prominent syntaxsemantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions .

towards using structural events to assess non-native speech educational testing service ( ets )
we investigated using structural events , e.g. , clause and disfluency structure , from transcriptions of spontaneous non-native speech , to compute features for measuring speaking proficiency . using a set of transcribed audio files collected from the toefl practice test online ( tpo ) , we conducted a sophisticated annotation of structural events , including clause boundaries and types , as well as disfluencies . based on words and the annotated structural events , we extracted features related to syntactic complexity , e.g. , the mean length of clause ( mlc ) and dependent clause frequency ( depc ) , and a feature related to disfluencies , the interruption point frequency per clause ( ipc ) . among these features , the ipc shows the highest correlation with holistic scores ( r = 0.344 ) . furthermore , we increased the correlation with human scores by normalizing ipc by ( 1 ) mlc ( r = 0.386 ) , ( 2 ) depc ( r = 0.429 ) , and ( 3 ) both ( r = 0.462 ) . in this research , the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proficiency . this suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech .

memory-based learning of morphology with stochastic transducers uni-mail , boulevard du pont-darve ,
this paper discusses the supervised learning of morphology using stochastic transducers , trained using the expectationmaximization ( em ) algorithm . two approaches are presented : first , using the transducers directly to model the process , and secondly using them to define a similarity measure , related to the fisher kernel method ( jaakkola and haussler , 1998 ) , and then using a memory-based learning ( mbl ) technique . these are evaluated and compared on data sets from english , german , slovene and arabic .

a self-adaptive classifier for efficient text-stream processing
a self-adaptive classifier for efficient text-stream processing is proposed . the proposed classifier adaptively speeds up its classification while processing a given text stream for various nlp tasks . the key idea behind the classifier is to reuse results for past classification problems to solve forthcoming classification problems . a set of classification problems commonly seen in a text stream is stored to reuse the classification results , while the set size is controlled by removing the least-frequently-used or least-recently-used classification problems . experimental results with twitter streams confirmed that the proposed classifier applied to a state-of-the-art base-phrase chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7 , respectively .

joint language and translation modeling with recurrent neural networks
we present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words . the weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models . we tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically . our joint model builds on a well known recurrent neural network language model ( mikolov , 2012 ) augmented by a layer of additional inputs from the source language . we show competitive accuracy compared to the traditional channel model features . our best results improve the output of a system trained on wmt 2012 french-english data by up to 1.5 bleu , and by 1.1 bleu on average across several test sets .

computational analysis of move structures in academic abstracts
this paper introduces a method for computational analysis of move structures in abstracts of research articles . in our approach , sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions . the method involves automatically gathering a large number of abstracts from the web and building a language model of abstract moves . we also present a prototype concordancer , care , which exploits the move-tagged abstracts for digital learning . this system provides a promising approach to webbased computer-assisted academic writing .

a joint model for quotation attribution and coreference resolution
we address the problem of automatically attributing quotations to speakers , which has great relevance in text mining and media monitoring applications . while current systems report high accuracies for this task , they either work at mentionlevel ( getting credit for detecting uninformative mentions such as pronouns ) , or assume the coreferent mentions have been detected beforehand ; the inaccuracies in this preprocessing step may lead to error propagation . in this paper , we introduce a joint model for entity-level quotation attribution and coreference resolution , exploiting correlations between the two tasks . we design an evaluation metric for attribution that captures all speakers mentions . we present results showing that both tasks benefit from being treated jointly .

graph transformations in data-driven dependency parsing
transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations . in this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing . experiments on the prague dependency treebank show that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser . combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-ofthe-art accuracy for the given data set .

incremental predictive parsing with turboparser
most approaches to incremental parsing either incur a degradation of accuracy or they have to postpone decisions , yielding underspecified intermediate output . we present an incremental predictive dependency parser that is fast , accurate , and largely language independent . by extending a state-of-the-art dependency parser , connected analyses for sentence prefixes are obtained , which even predict properties and the structural embedding of upcoming words . in contrast to other approaches , accuracy for complete sentence analyses does not decrease .

classifying idiomatic and literal expressions using topic models and intensity of emotions jing peng & anna feldman
we describe an algorithm for automatic classification of idiomatic and literal expressions . our starting point is that words in a given text segment , such as a paragraph , that are highranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression . our additional hypothesis is that contexts in which idioms occur , typically , are more affective and therefore , we incorporate a simple analysis of the intensity of the emotions expressed by the contexts . we investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal ( a target phrase ) . we extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method , latent dirichlet allocation ( lda ) ( blei et al. , 2003 ) . since idiomatic expressions exhibit the property of non-compositionality , we assume that they usually present different semantics than the words used in the local topic . we treat idioms as semantic outliers , and the identification of a semantic shift as outlier detection . thus , this topic representation allows us to differentiate idioms from literals using local semantic contexts . our results are encouraging .

a temporal model of text periodicities using gaussian processes
temporal variations of text are usually ignored in nlp applications . however , text use changes with time , which can affect many applications . in this paper we model periodic distributions of words over time . focusing on hashtag frequency in twitter , we first automatically identify the periodic patterns . we use this for regression in order to forecast the volume of a hashtag based on past data . we use gaussian processes , a state-ofthe-art bayesian non-parametric model , with a novel periodic kernel . we demonstrate this in a text classification setting , assigning the tweet hashtag based on the rest of its text . this method shows significant improvements over competitive baselines .

framework for abstractive summarization using text-to-text generation
pierre-etienne genest , guy lapalme rali-diro universite de montreal p.o . box 6128 , succ . centre-ville montreal , quebec canada , h3c 3j7 { genestpe , lapalme } @ iro.umontreal.ca abstract we propose a new , ambitious framework for abstractive summarization , which aims at selecting the content of a summary not from sentences , but from an abstract representation of the source documents . this abstract representation relies on the concept of information items ( init ) , which we define as the smallest element of coherent information in a text or a sentence . our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text . we present a first attempt made at developing a system from this framework , along with evaluation results for it from tac 2010. we also present related work , both from within and outside of the automatic summarization domain .

exploiting multiple hypotheses for multilingual spoken language
in this work , we present an approach for multilingual portability of spoken language understanding systems . the goal of this approach is to avoid the effort of acquiring and labeling new corpora to learn models when changing the language . the work presented in this paper is focused on the learning of a specific translator for the task and the mechanism of transmitting the information among the modules by means of graphs . these graphs represent a set of hypotheses ( a language ) that is the input to the statistical semantic decoder that provides the meaning of the sentence . some experiments in a spanish task evaluated with input french utterances and text are presented . they show the good behavior of the system , mainly when speech input is considered .

parsing with generative models of predicate-argument structure
the model used by the ccg parser of hockenmaier and steedman ( 2002b ) would fail to capture the correct bilexical dependencies in a language with freer word order , such as dutch . this paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure , as in the model of clark et al ( 2002 ) , and defines a generative model for ccg derivations that captures these dependencies , including bounded and unbounded long-range dependencies .

paraphrase fragment extraction from monolingual comparable corpora
we present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events . the procedure consists of document pair extraction , sentence pair extraction , and fragment pair extraction . at each stage , we evaluate the intermediate results manually , and tune the later stages accordingly . with this minimally supervised approach , we achieve 62 % of accuracy on the paraphrase fragment pairs we collected and 67 % extracted from the msr corpus . the results look promising , given the minimal supervision of the approach , which can be further scaled up .

telida : a package for manipulation and visualization of timed linguistic data titus
we present a toolkit for manipulating and visualising time-aligned linguistic data such as dialogue transcripts or language processing data . the package complements existing editing tools by allowing for conversion between their formats , information extraction from the raw files , and by adding sophisticated , and easily extended methods for visualising the dynamics of dialogue processing . to illustrate the versatility of the package , we describe its use in three different projects at our site .

automatic domain adaptation for parsing
current statistical parsers tend to perform well only on their training domain and nearby genres . while strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains . when parsing document collections involving heterogeneous domains ( e.g . the web ) , the optimal parsing model for each document is typically not obvious . we study this problem as a new task multiple source parser adaptation . our system trains on corpora from many different domains . it learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy . given a specific target text , the resulting system proposes linear combinations of parsing models trained on the source corpora . tested across six domains , our system outperforms all non-oracle baselines including the best domain-independent parsing model . thus , we are able to demonstrate the value of customizing parsing models to specific domains .

the arc project : creating logical models of gothic cathedrals using natural language processing
the arc project ( for architecture represented computationally ) is an attempt to reproduce in computer form the architectural historians mental model of the gothic cathedral . this model includes the background information necessary to understand a natural language architectural description . our first task is to formalize the description of gothic cathedrals in a logical language , and provide a means for translating into this language from natural language . such a system could then be used by architectural historians and others to facilitate the task of gathering and using information from architectural descriptions . we believe the arc project will represent an important contribution to the preservation of cultural heritage , because it will offer a logical framework for understanding the description of landmark monuments of the past . this paper presents an outline of our plan for the arc system , and examines some of the issues we face in implementing it .

two stage constraint based hybrid approach to free word order language dependency parsing
the paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing . we define the two stages and show how different grammatical construct are parsed at appropriate stages . this division leads to selective identification and resolution of specific dependency relations at the two stages . furthermore , we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser . finally , we evaluate the implemented parser on hindi and compare the results with that of two data driven dependency parsers .

a semantic approach to recognizing textual entailment language computer corporation
exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art nlp systems . in this paper , we take a step closer to this objective . we combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system .

challenges in creating a multilingual sentiment analysis application for social media mining
in the past years , there has been an increasing amount of research done in the field of sentiment analysis . this was motivated by the growth in the volume of user-generated online data , the information flood in social media and the applications sentiment analysis has to different fields marketing , business intelligence , e-law making , decision support systems , etc . although many methods have been proposed to deal with sentiment detection and classification in diverse types of texts and languages , many challenges still arise when passing these methods from the research settings to real-life applications . in this talk , we will describe the manner in which we employed machine translation together with human-annotated data to extend a sentiment analysis system to various languages . additionally , we will describe how a joint multilingual model that detects and classifies sentiments expressed in texts from social media has been developed ( at this point for twitter and facebook ) and demo its use in a real-life application : a project aimed at detecting the citizens attitude on science and technology .

dual decomposition for parsing with non-projective head automata
this paper introduces algorithms for nonprojective parsing based on dual decomposition . we focus on parsing algorithms for nonprojective head automata , a generalization of head-automata models to non-projective structures . the dual decomposition algorithms are simple and efficient , relying on standard dynamic programming and minimum spanning tree algorithms . they provably solve an lp relaxation of the non-projective parsing problem . empirically the lp relaxation is very often tight : for many languages , exact solutions are achieved on over 98 % of test sentences . the accuracy of our models is higher than previous work on a broad range of datasets .

reducing semantic drift with bagging and distributional similarity
iterative bootstrapping algorithms are typically compared using a single set of handpicked seeds . however , we demonstrate that performance varies greatly depending on these seeds , and favourable seeds for one algorithm can perform very poorly with others , making comparisons unreliable . we exploit this wide variation with bagging , sampling from automatically extracted seeds to reduce semantic drift . however , semantic drift still occurs in later iterations . we propose an integrated distributional similarity filter to identify and censor potential semantic drifts , ensuring over 10 % higher precision when extracting large semantic lexicons .

automatic detection and language identification of multilingual documents marco lui , jey han lau and timothy baldwin
language identification is the task of automatically detecting the language ( s ) present in a document based on the content of the document . in this work , we address the problem of detecting documents that contain text from more than one language ( multilingual documents ) . we introduce a method that is able to detect that a document is multilingual , identify the languages present , and estimate their relative proportions . we demonstrate the effectiveness of our method over synthetic data , as well as real-world multilingual documents collected from the web .

latent anaphora resolution for cross-lingual pronoun prediction christian hardmeier jrg tiedemann joakim nivre
this paper addresses the task of predicting the correct french translations of third-person subject pronouns in english discourse , a problem that is relevant as a prerequisite for machine translation and that requires anaphora resolution . we present an approach based on neural networks that models anaphoric links as latent variables and show that its performance is competitive with that of a system with separate anaphora resolution while not requiring any coreference-annotated training data . this demonstrates that the information contained in parallel bitexts can successfully be used to acquire knowledge about pronominal anaphora in an unsupervised way .

sociolinguistics for computational social science
in recent years , a major growth area in applied natural language processing has been the application of automated techniques to massive datasets in order to answer questions about society , and by extension people . sociolinguistics , which combines anthropology , statistics and linguistics ( e.g . labov 1994 , 2001 ) , studies linguistic data in order to answer key questions about the relationship of language and society . sociolinguists focus on frequency and patterns in linguistic usage , correlations , strength of factors and significance , which together reveal information about the sex , age , education and occupation of speakers/writers but also their history , culture , place of residence , social relationships and affiliations . the findings arising from this type research offer important insights into the nature of human organizations at the global , national or community level . they also reveal connections and interactions , the convergence and divergence of groups , historical associations and developing trends . in this paper , i will introduce sociolinguistic research and the nature of sociolinguistic field techniques and sample design . i will argue that socially embedded data is critical for analyzing and discovering social meaning . then , i will summarize the findings of several case studies . what does the use of a 3rd singular morpheme -s , as in ( 1 ) , tell us about the history and culture of a community ( tagliamonte 2012 , 2013 ) how is quotative be like , ( 2 ) , spreading in geographic space ( tagliamonte to appear ) what is the mechanism that underlies linguistic change ( tagliamonte & darcy 2009 ) and by extension cultural trends and projections 1. the english people speaks with grammar .

domain adaptation in statistical machine translation with mixture
mixture modelling is a standard technique for density estimation , but its use in statistical machine translation ( smt ) has just started to be explored . one of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset . this feature is even more important in smt given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears . in this paper , we describe a mixture extension of the hmm alignment model and the derivation of viterbi alignments to feed a state-of-the-art phrase-based system . experiments carried out on the europarl and news commentary corpora show the potential interest and limitations of mixture modelling .

summarizing email threads
summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure . we present initial research which shows that sentence extraction techniques can work for email threads as well , but profit from email-specific features . in addition , the presentation of the summary should take into account the dialogic structure of email communication .

fast lr parsing using rich ( tree adjoining ) grammars
we describe an lr parser of parts-ofspeech ( and punctuation labels ) for tree adjoining grammars ( tags ) , that solves table conflicts in a greedy way , with limited amount of backtracking . we evaluate the parser using the penn treebank showing that the method yield very fast parsers with at least reasonable accuracy , confirming the intuition that lr parsing benefits from the use of rich grammars .

random walks for text semantic similarity
many tasks in nlp stand to benefit from robust measures of semantic similarity for units above the level of individual words . rich semantic resources such as wordnet provide local semantic information at the lexical level . however , effectively combining this information to compute scores for phrases or sentences is an open problem . our algorithm aggregates local relatedness information via a random walk over a graph constructed from an underlying lexical resource . the stationary distribution of the graph walk forms a semantic signature that can be compared to another such distribution to get a relatedness score for texts . on a paraphrase recognition task , the algorithm achieves an 18.5 % relative reduction in error rate over a vector-space baseline . we also show that the graph walk similarity between texts has complementary value as a feature for recognizing textual entailment , improving on a competitive baseline system .

a semi-automatic evaluation scheme : automated nuggetization for
in this paper we describe automatic information nuggetization and its application to text comparison . more specifically , we take a close look at how machine-generated nuggets can be used to create evaluation material . a semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement .

finite-state description of vietnamese reduplication le hong phuong nguyen thi minh huyen
we present for the first time a computational model for the reduplication of the vietnamese language . reduplication is a popular phenomenon of vietnamese in which reduplicative words are created by the combination of multiple syllables whose phonics are similar . we first give a systematical study of vietnamese reduplicative words , bringing into focus clear principles for the formation of a large class of bi-syllabic reduplicative words . we then make use of optimal finite-state devices , in particular minimal sequential string-to string transducers to build a computational model for very efficient recognition and production of those words . finally , several nice applications of this computational model are discussed .

pos-tag based poetry generation with wordnet
in this paper we present the preliminary work of a basque poetry generation system . basically , we have extracted the pos-tag sequences from some verse corpora and calculated the probability of each sequence . for the generation process we have defined 3 different experiments : based on a strophe from the corpora , we ( a ) replace each word with other according to its pos-tag and suffixes , ( b ) replace each noun and adjective with another equally inflected word and ( c ) replace only nouns with semantically related ones ( inflected ) . finally we evaluate those strategies using a turing test-like evaluation .

syntactic complexity measures for detecting mild cognitive impairment
we consider the diagnostic utility of various syntactic complexity measures when extracted from spoken language samples of healthy and cognitively impaired subjects . we examine measures calculated from manually built parse trees , as well as the same measures calculated from automatic parses . we show statistically significant differences between clinical subject groups for a number of syntactic complexity measures , and these differences are preserved with automatic parsing . different measures show different patterns for our data set , indicating that using multiple , complementary measures is important for such an application .

leveraging verb-argument structures to infer semantic relations
this paper presents a methodology to infer implicit semantic relations from verbargument structures . an annotation effort shows implicit relations boost the amount of meaning explicitly encoded for verbs . experimental results with automatically obtained parse trees and verb-argument structures demonstrate that inferring implicit relations is a doable task .

pcfg parsing for restricted classical chinese texts
the probabilistic context-free grammar ( pcfg ) model is widely used for parsing natural languages , including modern chinese . but for classical chinese , the computer processing is just commencing . our previous study on the part-of-speech ( pos ) tagging of classical chinese is a pioneering work in this area . now in this paper , we move on to the pcfg parsing of classical chinese texts . we continue to use the same tagset and corpus as our previous study , and apply the bigram-based forward-backward algorithm to obtain the context-dependent probabilities . then for the pcfg model , we restrict the rewriting rules to be binary/unary rules , which will simplify our programming . a small-sized rule-set was developed that could account for the grammatical phenomena occurred in the corpus . the restriction of texts lies in the limitation on the amount of proper nouns and difficult characters . in our preliminary experiments , the parser gives a promising accuracy of 82.3 % .

co-occurrence cluster features for lexical substitutions in context
this paper examines the influence of features based on clusters of co-occurrences for supervised word sense disambiguation and lexical substitution . cooccurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely unsupervised fashion . clusters can be assigned in context and are used as features in a supervised wsd system . experiments fitting a strong baseline system with these additional features are conducted on two datasets , showing improvements . cooccurrence features are a simple way to mimic topic signatures ( martnez et al , 2008 ) without needing to construct resources manually . further , a system is described that produces lexical substitutions in context with very high precision .

extracting social meaning : identifying interactional style in spoken
automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing . we describe a system for detecting elements of interactional style : whether a speaker is awkward , friendly , or flirtatious . we create and use a new spoken corpus of 991 4-minute speed-dates . participants rated their interlocutors for these elements of style . using rich dialogue , lexical , and prosodic features , we are able to detect flirtatious , awkward , and friendly styles in noisy natural conversational data with up to 75 % accuracy , compared to a 50 % baseline . we describe simple ways to extract relatively rich dialogue features , and analyze which features performed similarly for men and women and which were gender-specific .

correcting keyboard layout errors and homoglyphs in queries
keyboard layout errors and homoglyphs in cross-language queries impact our ability to correctly interpret user information needs and offer relevant results . we present a machine learning approach to correcting these errors , based largely on character-level n-gram features . we demonstrate superior performance over rule-based methods , as well as a significant reduction in the number of queries that yield null search results .

summarization with a joint model for sentence extraction and compression
text summarization is one of the oldest problems in natural language processing . popular approaches rely on extracting relevant sentences from the original documents . as a side effect , sentences that are too long but partly relevant are doomed to either not appear in the final summary , or prevent inclusion of other relevant sentences . sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence . this work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program . we report favorable experimental results on newswire data .

an incremental model for coreference resolution with restrictive
we introduce an incremental model for coreference resolution that competed in the conll 2011 shared task ( open regular ) . we decided to participate with our baseline model , since it worked well with two other datasets . the benefits of an incremental over a mention-pair architecture are : a drastic reduction of the number of candidate pairs , a means to overcome the problem of underspecified items in pairwise classification and the natural integration of global constraints such as transitivity . we do not apply machine learning , instead the system uses an empirically derived salience measure based on the dependency labels of the true mentions . our experiments seem to indicate that such a system already is on par with machine learning approaches .

corpus creation for new genres : a crowdsourced approach to pp attachment
this paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowdsourcing judgments . we develop and present a system to extract prepositional phrases and their potential attachments from ungrammatical and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from amazons mechanical turk service . our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text , and this semi-automated strategy holds promise for similar annotation projects in new genres .

class model adaptation for speech summarisation
the performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation . we extend such adaptation to the use of class models , whose robustness further improves summarisation performance on a wider variety of objective evaluation metrics such as rouge-2 and rouge-su4 used in the text summarisation literature . summaries made from automatic speech recogniser transcriptions benefit from relative improvements ranging from 6.0 % to 22.2 % on all investigated metrics .

interactive topic modeling
topic models have been used extensively as a tool for corpus exploration , and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data . however , creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software . in this work , we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent dirichlet alocation ( lda ) by adding constraints that enforce that sets of words must appear together in the same topic . we incorporate these constraints interactively by selectively removing elements in the state of a markov chain used for inference ; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions .

quantifiers in dependency tree semantics
dependency tree semantics ( dts ) is an underspecified formalism for representing quantifier scope ambiguities in natural language . dts features a direct interface with a dependency grammar and an incremental , constraint-based disambiguation mechanism . in this paper , we discuss the meaning of quantifier dependency in dts by translating its well formed structures into formulae of a second order logic augmented with mostowskian generalized quantifiers .

a robotic agent in a virtual environment that performs situated incremental understanding of navigational utterances
we demonstrate a robotic agent in a 3d virtual environment that understands human navigational instructions . such an agent needs to select actions based on not only instructions but also situations . it is also expected to immediately react to the instructions . our agent incrementally understands spoken instructions and immediately controls a mobile robot based on the incremental understanding results and situation information such as the locations of obstacles and moving history . it can be used as an experimental system for collecting human-robot interactions in dynamically changing situations .

outline of the international standard linguistic annotation equipe langue et dialogue
this paper describes the outline of a linguistic annotation framework under development by iso tc37 sc wg1-1 . this international standard provides an architecture for the creation , annotation , and manipulation of linguistic resources and processing software . the goal is to provide maximum flexibility for encoders and annotators , while at the same time enabling interchange and re-use of annotated linguistic resources . we describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework .

effectively using syntax for recognizing false entailment
recognizing textual entailment is a challenging problem and a fundamental component of many applications in natural language processing . we present a novel framework for recognizing textual entailment that focuses on the use of syntactic heuristics to recognize false entailment . we give a thorough analysis of our system , which demonstrates state-of-the-art performance on a widely-used test set .

multi-adaptive natural language generation using principal component
we present feedbackgen , a system that uses a multi-adaptive approach to natural language generation . with the term multi-adaptive , we refer to a system that is able to adapt its content to different user groups simultaneously , in our case adapting to both lecturers and students . we present a novel approach to student feedback generation , which simultaneously takes into account the preferences of lecturers and students when determining the content to be conveyed in a feedback summary . in this framework , we utilise knowledge derived from ratings on feedback summaries by extracting the most relevant features using principal component regression ( pcr ) analysis . we then model a reward function that is used for training a reinforcement learning agent . our results with students suggest that , from the students perspective , such an approach can generate more preferable summaries than a purely lecturer-adapted approach .

on jointly recognizing and aligning bilingual named entities behavior design corporation
we observe that ( 1 ) how a given named entity ( ne ) is translated ( i.e. , either semantically or phonetically ) depends greatly on its associated entity type , and ( 2 ) entities within an aligned pair should share the same type . also , ( 3 ) those initially detected nes are anchors , whose information should be used to give certainty scores when selecting candidates . from this basis , an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between chinese and english . it adopts a new mapping type ratio feature ( which is the proportion of ne internal tokens that are semantically translated ) , enforces an entity type consistency constraint , and utilizes additional monolingual candidate certainty factors ( based on those ne anchors ) . the experiments show that this novel approach has substantially raised the type-sensitive f-score of identified ne-pairs from 68.4 % to 81.7 % ( 42.1 % f-score imperfection reduction ) in our chinese-english ne alignment task .

minimally supervised multilingual taxonomy and translation lexicon induction
we present a novel algorithm for the acquisition of multilingual lexical taxonomies ( including hyponymy/hypernymy , meronymy and taxonomic cousinhood ) , from monolingual corpora with minimal supervision in the form of seed exemplars using discriminative learning across the major wordnet semantic relationships . this capability is also extended robustly and effectively to a second language ( hindi ) via cross-language projection of the various seed exemplars . we also present a novel model of translation dictionary induction via multilingual transitive models of hypernymy and hyponymy , using these induced taxonomies . candidate lexical translation probabilities are based on the probability that their induced hyponyms and/or hypernyms are translations of one another . we evaluate all of the above models on english and hindi .

a situated context model for resolution and generation of referring expressions
the background for this paper is the aim to build robotic assistants that can naturally interact with humans . one prerequisite for this is that the robot can correctly identify objects or places a user refers to , and produce comprehensible references itself . as robots typically act in environments that are larger than what is immediately perceivable , the problem arises how to identify the appropriate context , against which to resolve or produce a referring expression ( re ) . existing algorithms for generating res generally bypass this problem by assuming a given context . in this paper , we explicitly address this problem , proposing a method for context determination in large-scale space . we show how it can be applied both for resolving and producing res .

towards multi-document summarization of scientific articles : making interesting comparisons with scisumm ravi shankar reddy carolyn penstein rose
we present a novel unsupervised approach to the problem of multi-document summarization of scientific articles , in which the document collection is a list of papers cited together within the same source article , otherwise known as a co-citation . at the heart of the approach is a topic based clustering of fragments extracted from each co-cited article and relevance ranking using a query generated from the context surrounding the cocited list of papers . this analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found . we present a system called scisumm that embodies this approach and apply it to the 2008 acl anthology . we evaluate this summarization system for relevant content selection using gold standard summaries prepared on principle based guidelines . evaluation with gold standard summaries demonstrates that our system performs better in content selection than an existing summarization system ( mead ) . we present a detailed summary of our findings and discuss possible directions for future research .

gappy phrasal alignment by agreement
we propose a principled and efficient phraseto-phrase alignment model , useful in machine translation as well as other related natural language processing problems . in a hidden semimarkov model , word-to-phrase and phraseto-word translations are modeled directly by the system . agreement between two directional models encourages the selection of parsimonious phrasal alignments , avoiding the overfitting commonly encountered in unsupervised training with multi-word units . expanding the state space to include gappy phrases ( such as french ne pas ) makes the alignment space more symmetric ; thus , it allows agreement between discontinuous alignments . the resulting system shows substantial improvements in both alignment quality and translation quality over word-based hidden markov models , while maintaining asymptotically equivalent runtime .

exploiting syntactico-semantic structures for relation extraction
in this paper , we observe that there exists a second dimension to the relation extraction ( re ) problem that is orthogonal to the relation type dimension . we show that most of these second dimensional structures are relatively constrained and not difficult to identify . we propose a novel algorithmic approach to re that starts by first identifying these structures and then , within these , identifying the semantic type of the relation . in the real re problem where relation arguments need to be identified , exploiting these structures also allows reducing pipelined propagated errors . we show that this re framework provides significant improvement in re performance .

an analysis of clarification dialogue for question answering
we examine clarification dialogue , a mechanism for refining user questions with follow-up questions , in the context of open domain question answering systems . we develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering . the algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval .

reordering constraint based on document-level context
one problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders , such as japanese-english . in this paper , we propose a method of imposing reordering constraints using document-level context . as the documentlevel context , we use noun phrases which significantly occur in context documents containing source sentences . given a source sentence , zones which cover the noun phrases are used as reordering constraints . then , in decoding , reorderings which violate the zones are restricted . experiment results for patent translation tasks show a significant improvement of 1.20 % bleu points in japaneseenglish translation and 1.41 % bleu points in english-japanese translation .

semantic dependency parsing of nombank and propbank : an efficient integrated approach via a large-scale feature selection
we present an integrated dependencybased semantic role labeling system for english from both nombank and propbank . by introducing assistant argument labels and considering much more feature templates , two optimal feature template sets are obtained through an effective feature selection procedure and help construct a high performance single srl system . from the evaluations on the date set of conll-2008 shared task , the performance of our system is quite close to the state of the art . as to our knowledge , this is the first integrated srl system that achieves a competitive performance against previous pipeline systems .

relieving the computational bottleneck : joint inference for event extraction with high-dimensional features
several state-of-the-art event extraction systems employ models based on support vector machines ( svms ) in a pipeline architecture , which fails to exploit the joint dependencies that typically exist among events and arguments . while there have been attempts to overcome this limitation using markov logic networks ( mlns ) , it remains challenging to perform joint inference in mlns when the model encodes many high-dimensional sophisticated features such as those essential for event extraction . in this paper , we propose a new model for event extraction that combines the power of mlns and svms , dwarfing their limitations . the key idea is to reliably learn and process high-dimensional features using svms ; encode the output of svms as low-dimensional , soft formulas in mlns ; and use the superior joint inferencing power of mlns to enforce joint consistency constraints over the soft formulas . we evaluate our approach for the task of extracting biomedical events on the bionlp 2013 , 2011 and 2009 genia shared task datasets . our approach yields the best f1 score to date on the bionlp13 ( 53.61 ) and bionlp11 ( 58.07 ) datasets and the second-best f1 score to date on the bionlp09 dataset ( 58.16 ) .

using synonym relations in chinese collocation extraction
a challenging task in chinese collocation extraction is to improve both the precision and recall rate . most lexical statistical methods including xtract face the problem of unable to extract collocations with lower frequencies than a given threshold . this paper presents a method where hownet is used to find synonyms using a similarity function . based on such synonym information , we have successfully extracted synonymous collocations which normally can not be extracted using the lexical statistical approach . we applied synonyms mapping to each headword to extract more synonymous word bi-grams . our evaluation over 60mb tagged corpus shows that we can extract synonymous collocations that occur with very low frequency , sometimes even for collocations that occur only once in the training set . comparing to a collocation extraction system based on xtract , we have reached the precision rate of 43 % on word bi-grams for a set of 9 headwords , almost 50 % improvement from precision rate of 30 % in the xtract system . furthermore , it improves the recall rate of word bi-gram collocation extraction by 30 % .

a named-entity disambiguation framework for arabic text
there has been recently a great progress in the field of automatically generated knowledge bases and corresponding disambiguation systems that are capable of mapping text mentions onto canonical entities . efforts like the before mentioned have enabled researchers and analysts from various disciplines to semantically understand contents . however , most of the approaches have been specifically designed for the english language and - in particular - support for arabic is still in its infancy . since the amount of arabic web contents ( e.g . in social media ) has been increasing dramatically over the last years , we see a great potential for endeavors that support an entity-level analytics of these data . to this end , we have developed a framework called aidarabic that extends the existing aida system by additional components that allow the disambiguation of arabic texts based on an automatically generated knowledge base distilled from wikipedia . even further , we overcome the still existing sparsity of the arabic wikipedia by exploiting the interwiki links between arabic and english contents in wikipedia , thus , enriching the entity catalog as well as disambiguation context .

an unsupervised aspect-sentiment model for online reviews
with the increase in popularity of online review sites comes a corresponding need for tools capable of extracting the information most important to the user from the plain text data . due to the diversity in products and services being reviewed , supervised methods are often not practical . we present an unsupervised system for extracting aspects and determining sentiment in review text . the method is simple and flexible with regard to domain and language , and takes into account the influence of aspect on sentiment polarity , an issue largely ignored in previous literature . we demonstrate its effectiveness on both component tasks , where it achieves similar results to more complex semi-supervised methods that are restricted by their reliance on manual annotation and extensive knowledge sources .

robust sentiment detection on twitter from biased and noisy data
in this paper , we propose an approach to automatically detect sentiments on twitter messages ( tweets ) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages . moreover , we leverage sources of noisy labels as our training data . these noisy labels were provided by a few sentiment detection websites over twitter data . in our experiments , we show that since our features are able to capture a more abstract representation of tweets , our solution is more effective than previous ones and also more robust regarding biased and noisy data , which is the kind of data provided by these sources .

opinion graphs for polarity and discourse classification
this work shows how to construct discourse-level opinion graphs to perform a joint interpretation of opinions and discourse relations . specifically , our opinion graphs enable us to factor in discourse information for polarity classification , and polarity information for discourse-link classification . this inter-dependent framework can be used to augment and improve the performance of local polarity and discourse-link classifiers .

interpretation and generation in a knowledge-based tutorial system
we discuss how deep interpretation and generation can be integrated with a knowledge representation designed for question answering to build a tutorial dialogue system . we use a knowledge representation known to perform well in answering exam-type questions and show that to support tutorial dialogue it needs additional features , in particular , compositional representations for interpretation and structured explanation representations .

combining prediction by partial matching and logistic regression for thai word segmentation
word segmentation is an important part of many applications , including information retrieval , information filtering , document analysis , and text summarization . in thai language , the process is complicated since words are written continuously , and their structures are not well-defined . a recognized effective approach to word segmentation is longest matching , a method based on dictionary . nevertheless , this method suffers from character-level and syllable-level ambiguities in determining word boundaries . this paper proposes a technique to thai word segmentation using a two-step approach . first , text is segmented , using an application of prediction by partial matching , into syllables whose structures are more well-defined . this reduces the earlier type of ambiguity . then , the syllables are combined into words by an application of a syllable-level longest matching method together with a logistic regression model which takes into account contextual information . the experimental results show the syllable segmentation accuracy of more than 96.65 % and the overall word segmentation accuracy of 97 % .

a discriminative global training algorithm for statistical mt
this paper presents a novel training algorithm for a linearly-scored block sequence translation model . the key component is a new procedure to directly optimize the global scoring function used by a smt decoder . no translation , language , or distortion model probabilities are used as in earlier work on smt . therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches . moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme . the training algorithm is evaluated on a standard arabic-english translation task .

can crowds build parallel corpora for machine translation systems
corpus based approaches to machine translation ( mt ) rely on the availability of parallel corpora . in this paper we explore the effectiveness of mechanical turk for creating parallel corpora . we explore the task of sentence translation , both into and out of a language . we also perform preliminary experiments for the task of phrase translation , where ambiguous phrases are provided to the turker for translation in isolation and in the context of the sentence it originated from .

building large-scale twitter-specific sentiment lexicon : a representation learning approach
in this paper , we propose to build large-scale sentiment lexicon from twitter with a representation learning approach . we cast sentiment lexicon learning as a phrase-level sentiment classification task . the challenges are developing effective feature representation of phrases and obtaining training data with minor manual annotations for building the sentiment classifier . specifically , we develop a dedicated neural architecture and integrate the sentiment information of text ( e.g . sentences or tweets ) into its hybrid loss function for learning sentiment-specific phrase embedding ( sspe ) . the neural network is trained from massive tweets collected with positive and negative emoticons , without any manual annotation . furthermore , we introduce the urban dictionary to expand a small number of sentiment seeds to obtain more training data for building the phrase-level sentiment classifier . we evaluate our sentiment lexicon ( ts-lex ) by applying it in a supervised learning framework for twitter sentiment classification . experiment results on the benchmark dataset of semeval 2013 show that , ts-lex yields better performance than previously introduced sentiment lexicons .

generalizing over lexical features : selectional preferences for semantic role classification
this paper explores methods to alleviate the effect of lexical sparseness in the classification of verbal arguments . we show how automatically generated selectional preferences are able to generalize and perform better than lexical features in a large dataset for semantic role classification . the best results are obtained with a novel second-order distributional similarity measure , and the positive effect is specially relevant for out-of-domain data . our findings suggest that selectional preferences have potential for improving a full system for semantic role labeling .

systemt : an algebraic approach to declarative information extraction
as information extraction ( ie ) becomes more central to enterprise applications , rule-based ie engines have become increasingly important . in this paper , we describe systemt , a rule-based ie system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars . systemt uses a declarative rule language , aql , and an optimizer that generates high-performance algebraic execution plans for aql rules . we compare systemts approach against cascading grammars , both theoretically and with a thorough experimental evaluation . our results show that systemt can deliver result quality comparable to the state-of-theart and an order of magnitude higher annotation throughput .

word error rates : decomposition over pos classes and applications for
evaluation and error analysis of machine translation output are important but difficult tasks . in this work , we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of word error rate ( wer ) and position independent word error rate ( per ) over different partof-speech ( pos ) classes . furthermore , we investigate two possible aspects of the use of these decompositions for automatic error analysis : estimation of inflectional errors and distribution of missing words over pos classes . the obtained results are shown to correspond to the results of a human error analysis . the results obtained on the european parliament plenary session corpus in spanish and english give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system .

nulex : an open-license broad coverage lexicon
broad coverage lexicons for the english language have traditionally been handmade . this approach , while accurate , requires too much human labor . furthermore , resources contain gaps in coverage , contain specific types of information , or are incompatible with other resources . we believe that the state of open-license technology is such that a comprehensive syntactic lexicon can be automatically compiled . this paper describes the creation of such a lexicon , nu-lex , an open-license feature-based lexicon for general purpose parsing that combines wordnet , verbnet , and wiktionary and contains over 100,000 words . nu-lex was integrated into a bottom up chart parser . we ran the parser through three sets of sentences , 50 sentences total , from the simple english wikipedia and compared its performance to the same parser using comlex . both parsers performed almost equally with nu-lex finding all lex-items for 50 % of the sentences and comlex succeeding for 52 % . furthermore , nulexs shortcomings primarily fell into two categories , suggesting future research directions .

in search of the right word
we report on a user needs investigation carried out in the framework of the pro-ject ekfrasis that developed a plat-form for supporting authoring work in modern greek . the platform had as a backbone a conceptually organised dic-tionary enhanced with rich lexicographic and morphosyntactic information . or-ganisation of information and encoding drew on semantic web technologies ( on-tologies ) . users were all professional au-thors ( of literature , editors , translators , journalists ) working for well-established firms . they were all familiar with printed conceptually organized dictionar-ies while most of them used a computer . they were asked to specify how the plat-form would be helpful to them when they searched for a word for which they had only vague or few clues , a situation that was familiar to all of them . users preferred to have , in a first step , easy ac-cess to limited but to-the-point lexical in-formation while access to rich semantic information should be provided at a sec-ond step . they were interested in rich lexical material although they were not really able to identify the relations that would help them retrieve it . they strongly preferred an organization of ma-terial by concept and pos and appreciated easy access to normative informa-tion .

the acl rd-tec : a dataset for benchmarking terminology extraction and classification in computational linguistics and siegfried handschuh
this paper introduces acl rd-tec : a dataset for evaluating the extraction and classification of terms from literature in the domain of computational linguistics . the dataset is derived from the association for computational linguistics anthology reference corpus ( acl arc ) . in its first release , the acl rd-tec consists of automatically segmented , part-of-speech-tagged acl arc documents , three lists of candidate terms , and more than 82,000 manually annotated terms . the annotated terms are marked as either valid or invalid , and valid terms are further classified as technology and non-technology terms . technology terms signify methods , algorithms , and solutions in computational linguistics . the paper describes the dataset and reports the relevant statistics . we hope the step described in this paper encourages a collaborative effort towards building a full-fledged annotated corpus from the computational linguistics literature .

dynamic programming algorithms for transition-based dependency parsers
we develop a general dynamic programming technique for the tabulation of transition-based dependency parsers , and apply it to obtain novel , polynomial-time algorithms for parsing with the arc-standard and arc-eager models . we also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods . additionally , we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms .

proalign : shared task system description
proalign combines several different approaches in order to produce high quality word word alignments . like competitive linking , proalign uses a constrained search to find high scoring alignments . like em-based methods , a probability model is used to rank possible alignments . the goal of this paper is to give a birds eye view of the proalign system to encourage discussion and comparison .

subword and spatiotemporal models for identifying actionable information in haitian kreyol
crisis-affected populations are often able to maintain digital communications but in a sudden-onset crisis any aid organizations will have the least free resources to process such communications . information that aid agencies can actually act on , actionable information , will be sparse so there is great potential to ( semi ) automatically identify actionable communications . however , there are hurdles as the languages spoken will often be underresourced , have orthographic variation , and the precise definition of actionable will be response-specific and evolving . we present a novel system that addresses this , drawing on 40,000 emergency text messages sent in haiti following the january 12 , 2010 earthquake , predominantly in haitian kreyol . we show that keyword/ngram-based models using streaming maxent achieve up to f=0.21 accuracy . further , we find current state-ofthe-art subword models increase this substantially to f=0.33 accuracy , while modeling the spatial , temporal , topic and source contexts of the messages can increase this to a very accurate f=0.86 over direct text messages and f=0.90-0.97 over social media , making it a viable strategy for message prioritization .

a pipeline approach to chinese personal name ministry of education , china
in this paper , we describe our system for chinese personal name disambiguation task in the first cipssighan joint conference on chinese language processing ( clp2010 ) . we use a pipeline approach , in which preprocessing , unrelated documents discarding , chinese personal name extension and document clustering are performed separately . chinese personal name extension is the most important part of the system . it uses two additional dictionaries to extract full personal names in chinese text . and then document clustering is performed under different personal names . experimental results show that our system can achieve good performances .

natural language searching in onomasiological dictionaries
when consulting a dictionary , people can find the meaning of a word via the definition , which usually contains the relevant information to fulfil their requirement . lexicographers produce dictionaries and their work consists in presenting information essential for grasping the meaning of words . however , when people need to find a word it is likely that they do not obtain the information they are looking for . there is a gap between dictionary definitions and the information being available in peoples mind . this paper attempts to present the conceptualisation people engage in , in order to arrive at a word from its meaning . the insights of an experiment conducted show us the differences between the knowledge available in peoples minds and in dictionary definitions .

the imagination of crowds : conversational aac language modeling using crowdsourcing and large data sources per ola kristensson
augmented and alternative communication ( aac ) devices enable users with certain communication disabilities to participate in everyday conversations . such devices often rely on statistical language models to improve text entry by offering word predictions . these predictions can be improved if the language model is trained on data that closely reflects the style of the users intended communications . unfortunately , there is no large dataset consisting of genuine aac messages . in this paper we demonstrate how we can crowdsource the creation of a large set of fictional aac messages . we show that these messages model conversational aac better than the currently used datasets based on telephone conversations or newswire text . we leverage our crowdsourced messages to intelligently select sentences from much larger sets of twitter , blog and usenet data . compared to a model trained only on telephone transcripts , our best performing model reduced perplexity on three test sets of aac-like communications by 60 82 % relative . this translated to a potential keystroke savings in a predictive keyboard interface of 511 % .

no sentence is too confusing to ignore
we consider sentences of the form no x is too y to z , in which x is a noun phrase , y is an adjective phrase , and z is a verb phrase . such constructions are ambiguous , with two possible ( and opposite ! ) interpretations , roughly meaning either that every x zs , or that no x zs . the interpretations have been noted to depend on semantic and pragmatic factors . we show here that automatic disambiguation of this pragmatically complex construction can be largely achieved by using features of the lexical semantic properties of the verb ( i.e. , z ) participating in the construction . we discuss our experimental findings in the context of construction grammar , which suggests a possible account of this phenomenon .

web service integration for next generation localisation
developments in natural language processing technologies promise a variety of benefits to the localization industry , both in its current form in performing bulk enterprise-based localization and in the future in supporting personalized web-based localization on increasingly user-generated content . as an increasing variety of natural language processing services become available , it is vital that the localization industry employs the flexible software integration techniques that will enable it to make best use of these technologies . to date however , the localization industry has been slow reap the benefits of modern integration technologies such as web service integration and orchestration . based on recent integration experiences , we examine how the localization industry can best exploit web-based integration technologies in developing new services and exploring new business models

breaking out of local optima with count transforms and model recombination : a study in grammar induction
many statistical learning problems in nlp call for local model search methods . but accuracy tends to suffer with current techniques , which often explore either too narrowly or too broadly : hill-climbers can get stuck in local optima , whereas samplers may be inefficient . we propose to arrange individual local optimizers into organized networks . our building blocks are operators of two types : ( i ) transform , which suggests new places to search , via non-random restarts from already-found local optima ; and ( ii ) join , which merges candidate solutions to find better optima . experiments on grammar induction show that pursuing different transforms ( e.g. , discarding parts of a learned model or ignoring portions of training data ) results in improvements . groups of locally-optimal solutions can be further perturbed jointly , by constructing mixtures . using these tools , we designed several modular dependency grammar induction networks of increasing complexity . our complete system achieves 48.6 % accuracy ( directed dependency macro-average over all 19 languages in the 2006/7 conll data ) more than 5 % higher than the previous state-of-the-art .

the nite xml toolkit : demonstration from ve corpora
the nite xml toolkit ( nxt ) is open source software for working with multimodal , spoken , or text language corpora . it is specifically designed to support the tasks of human annotators and analysts of heavily cross-annotated data sets , and has been used successfully on a range of projects with varying needs . in this text to accompany a demonstration , we describe nxt along with four uses on different corpora that together span its most novel features . the examples involve the ami and icsi meeting corpora ; a study of multimodal reference ; a syntactic analysis of genesis in classical hebrew ; and discourse annotation of switchboard dialogues .

a comparative study on ranking and selection strategies for
this paper presents a comparative study on two key problems existing in extractive summarization : the ranking problem and the selection problem . to this end , we presented a systematic study of comparing different learning-to-rank algorithms and comparing different selection strategies . this is the first work of providing systematic analysis on these problems . experimental results on two benchmark datasets demonstrate three findings : ( 1 ) pairwise and listwise learning-to-rank algorithms outperform the baselines significantly ; ( 2 ) there is no significant difference among the learning-to-rank algorithms ; and ( 3 ) the integer linear programming selection strategy generally outperformed maximum marginal relevance and diversity penalty strategies .

tests in spoken language proficiency assessment applications
this work introduces new methods for detecting non-scorable tests , i.e. , tests that can not be accurately scored automatically , in educational applications of spoken language proficiency assessment . those include cases of unreliable automatic speech recognition ( asr ) , often because of noisy , off-topic , foreign or unintelligible speech . we examine features that estimate signalderived syllable information and compare it with asr results in order to detect responses with problematic recognition . further , we explore the usefulness of language model based features , both for language models that are highly constrained to the spoken task , and for task independent phoneme language models . we validate our methods on a challenging dataset of young english language learners ( ells ) interacting with an automatic spoken assessment system . our proposed methods achieve comparable performance compared to existing non-scorable detection approaches , and lead to a 21 % relative performance increase when combined with existing approaches .

sinai : voting system for twitter sentiment analysis
this article presents the participation of the sinai research group in the task sentiment analysis in twitter of the semeval workshop . our proposal consists of a voting system of three polarity classifiers which follow a lexicon-based approach .

from machine translation to computer assisted translation using
state-of-the-art machine translation techniques are still far from producing high quality translations . this drawback leads us to introduce an alternative approach to the translation problem that brings human expertise into the machine translation scenario . in this framework , namely computer assisted translation ( cat ) , human translators interact with a translation system , as an assistance tool , that dinamically offers , a list of translations that best completes the part of the sentence already translated . in this paper , finite state transducers are presented as a candidate technology in the cat paradigm . the appropriateness of this technique is evaluated on a printer manual corpus and results from preliminary experiments confirm that human translators would reduce to less than 25 % the amount of work to be done for the same task .

transfer learning for constituency-based grammars
in this paper , we consider the problem of cross-formalism transfer in parsing . we are interested in parsing constituencybased grammars such as hpsg and ccg using a small amount of data specific for the target formalism , and a large quantity of coarse cfg annotations from the penn treebank . while all of the target formalisms share a similar basic syntactic structure with penn treebank cfg , they also encode additional constraints and semantic features . to handle this apparent discrepancy , we design a probabilistic model that jointly generates cfg and target formalism parses . the model includes features of both parses , allowing transfer between the formalisms , while preserving parsing efficiency . we evaluate our approach on three constituency-based grammars ccg , hpsg , and lfg , augmented with the penn treebank-1 . our experiments show that across all three formalisms , the target parsers significantly benefit from the coarse annotations.1

unsupervised learning summarization templates from concise summaries
we here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages . we evaluate the two approaches in two different information extraction settings : monolingual and cross-lingual information extraction . the extraction systems are trained on auto-annotated summaries ( containing the induced concepts ) and evaluated on humanannotated documents . extraction results are promising , being close in performance to those achieved when the system is trained on human-annotated summaries .

document expansion based on wordnet for robust ir
the use of semantic information to improve ir is a long-standing goal . this paper presents a novel document expansion method based on a wordnet-based system to find related concepts and words . expansion words are indexed separately , and when combined with the regular index , they improve the results in three datasets over a state-of-the-art ir engine . considering that many ir systems are not robust in the sense that they need careful finetuning and optimization of their parameters , we explored some parameter settings . the results show that our method is specially effective for realistic , non-optimal settings , adding robustness to the ir engine . we also explored the effect of document length , and show that our method is specially successful with shorter documents .

a second language acquisition model using ari rappoport vera sheinman
we present a computational model of acquiring a second language from example sentences . our learning algorithms build a construction grammar language model , and generalize using form-based patterns and the learners conceptual system . we use a unique professional language learning corpus , and show that substantial reliable learning can be achieved even though the corpus is very small . the model is applied to assisting the authoring of japanese language learning corpora .

annotating multiparty discourse : challenges for agreement metrics
to computationally model discourse phenomena such as argumentation we need corpora with reliable annotation of the phenomena under study . annotating complex discourse phenomena poses two challenges : fuzziness of unit boundaries and the need for multiple annotators . we show that current metrics for inter-annotator agreement ( iaa ) such as p/r/f1 and krippendorffs provide inconsistent results for the same text . in addition , iaa metrics do not tell us what parts of a text are easier or harder for human judges to annotate and so do not provide sufficiently specific information for evaluating systems that automatically identify discourse units . we propose a hierarchical clustering approach that aggregates overlapping text segments of text identified by multiple annotators ; the more annotators who identify a text segment , the easier we assume that the text segment is to annotate . the clusters make it possible to quantify the extent of agreement judges show about text segments ; this information can be used to assess the output of systems that automatically identify discourse units .

matching readers preferences and reading skills with appropriate web
this paper describes read-x , a system designed to identify text that is appropriate for the reader given his thematic choices and the reading ability associated with his educational background . to our knowledge , read-x is the first web-based system that performs real-time searches and returns results classified thematically and by reading level within seconds . to facilitate educators or students searching for reading material at specific reading levels , read-x extracts the text from the html , pdf , doc , or xml format and makes available a text editor for viewing and editing the extracted text .

paraphrase alignment for synonym evidence discovery
we describe a new unsupervised approach for synonymy discovery by aligning paraphrases in monolingual domain corpora . for that purpose , we identify phrasal terms that convey most of the concepts within domains and adapt a methodology for the automatic extraction and alignment of paraphrases to identify paraphrase casts from which valid synonyms are discovered . results performed on two different domain corpora show that general synonyms as well as synonymic expressions can be identified with a 67.27 % precision .

learning non-cooperative dialogue behaviours
non-cooperative dialogue behaviour has been identified as important in a variety of application areas , including education , military operations , video games and healthcare . however , it has not been addressed using statistical approaches to dialogue management , which have always been trained for co-operative dialogue . we develop and evaluate a statistical dialogue agent which learns to perform noncooperative dialogue moves in order to complete its own objectives in a stochastic trading game . we show that , when given the ability to perform both cooperative and non-cooperative dialogue moves , such an agent can learn to bluff and to lie so as to win games more often against a variety of adversaries , and under various conditions such as risking penalties for being caught in deception . for example , we show that a non-cooperative dialogue agent can learn to win an additional 15.47 % of games against a strong rulebased adversary , when compared to an optimised agent which can not perform noncooperative moves . this work is the first to show how an agent can learn to use noncooperative dialogue to effectively meet its own goals .

single document summarization based on nested tree structure
many methods of text summarization combining sentence selection and sentence compression have recently been proposed . although the dependency between words has been used in most of these methods , the dependency between sentences , i.e. , rhetorical structures , has not been exploited in such joint methods . we used both dependency between words and dependency between sentences by constructing a nested tree , in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words . we formulated a summarization task as a combinatorial optimization problem , in which the nested tree was trimmed without losing important content in the source document . the results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts .

parsing croatian and serbian by using croatian dependency treebanks zeljko agic danijela merkler dasa berovic
we investigate statistical dependency parsing of two closely related languages , croatian and serbian . as these two morphologically complex languages of relaxed word order are generally under-resourced with the topic of dependency parsing still largely unaddressed , especially for serbian we make use of the two available dependency treebanks of croatian to produce state-of-the-art parsing models for both languages . we observe parsing accuracy on four test sets from two domains . we give insight into overall parser performance for croatian and serbian , impact of preprocessing for lemmas and morphosyntactic tags and influence of selected morphosyntactic features on parsing accuracy .

stochastic discourse modeling in spoken dialogue systems using semantic dependency graphs
this investigation proposes an approach to modeling the discourse of spoken dialogue using semantic dependency graphs . by characterizing the discourse as a sequence of speech acts , discourse modeling becomes the identification of the speech act sequence . a statistical approach is adopted to model the relations between words in the users utterance using the semantic dependency graphs . dependency relation between the headword and other words in a sentence is detected using the semantic dependency grammar . in order to evaluate the proposed method , a dialogue system for medical service is developed . experimental results show that the rates for speech act detection and taskcompletion are 95.6 % and 85.24 % , respectively , and the average number of turns of each dialogue is 8.3. compared with the bayes classifier and the partialpattern tree based approaches , we obtain 14.9 % and 12.47 % improvements in accuracy for speech act identification , respectively .

computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech
this paper focuses on identifying , extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence . our goal is to find effective features , selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses , and to build automatic scoring models based on the most promising features by using machine learning methods . on human transcriptions with manually annotated clause and sentence boundaries , our best scoring model achieves an overall pearson correlation with human rater scores of r=0.49 on an unseen test set , whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2 .

some considerations on guidelines
despite progress in the development of computational means , human input is still critical in the production of consistent and useable aligned corpora and term banks . this is especially true for specialized corpora and term banks whose end-users are often professionals with very stringent requirements for accuracy , consistency and coverage . in the compilation of a high quality chinese-english legal glossary for eldos project , we have identified a number of issues that make the role human input critical for term alignment and extraction . they include the identification of low frequency terms , paraphrastic expressions , discontinuous units , and maintaining consistent term granularity , etc . although manual intervention can more satisfactorily address these issues , steps must also be taken to address intra- and inter-annotator inconsistency . keyword : legal terminology , bilingual terminology , bilingual alignment , corpus-based linguistics

mimus : a multimodal and multilingual dialogue system for the home
this paper describes mimus , a multimodal and multilingual dialogue system for the in home scenario , which allows users to control some home devices by voice and/or clicks . its design relies on wizard of oz experiments and is targeted at disabled users . mimus follows the information state update approach to dialogue management , and supports english , german and spanish , with the possibility of changing language onthe fly . mimus includes a gesturesenabled talking head which endows the system with a humanlike personality .

a hybrid hierarchical model for multi-document summarization
scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization . in this paper , we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference . we calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model . then , using these scores , we train a regression model based on the lexical and structural characteristics of the sentences , and use the model to score sentences of new documents to form a summary . our system advances current state-of-the-art improving rouge scores by 7 % . generated summaries are less redundant and more coherent based upon manual quality evaluations .

semantic and pragmatic presupposition in discourse representation
this paper investigates semantic and pragmatic presupposition in discourse representation theory ( drt ) and enhances the pragmatic perspective of presupposition in drt . in doing so , it draws attention to the need to account for agent presupposition ( i.e . both speaker and hearer presupposition ) when dealing with pragmatic presupposition . furthermore , this paper links this pragmatic conception of presupposition with the semantic one ( sentence presupposition ) through using information checks which agents are hypothesized to employ when making and receiving utterances.1

wbi-ner : the impact of domain-specific features on the performance of identifying and classifying mentions of drugs
named entity recognition ( ner ) systems are often based on machine learning techniques to reduce the labor-intensive development of hand-crafted extraction rules and domain-dependent dictionaries . nevertheless , time-consuming feature engineering is often needed to achieve state-of-the-art performance . in this study , we investigate the impact of such domain-specific features on the performance of recognizing and classifying mentions of pharmacological substances . we compare the performance of a system based on general features , which have been successfully applied to a wide range of ner tasks , with a system that additionally uses features generated from the output of an existing chemical ner tool and a collection of domain-specific resources . we demonstrate that acceptable results can be achieved with the former system . still , our experiments show that using domain-specific features outperforms this general approach . our system ranked first in the semeval-2013 task 9.1 : recognition and classification of pharmacological substances .

sensicon : an automatically constructed sensorial lexicon
connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge . with this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language . however , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource . in this paper , we present a sensorial lexicon that associates english words with senses . to obtain this resource , we apply a computational method based on bootstrapping and corpus statistics . the quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing . the results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .

umcc_dlsi_prob : a probabilistic automata for aspect based
this work introduces a new approach for aspect based sentiment analysis task . its main purpose is to automatically assign the correct polarity for the aspect term in a phrase . it is a probabilistic automata where each state consists of all the nouns , adjectives , verbs and adverbs found in an annotated corpora . each one of them contains the number of occurrences in the annotated corpora for the four required polarities ( i.e . positive , negative , neutral and conflict ) . also , the transitions between states have been taken into account . these values were used to assign the predicted polarity when a pattern was found in a sentence ; if a pattern can not be applied , the probabilities of the polarities between states were computed in order to predict the right polarity . the system achieved results around 66 % and 57 % of recall for the restaurant and laptop domain respectively .

learning theories from text
in this paper we describe a method of automatically learning domain theories from parsed corpora of sentences from the relevant domain and use fsa techniques for the graphical representation of such a theory . by a domain theory we mean a collection of facts and generalisations or rules which capture what commonly happens ( or does not happen ) in some domain of interest . as language users , we implicitly draw on such theories in various disambiguation tasks , such as anaphora resolution and prepositional phrase attachment , and formal encodings of domain theories can be used for this purpose in natural language processing . they may also be objects of interest in their own right , that is , as the output of a knowledge discovery process . the approach is generizable to different domains provided it is possible to get logical forms for the text in the domain .

improving mention detection robustness to noisy input
information-extraction ( ie ) research typically focuses on clean-text inputs . however , an ie engine serving real applications yields many false alarms due to less-well-formed input . for example , ie in a multilingual broadcast processing system has to deal with inaccurate automatic transcription and translation . the resulting presence of non-target-language text in this case , and non-language material interspersed in data from other applications , raise the research problem of making ie robust to such noisy input text . we address one such ie task : entity-mention detection . we describe augmenting a statistical mention-detection system in order to reduce false alarms from spurious passages . the diverse nature of input noise leads us to pursue a multi-faceted approach to robustness . for our english-language system , at various miss rates we eliminate 97 % of false alarms on inputs from other latin-alphabet languages . in another experiment , representing scenarios in which genre-specific training is infeasible , we process real financial-transactions text containing mixed languages and data-set codes . on these data , because we do not train on data like it , we achieve a smaller but significant improvement .

predicting party affiliations from european parliament debates
this paper documents an ongoing effort to assess whether party group affiliation of participants in european parliament debates can be automatically predicted on the basis of the content of their speeches , using a support vector machine multi-class model . the work represents a joint effort between researchers within political science and language technology .

catib : the columbia arabic treebank
the columbia arabic treebank ( catib ) is a database of syntactic analyses of arabic sentences . catib contrasts with previous approaches to arabic treebanking in its emphasis on speed with some constraints on linguistic richness . two basic ideas inspire the catib approach : no annotation of redundant information and using representations and terminology inspired by traditional arabic syntax . we describe catibs representation and annotation procedure , and report on interannotator agreement and speed .

close = relevant the role of context in efficient language production
we formally derive a mathematical model for evaluating the effect of context relevance in language production . the model is based on the principle that distant contextual cues tend to gradually lose their relevance for predicting upcoming linguistic signals . we evaluate our model against a hypothesis of efficient communication ( genzel and charniaks constant entropy rate hypothesis ) . we show that the development of entropy throughout discourses is described significantly better by a model with cue relevance decay than by previous models that do not consider context effects .

corpus-based discourse understanding in spoken dialogue systems
this paper concerns the discourse understanding process in spoken dialogue systems . this process enables the system to understand user utterances based on the context of a dialogue . since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance . by holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses , the discourse understanding accuracy can be improved . this paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora . unlike conventional methods that use hand-crafted rules , the proposed method enables easy design of the discourse understanding process . experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective . currently with the school of media science , tokyo university of technology , 1404-1 katakuracho , hachioji , tokyo 192-0982 , japan .

the role of polarity in inferring acceptance and rejection in dialogue
we study the role that logical polarity plays in determining the rejection or acceptance function of an utterance in dialogue . we develop a model inspired by recent work on the semantics of negation and polarity particles and test it on annotated data from two spoken dialogue corpora : the switchboard corpus and the ami meeting corpus . our experiments show that taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora .

dynamic correspondences : an object-oriented approach to tracking sound reconstructions belem para brasil
this paper reports the results of a research project that experiments with crosstabulation in aiding phonemic reconstruction . data from the tup stock was used , and three tests were conducted in order to determine the efficacy of this application : the confirmation and challenging of a previously established reconstruction in the family ; testing a new reconstruction generated by our model ; and testing the upper limit of simultaneous , multiple correspondences across several languages . our conclusion is that the use of cross tabulations ( implemented within a database as pivot tables ) offers an innovative and effective tool in comparative study and sound reconstruction .

robust interpretation of user requests for text retrieval in a multimodal
we describe a parser for robust and flexible interpretation of user utterances in a multi-modal system for web search in newspaper databases . users can speak or type , and they can navigate and follow links using mouse clicks . spoken or written queries may combine search expressions with browser commands and search space restrictions . in interpreting input queries , the system has to be fault-tolerant to account for spontanous speech phenomena as well as typing or speech recognition errors which often distort the meaning of the utterance and are difficult to detect and correct . our parser integrates shallow parsing techniques with knowledge-based text retrieval to allow for robust processing and coordination of input modes . parsing relies on a two-layered approach : typical meta-expressions like those concerning search , newspaper types and dates are identified and excluded from the search string to be sent to the search engine . the search terms which are left after preprocessing are then grouped according to co-occurrence statistics which have been derived from a newspaper corpus . these co-occurrence statistics concern typical noun phrases as they appear in newspaper texts .

toward a task-based gold standard for evaluation of np chunks and technical terms
we propose a gold standard for evaluating two types of information extraction output -- noun phrase ( np ) chunks ( abney 1991 ; ramshaw and marcus 1995 ) and technical terms ( justeson and katz 1995 ; daille 2000 ; jacquemin 2002 ) . the gold standard is built around the notion that since different semantic and syntactic variants of terms are arguably correct , a fully satisfactory assessment of the quality of the output must include task-based evaluation . we conducted an experiment that assessed subjects choice of index terms in an information access task . subjects showed significant preference for index terms that are longer , as measured by number of words , and more complex , as measured by number of prepositions . these terms , which were identified by a human indexer , serve as the gold standard . the experimental protocol is a reliable and rigorous method for evaluating the quality of a set of terms . an important advantage of this task-based evaluation is that a set of index terms which is different than the gold standard can win by providing better information access than the gold standard itself does . and although the individual human subject experiments are time consuming , the experimental interface , test materials and data analysis programs are completely re-usable .

the effect of translation quality in mt-based cross-language information retrieval
this paper explores the relationship between the translation quality and the retrieval effectiveness in machine translation ( mt ) based cross-language information retrieval ( clir ) . to obtain mt systems of different translation quality , we degrade a rule-based mt system by decreasing the size of the rule base and the size of the dictionary . we use the degraded mt systems to translate queries and submit the translated queries of varying quality to the ir system . retrieval effectiveness is found to correlate highly with the translation quality of the queries . we further analyze the factors that affect the retrieval effectiveness . title queries are found to be preferred in mt-based clir . in addition , dictionary-based degradation is shown to have stronger impact than rule-based degradation in mt-based clir .

extractive summarization using inter- and intra- event relevance
event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe . each event has its own internal structure , and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally . in this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering . we then apply pagerank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived . experiments on the duc 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate . it also reveals that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like word-net .

context comparison as a minimum cost flow problem
comparing word contexts is a key component of many nlp tasks , but rarely is it used in conjunction with additional ontological knowledge . one problem is that the amount of overhead required can be high . in this paper , we provide a graphical method which easily combines an ontology with contextual information . we take advantage of the intrinsic graphical structure of an ontology for representing a context . in addition , we turn the ontology into a metric space , such that subgraphs within it , which represent contexts , can be compared . we develop two variants of our graphical method for comparing contexts . our analysis indicates that our method performs the comparison efficiently and offers a competitive alternative to non-graphical methods .

a bayesian belief updating model of phonetic recalibration and selective
the mapping from phonetic categories to acoustic cue values is highly flexible , and adapts rapidly in response to exposure . there is currently , however , no theoretical framework which captures the range of this adaptation . we develop a novel approach to modeling phonetic adaptation via a belief-updating model , and demonstrate that this model naturally unifies two adaptation phenomena traditionally considered to be distinct .

toward hierarchical models for statistical machine translation of lehrstuhl fur informatik vi ,
in statistical machine translation , correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models . existing statistical systems for mt often treat different derivatives of the same lemma as if they were independent of each other . in this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives . we do this along two directions : usage of hierarchical lexicon

online gaming for crowd-sourcing phrase-equivalents
we propose the use of a game with a purpose ( gwap ) to facilitate crowd-sourcing of phraseequivalents , as an alternative to expert or paid crowd-sourcing . doodling is an online multiplayer game , in which one player ( drawer ) , draws pictures on a shared board to get the other players ( guessers ) to guess the meaning behind an assigned phrase . in this paper we describe the system and results from several experiments intended to improve the quality of information generated by the play . in addition , we describe the mechanism by which we take candidate phrases generated during the games and filter out true phrase equivalents . we expect that , at scale , this game will be more cost-efficient than paid mechanisms for a similar task , and demonstrate this by comparing the productivity of an hour of game play to an equivalent crowd-sourced amazon mechanical turk task to produce phrase-equivalents over one week .

analysis of selective strategies to build a dependency-analyzed corpus
this paper discusses sampling strategies for building a dependency-analyzed corpus and analyzes them with different kinds of corpora . we used the kyoto text corpus , a dependency-analyzed corpus of newspaper articles , and prepared the ipal corpus , a dependency-analyzed corpus of example sentences in dictionaries , as a new and different kind of corpus . the experimental results revealed that the length of the test set controlled the accuracy and that the longest-first strategy was good for an expanding corpus , but this was not the case when constructing a corpus from scratch .

a new string-to-dependency machine translation algorithm with a target dependency language model
in this paper , we propose a novel string-todependency algorithm for statistical machine translation . with this new framework , we employ a target dependency language model during decoding to exploit long distance word relations , which are unavailable with a traditional n-gram language model . our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in bleu and 2.53 point improvement in ter compared to a standard hierarchical string-tostring system on the nist 04 chinese-english evaluation set .

evaluation in a translation learner corpus
the realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the mt community . however , research in this field , by now , has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline , namely translation studies , but also failed to provide a deeper understanding of the nature of '' translation errors '' and '' translation quality '' . this paper presents an empirical take on the latter concept , translation quality , by comparing human and automatic evaluations of learner translations in the kopte corpus . we will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation . moreover , by applying well-established mt evaluation scores , namely bleu and meteor , to kopte learner translations that were graded by a human expert , we hope to shed light on properties ( and potential shortcomings ) of these scores .

source language categorization for improving a speech into sign language translation system
this paper describes a categorization module for improving the performance of a spanish into spanish sign language ( lse ) translation system . this categorization module replaces spanish words with associated tags . when implementing this module , several alternatives for dealing with non-relevant words have been studied . nonrelevant words are spanish words not relevant in the translation process . the categorization module has been incorporated into a phrase-based system and a statistical finite state transducer ( sfst ) . the evaluation results reveal that the bleu has increased from 69.11 % to 78.79 % for the phrase-based system and from 69.84 % to 75.59 % for the sfst .

supervised word sense disambiguation with support vector machines and multiple knowledge sources
we participated in the senseval-3 english lexical sample task and multilingual lexical sample task . we adopted a supervised learning approach with support vector machines , using only the official training data provided . no other external resources were used . the knowledge sources used were partof-speech of neighboring words , single words in the surrounding context , local collocations , and syntactic relations . for the translation and sense subtask of the multilingual lexical sample task , the english sense given for the target word was also used as an additional knowledge source . for the english lexical sample task , we obtained fine-grained and coarse-grained score ( for both recall and precision ) of 0.724 and 0.788 respectively . for the multilingual lexical sample task , we obtained recall ( and precision ) of 0.634 for the translation subtask , and 0.673 for the translation and sense subtask .

bootstrapped named entity recognition for product attribute extraction duangmanee ( pew ) putthividhya
we present a named entity recognition ( ner ) system for extracting product attributes and values from listing titles . information extraction from short listing titles present a unique challenge , with the lack of informative context and grammatical structure . in this work , we combine supervised ner with bootstrapping to expand the seed list , and output normalized results . focusing on listings from ebays clothing and shoes categories , our bootstrapped ner system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands , as well as identifying novel brands . among the top 300 new brands predicted , our system achieves 90.33 % precision . to output normalized attribute values , we explore several string comparison algorithms and found n-gram substring matching to work well in practice .

directl : a language-independent approach to transliteration
we present directl : an online discriminative sequence prediction model that employs a many-to-many alignment between target and source . our system incorporates input segmentation , target character prediction , and sequence modeling in a unified dynamic programming framework . experimental results suggest that directl is able to independently discover many of the language-specific regularities in the training data .

linefeed insertion into japanese spoken monologue for captioning
to support the real-time understanding of spoken monologue such as lectures and commentaries , the development of a captioning system is required . in monologues , since a sentence tends to be long , each sentence is often displayed in multi lines on one screen , it is necessary to insert linefeeds into a text so that the text becomes easy to read . this paper proposes a technique for inserting linefeeds into a japanese spoken monologue text as an elemental technique to generate the readable captions . our method appropriately inserts linefeeds into a sentence by machine learning , based on the information such as dependencies , clause boundaries , pauses and line length . an experiment using japanese speech data has shown the effectiveness of our technique .

adapting to personality over time : examining the effectiveness of dialogue policy progressions in task-oriented interaction
this paper explores dialogue adaptation over repeated interactions within a taskoriented human tutorial dialogue corpus . we hypothesize that over the course of four tutorial dialogue sessions , tutors adapt their strategies based on the personality of the student , and in particular to student introversion or extraversion . we model changes in strategy over time and use them to predict how effectively the tutorial interactions support student learning . the results suggest that students leaning toward introversion learn more effectively with a minimal amount of interruption during task activity , but occasionally require a tutor prompt before voicing uncertainty ; on the other hand , students tending toward extraversion benefit significantly from increased interaction , particularly through tutor prompts for reflection on task activity . this line of investigation will inform the development of future user-adaptive dialogue systems .

parameter estimation for agenda-based user simulation
this paper presents an agenda-based user simulator which has been extended to be trainable on real data with the aim of more closely modelling the complex rational behaviour exhibited by real users . the trainable part is formed by a set of random decision points that may be encountered during the process of receiving a system act and responding with a user act . a samplebased method is presented for using real user data to estimate the parameters that control these decisions . evaluation results are given both in terms of statistics of generated user behaviour and the quality of policies trained with different simulators . compared to a handcrafted simulator , the trained system provides a much better fit to corpus data and evaluations suggest that this better fit should result in improved dialogue performance .

construction of english mwe dictionary and its application to pos tagging
this paper reports our ongoing project for constructing an english multiword expression ( mwe ) dictionary and nlp tools based on the developed dictionary . we extracted functional mwes from the english part of wiktionary , annotated the penn treebank ( ptb ) with mwe information , and conducted pos tagging experiments . we report how the mwe annotation is done on ptb and the results of pos and mwe tagging experiments .

anja belz eric kow
the grec-msr task at generation challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from wikipedia . three teams submitted one system each , and we additionally created four baseline systems . systems were tested automatically using existing intrinsic metrics . we also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools . in addition , systems were tested in an intrinsic evaluation involving human judges . this report describes the grec-msr task and the evaluation methods applied , gives brief descriptions of the participating systems , and presents the evaluation results .

a fast and accurate dependency parser using neural networks
almost all current dependency parsers classify based on millions of sparse indicator features . not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly . in this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser . because this classifier learns and uses just a small number of dense features , it can work very fast , while achieving an about 2 % improvement in unlabeled and labeled attachment scores on both english and chinese datasets . concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the english penn treebank .

going beyond shallow semantics
shallow semantic analyzers , such as semantic role labeling and sense tagging , are increasing in accuracy and becoming commonplace . however , they only provide limited and local representations of local words and individual predicate-argument structures . this talk will address some of the current challenges in producing deeper , connected representations of eventualities . available resources , such as verbnet , framenet and timebank , that can assist in this process will also be discussed , as well as some of their limitations . speakers bio martha palmer is a full professor at the university of colorado with joint appointments in linguistics and computer science and is an institute of cognitive science faculty fellow . she recently won a boulder faculty assembly 2010 research award . beginning with her dissertation work at edinburgh and her first job as a research scientist at unisys , her research has been focused on trying to capture the meanings of words in representations that the computer can use to build up meanings of complex sentences and documents . these representations can in turn be used to improve the computers ability to perform question answering , information retrieval , and machine translation . current approaches rely on techniques for applying supervised machine learning algorithms , which use vast amounts of annotated training data .

four student nlp projects for low-resource languages
this paper describes a local effort to bridge the gap between computational and documentary linguistics by teaching students and young researchers in computational linguistics about doing research and developing systems for low-resource languages . we describe four student software projects developed within one semester . the projects range from a front-end for building small-vocabulary speech recognition systems , to a broad-coverage ( more than 1000 languages ) language identification system , to language-specific systems : a lemmatizer for the mayan language uspanteko and named entity recognition systems for both slovak and persian . teaching efforts such as these are an excellent way to develop not only tools for low-resource languages , but also computational linguists well-equipped to work on endangered and low-resource languages .

classifying ellipsis in dialogue : a machine learning approach
this paper presents a machine learning approach to bare sluice disambiguation in dialogue . we extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic horn clauses . we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , a rule-based learning algorithm , and timbl , a memory-based system . both learners perform well , yielding similar success rates of approx 90 % . the results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our horn clauses can be learnt automatically from these features .

stochastic iterative alignment for machine translation evaluation
a number of metrics for automatic evaluation of machine translation have been proposed in recent years , with some metrics focusing on measuring the adequacy of mt output , and other metrics focusing on fluency . adequacy-oriented metrics such as bleu measure n-gram overlap of mt outputs and their references , but do not represent sentence-level information . in contrast , fluency-oriented metrics such as rouge-w compute longest common subsequences , but ignore words not aligned by the lcs . we propose a metric based on stochastic iterative string alignment ( sia ) , which aims to combine the strengths of both approaches . we compare sia with existing metrics , and find that it outperforms them in overall evaluation , and works specially well in fluency evaluation .

named entity transliteration and discovery from multilingual comparable
named entity recognition ( ner ) is an important part of many natural language processing tasks . most current approaches employ machine learning techniques and require supervised data . however , many languages lack such resources . this paper presents an algorithm to automatically discover named entities ( nes ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language . we observe that nes have similar time distributions across such corpora , and that they are often transliterated , and develop an algorithm that exploits both iteratively . the algorithm makes use of a new , frequency based , metric for time distributions and a resource free discriminative approach to transliteration . we evaluate the algorithm on an english-russian corpus , and show high level of nes discovery in russian .

a provably correct learning algorithm for latent-variable pcfgs
we introduce a provably correct learning algorithm for latent-variable pcfgs . the algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of em applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition . experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .

altn : word alignment features for cross-lingual textual entailment
we present a supervised learning approach to cross-lingual textual entailment that explores statistical word alignment models to predict entailment relations between sentences written in different languages . our approach is language independent , and was used to participate in the clte task ( task # 8 ) organized within semeval 2013 ( negri et al , 2013 ) . the four runs submitted , one for each language combination covered by the test data ( i.e . spanish/english , german/english , french/english and italian/english ) , achieved encouraging results . in terms of accuracy , performance ranges from 38.8 % ( for german/english ) to 43.2 % ( for italian/english ) . on the italian/english and spanish/english test sets our systems ranked second among five participants , close to the top results ( respectively 43.4 % and 45.4 % ) .

monte carlo inference and maximization for phrase-based translation
recent advances in statistical machine translation have used beam search for approximate np-complete inference within probabilistic translation models . we present an alternative approach of sampling from the posterior distribution defined by a translation model . we define a novel gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution . in doing so we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as finding the maximum probability translation and minimum expected risk training and decoding .

automatic evaluation of linguistic quality in multi-document
to date , few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization . we present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text . we train and test linguistic quality models on consecutive years of nist evaluation data in order to show the generality of results . for grammaticality , the best results come from a set of syntactic features . focus , coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences , coreference information , and summarization specific features . our best results are 90 % accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70 % for ranking summaries of a specific input .

chinese chunking with another type of spec
spec is a critical issue for automatic chunking . this paper proposes a solution of chinese chunking with another type of spec , which is not derived from a complete syntactic tree but only based on the un-bracketed , pos tagged corpus . with this spec , a chunked data is built and hmm is used to build the chunker . tblbased error correction is used to further improve chunking performance . the average chunk length is about 1.38 tokens , f measure of chunking achieves 91.13 % , labeling accuracy alone achieves 99.80 % and the ratio of crossing brackets is 2.87 % . we also find that the hardest point of chinese chunking is to identify the chunking boundary inside noun-noun sequences1 .

unsupervised semantic parsing hoifung poon pedro domingos
we present the first unsupervised approach to the problem of learning a semantic parser , using markov logic . our usp system transforms dependency trees into quasi-logical forms , recursively induces lambda forms from these , and clusters them to abstract away syntactic variations of the same meaning . the map semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them . we evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions . usp substantially outperforms textrunner , dirt and an informed baseline on both precision and recall on this task .

outilex , a linguistic platform for text processing
we present outilex , a generalist linguistic platform for text processing . the platform includes several modules implementing the main operations for text processing and is designed to use large-coverage language resources . these resources ( dictionaries , grammars , annotated texts ) are formatted into xml , in accordance with current standards . evaluations on efficiency are given .

towards large-scale non-taxonomic relation extraction : estimating the precision of rote extractors enrique alfonseca maria ruiz-casado manabu okumura pablo castells
in this paper , we describe a rote extractor that learns patterns for finding semantic relations in unrestricted text , with new procedures for pattern generalisation and scoring . an improved method for estimating the precision of the extracted patterns is presented . we show that our method approximates the precision values as evaluated by hand much better than the procedure traditionally used in rote extractors .

rapid development of a corpus with discourse annotations using two-stage crowdsourcing yahoo japan corporation
we present a novel approach for rapidly developing a corpus with discourse annotations using crowdsourcing . although discourse annotations typically require much time and cost owing to their complex nature , we realize discourse annotations in an extremely short time while retaining good quality of the annotations by crowdsourcing two annotation subtasks . in fact , our experiment to create a corpus comprising 30,000 japanese sentences took less than eight hours to run . based on this corpus , we also develop a supervised discourse parser and evaluate its performance to verify the usefulness of the acquired corpus .

pageranking wordnet synsets : an application to opinion mining
this paper presents an application of pagerank , a random-walk model originally devised for ranking web search results , to ranking wordnet synsets in terms of how strongly they possess a given semantic property . the semantic properties we use for exemplifying the approach are positivity and negativity , two properties of central importance in sentiment analysis . the idea derives from the observation that wordnet may be seen as a graph in which synsets are connected through the binary relation a term belonging to synset sk occurs in the gloss of synset si , and on the hypothesis that this relation may be viewed as a transmitter of such semantic properties . the data for this relation can be obtained from extended wordnet , a publicly available sensedisambiguated version of wordnet . we argue that this relation is structurally akin to the relation between hyperlinked web pages , and thus lends itself to pagerank analysis . we report experimental results supporting our intuitions .

text specificity and impact on quality of news summaries
in our work we use an existing classifier to quantify and analyze the level of specific and general content in news documents and their human and automatic summaries . we discover that while human abstracts contain a more balanced mix of general and specific content , automatic summaries are overwhelmingly specific . we also provide an analysis of summary specificity and the summary quality scores assigned by people . we find that too much specificity could adversely affect the quality of content in the summary . our findings give strong evidence for the need for a new task in abstractive summarization : identification and generation of general sentences .

investigating context parameters in technology term recognition
we propose and evaluate the task of technology term recognition : a method to extract technology terms at a synchronic level from a corpus of scientific publications . the proposed method is built on the principles of terminology extraction and distributional semantics . it is realized as a regression task in a vector space model . in this method , candidate terms are first extracted from text . subsequently , using the random indexing technique , the extracted candidate terms are represented as vectors in a euclidean vector space of reduced dimensionality . these vectors are derived from the frequency of co-occurrences of candidate terms and words in windows of text surrounding candidate terms in the input corpus ( context window ) . the constructed vector space and a set of manually tagged technology terms ( reference vectors ) in a k-nearest neighbours regression framework is then used to identify terms that signify technology concepts . we examine a number of factors that play roles in the performance of the proposed method , i.e . the configuration of context windows , neighborhood size ( k ) selection , and reference vector size .

evaluating automatic extraction of rules for sentence plan construction
the freely available sparky sentence planner uses hand-written weighted rules for sentence plan construction , and a useror domain-specific second-stage ranker for sentence plan selection . however , coming up with sentence plan construction rules for a new domain can be difficult . in this paper , we automatically extract sentence plan construction rules from the rst-dt corpus . in our rules , we use only domainindependent features that are available to a sentence planner at runtime . we evaluate these rules , and outline ways in which they can be used for sentence planning . we have integrated them into a revised version of sparky .

natural reference to objects in a visual domain kees van deemter
this paper discusses the basic structures necessary for the generation of reference to objects in a visual scene . we construct a study designed to elicit naturalistic referring expressions to relatively complex objects , and find aspects of reference that have not been accounted for in work on referring expression generation ( reg ) . this includes reference to object parts , size comparisons without crisp measurements , and the use of analogies . by drawing on research in cognitive science , neurophysiology , and psycholinguistics , we begin developing the input structure and background knowledge necessary for an algorithm capable of generating the kinds of reference we observe .

semanticrank : ranking keywords and sentences using semantic graphs
# ! # $ % & ' $ ! ! $ ! $ $ ! # & ( ! $ # ) & & * $ * $ $ + & semanticrank $ , ! ! # # & semantic graph $ ! , $ , # $ * !

a morphological analyzer and generator for the arabic dialects
we present magead , a morphological analyzer and generator for the arabic language family . our work is novel in that it explicitly addresses the need for processing the morphology of the dialects . magead performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects . we present a detailed evaluation of magead .

a best-first probabilistic shift-reduce parser
recently proposed deterministic classifierbased parsers ( nivre and scholz , 2004 ; sagae and lavie , 2005 ; yamada and matsumoto , 2003 ) offer attractive alternatives to generative statistical parsers . deterministic parsers are fast , efficient , and simple to implement , but generally less accurate than optimal ( or nearly optimal ) statistical parsers . we present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers . the parsing model is essentially the same as one previously used for deterministic parsing , but the parser performs a best-first search instead of a greedy search . using the standard sections of the wsj corpus of the penn treebank for training and testing , our parser has 88.1 % precision and 87.8 % recall ( using automatically assigned part-of-speech tags ) . perhaps more interestingly , the parsing model is significantly different from the generative models used by other wellknown accurate parsers , allowing for a simple combination that produces precision and recall of 90.9 % and 90.7 % , respectively .

some issues on the normalization of a corpus of products reviews in
this paper describes the analysis of different kinds of noises in a corpus of products reviews in brazilian portuguese . case folding , punctuation , spelling and the use of internet slang are the major kinds of noise we face . after noting the effect of these noises on the pos tagging task , we propose some procedures to minimize them .

learning to link entities with knowledge base
this paper address the problem of entity linking . specifically , given an entity mentioned in unstructured texts , the task is to link this entity with an entry stored in the existing knowledge base . this is an important task for information extraction . it can serve as a convenient gateway to encyclopedic information , and can greatly improve the web users experience . previous learning based solutions mainly focus on classification framework . however , its more suitable to consider it as a ranking problem . in this paper , we propose a learning to rank algorithm for entity linking . it effectively utilizes the relationship information among the candidates when ranking . the experiment results on the tac 20091 dataset demonstrate the effectiveness of our proposed framework . the proposed method achieves 18.5 % improvement in terms of accuracy over the classification models for those entities which have corresponding entries in the knowledge base .

on the information conveyed by discourse markers
discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text . using the penn discourse treebank , we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives , and whether the specificity of discourse relations reflects general cognitive biases for establishing coherence . we also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy . this will open a way to the computational modeling of discourse processing .

near-synonym choice in an intelligent thesaurus
an intelligent thesaurus assists a writer with alternative choices of words and orders them by their suitability in the writing context . in this paper we focus on methods for automatically choosing nearsynonyms by their semantic coherence with the context . our statistical method uses the web as a corpus to compute mutual information scores . evaluation experiments show that this method performs better than a previous method on the same task . we also propose and evaluate two more methods , one that uses anticollocations , and one that uses supervised learning . to asses the difficulty of the task , we present results obtained by human judges .

detecting multi-word expressions improves word sense disambiguation
multi-word expressions ( mwes ) are prevalent in text and are also , on average , less polysemous than mono-words . this suggests that accurate mwe detection should lead to a nontrivial improvement in word sense disambiguation ( wsd ) . we show that a straightforward mwe detection strategy , due to arranz et al ( 2005 ) , can increase a wsd algorithms baseline f-measure by 5 percentage points . our measurements are consistent with arranzs , and our study goes further by using a portion of the semcor corpus containing 12,449 mwes - over 30 times more than the approximately 400 used by arranz . we also show that perfect mwe detection over semcor only nets a total 6 percentage point increase in wsd f-measure ; therefore there is little room for improvement over the results presented here . we provide our mwe detection algorithms , along with a general detection framework , in a free , open-source java library called jmwe . multi-word expressions ( mwes ) are prevalent in text . this is important for the classic task of word sense disambiguation ( wsd ) ( agirre and edmonds , 2007 ) , in which an algorithm attempts to assign to each word in a text the appropriate entry from a sense inventory . a wsd algorithm that can not correctly detect the mwes that are listed in its sense inventory will not only miss those sense assignments , it will also spuriously assign senses to mwe constituents that themselves have sense entries , dealing a double-blow to wsd performance . beyond this penalty , mwes listed in a sense inventory also present an opportunity to wsd algorithms - they are , on average , less polysemous than mono-words .

experiments to improve named entity recognition on turkish tweets
social media texts are significant information sources for several application areas including trend analysis , event monitoring , and opinion mining . unfortunately , existing solutions for tasks such as named entity recognition that perform well on formal texts usually perform poorly when applied to social media texts . in this paper , we report on experiments that have the purpose of improving named entity recognition on turkish tweets , using two different annotated data sets . in these experiments , starting with a baseline named entity recognition system , we adapt its recognition rules and resources to better fit twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources , and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on turkish tweets . the evaluation results of the system with these different settings are provided with discussions of these results .

efficient solutions for word reordering in german-english phrase-based statistical machine translation
despite being closely related languages , german and english are characterized by important word order differences . longrange reordering of verbs , in particular , represents a real challenge for state-of-theart smt systems and is one of the main reasons why translation quality is often so poor in this language pair . in this work , we review several solutions to improve the accuracy of german-english word reordering while preserving the efficiency of phrase-based decoding . among these , we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena . through an extensive evaluation including diverse translation quality metrics , we show that these solutions can significantly narrow the gap between phrase-based and hierarchical smt .

assas-band , an affix-exception-list based urdu stemmer
both inflectional and derivational morphology lead to multiple surface forms of a word . stemming reduces these forms back to its stem or root , and is a very useful tool for many applications . there has not been any work reported on urdu stemming . the current work develops an urdu stemmer or assas-band and improves the performance using more precise affix based exception lists , instead of the conventional lexical lookup employed for developing stemmers in other languages . testing shows an accuracy of 91.2 % . further enhancements are also suggested .

unsupervised word segmentation for sesotho using adaptor grammars
this paper describes a variety of nonparametric bayesian models of word segmentation based on adaptor grammars that model different aspects of the input and incorporate different kinds of prior knowledge , and applies them to the bantu language sesotho . while we find overall word segmentation accuracies lower than these models achieve on english , we also find some interesting differences in which factors contribute to better word segmentation . specifically , we found little improvement to word segmentation accuracy when we modeled contextual dependencies , while modeling morphological structure did improve segmentation accuracy .

stochastic contextual edit distance and probabilistic fsts
string similarity is most often measured by weighted or unweighted edit distance d ( x , y ) . ristad and yianilos ( 1998 ) defined stochastic edit distancea probability distribution p ( y | x ) whose parameters can be trained from data . we generalize this so that the probability of choosing each edit operation can depend on contextual features . we show how to construct and train a probabilistic finite-state transducer that computes our stochastic contextual edit distance . to illustrate the improvement from conditioning on context , we model typos found in social media text .

a context-based model for sentiment analysis in twitter
most of the recent literature on sentiment analysis over twitter is tied to the idea that the sentiment is a function of an incoming tweet . however , tweets are filtered through streams of posts , so that a wider context , e.g . a topic , is always available . in this work , the contribution of this contextual information is investigated . we modeled the polarity detection problem as a sequential classification task over streams of tweets . a markovian formulation of the support vector machine discriminative model as embodied by the svm hmm algorithm has been here employed to assign the sentiment polarity to entire sequences . the experimental evaluation proves that sequential tagging effectively embodies evidence about the contexts and is able to reach a relative increment in detection accuracy of around 20 % in f1 measure . these results are particularly interesting as the approach is flexible and does not require manually coded resources .

generating learner-like morphological errors in russian
to speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data , we describe a linguistically-informed method for generating learner-like morphological errors , focusing on russian . we outline a procedure to select likely errors , relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context .

for the sake of simplicity : unsupervised extraction of lexical simplifications from wikipedia
we report on work in progress on extracting lexical simplifications ( e.g. , collaborate work together ) , focusing on utilizing edit histories in simple english wikipedia for this task . we consider two main approaches : ( 1 ) deriving simplification probabilities via an edit model that accounts for a mixture of different operations , and ( 2 ) using metadata to focus on edits that are more likely to be simplification operations . we find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list .

automatic identification of sentiment vocabulary : exploiting low association with known sentiment terms
we describe an extension to the technique for the automatic identification and labeling of sentiment terms described in turney ( 2002 ) and turney and littman ( 2002 ) . their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level . we add a second assumption , namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level . this additional assumption allows us to identify sentiment-bearing terms very reliably . we then use these newly identified terms in various scenarios for the sentiment classification of sentences . we show that our approach outperforms turneys original approach . combining our approach with a naive bayes bootstrapping method yields a further small improvement of classifier performance . we finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data .

genedisease association extraction by text mining and network
biomedical relations play an important role in biological processes . in this work , we combine information filtering , grammar parsing and network analysis for gene-disease association extraction . the proposed method first extracts sentences potentially containing information about gene-diseases interactions based on maximum entropy classifier with topic features . and then probabilistic contextfree grammars is applied for gene-disease association extraction . the network of genes and the disease is constituted by the extracted interactions , network centrality metrics are used for calculating the importance of each gene . we used breast cancer as testing disease for system evaluation . the 31 top ranked genes and diseases by the weighted degree , betweenness , and closeness centralities have been checked relevance with breast cancer through ncbi database . the evaluation showed 83.9 % accuracy for the testing genes and diseases , 74.2 % accuracy for the testing genes .

using nlg and sensors to support personal narrative for children with complex communication needs rolf black joe reddington , ehud reiter , nava tintarev annalu waller
we are building a tool that helps children with complex communication needs1 ( ccn ) to create stories about their day at school . the tool uses natural language generation ( nlg ) technology to create a draft story based on sensor data of the childs activities , which the child can edit . this work is still in its early stages , but we believe it has great potential to support interactive personal narrative which is not well supported by current augmentative and alternative communication ( aac ) tools .

extraction and approximation of numerical attributes from the web
we present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the web . given an object-attribute pair , we discover and analyze attribute information for a set of comparable objects in order to infer the desired value . this allows us to approximate the desired numerical values even when no exact values can be found in the text . our framework makes use of relation defining patterns and wordnet similarity information . first , we obtain from the web and wordnet a list of terms similar to the given object . then we retrieve attribute values for each term in this list , and information that allows us to compare different objects in the list and to infer the attribute value range . finally , we combine the retrieved data for all terms from the list to select or approximate the requested value . we evaluate our method using automated question answering , wordnet enrichment , and comparison with answers given in wikipedia and by leading search engines . in all of these , our framework provides a significant improvement .

icarus an extensible graphical search tool for dependency treebanks
we present icarus , a versatile graphical search tool to query dependency treebanks . search results can be inspected both quantitatively and qualitatively by means of frequency lists , tables , or dependency graphs . icarus also ships with plugins that enable it to interface with tool chains running either locally or remotely .

recognising entailment within discourse
texts are commonly interpreted based on the entire discourse in which they are situated . discourse processing has been shown useful for inference-based application ; yet , most systems for textual entailment a generic paradigm for applied inference have only addressed discourse considerations via off-the-shelf coreference resolvers . in this paper we explore various discourse aspects in entailment inference , suggest initial solutions for them and investigate their impact on entailment performance . our experiments suggest that discourse provides useful information , which significantly improves entailment inference , and should be better addressed by future entailment systems .

combination of arabic preprocessing schemes for statistical machine translation
statistical machine translation is quite robust when it comes to the choice of input representation . it only requires consistency between training and testing . as a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation . this is even more so for morphologically rich languages such as arabic . in this paper , we study the effect of different word-level preprocessing schemes for arabic on the quality of phrase-based statistical machine translation . we also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .

using domain similarity for performance estimation vincent van asch
many natural language processing ( nlp ) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development . this makes it hard to develop nlp tools for domains for which annotated corpora are not available . this paper explores a number of metrics that attempt to predict the cross-domain performance of an nlp tool through statistical inference . we apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of nlp tool . we find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an nlp tool on out-of-domain data . the approach also provides a way to quantify the difference between domains .

turksent : a sentiment annotation tool for social media fatih samet cetin and meltem yank turkcell global bilgi turkcell global bilgi
in this paper , we present an annotation tool developed specifically for manual sentiment analysis of social media posts . the tool provides facilities for general and target based opinion marking on different type of posts ( i.e . comparative , ironic , conditional ) with a web based ui which supports synchronous annotation . it is also designed as a saas ( software as a service ) . the tools outstanding features are easy and fast annotation interface , detailed sentiment levels , multi-client support , easy to manage administrative modules and linguistic annotation capabilities .

an i-vector based approach to compact multi-granularity topic spaces representation of textual documents
various studies highlighted that topicbased approaches give a powerful spoken content representation of documents . nonetheless , these documents may contain more than one main theme , and their automatic transcription inevitably contains errors . in this study , we propose an original and promising framework based on a compact representation of a textual document , to solve issues related to topic space granularity . firstly , various topic spaces are estimated with different numbers of classes from a latent dirichlet allocation . then , this multiple topic space representation is compacted into an elementary segment , called c-vector , originally developed in the context of speaker recognition . experiments are conducted on the decoda corpus of conversations . results show the effectiveness of the proposed multi-view compact representation paradigm . our identification system reaches an accuracy of 85 % , with a significant gain of 9 points compared to the baseline ( best single topic space configuration ) .

dual coordinate descent algorithms for efficient large margin structured prediction ming-wei chang wen-tau yih
due to the nature of complex nlp problems , structured prediction algorithms have been important modeling tools for a wide range of tasks . while there exists evidence showing that linear structural support vector machine ( ssvm ) algorithm performs better than structured perceptron , the ssvm algorithm is still less frequently chosen in the nlp community because of its relatively slow training speed . in this paper , we propose a fast and easy-toimplement dual coordinate descent algorithm for ssvms . unlike algorithms such as perceptron and stochastic gradient descent , our method keeps track of dual variables and updates the weight vector more aggressively . as a result , this training process is as efficient as existing online learning methods , and yet derives consistently better models , as evaluated on four benchmark nlp datasets for part-ofspeech tagging , named-entity recognition and dependency parsing .

gender inference of twitter users in non-english contexts
while much work has considered the problem of latent attribute inference for users of social media such as twitter , little has been done on non-english-based content and users . here , we conduct the first assessment of latent attribute inference in languages beyond english , focusing on gender inference . we find that the gender inference problem in quite diverse languages can be addressed using existing machinery . further , accuracy gains can be made by taking language-specific features into account . we identify languages with complex orthography , such as japanese , as difficult for existing methods , suggesting a valuable direction for future research .

fast joint compression and summarization via graph cuts
extractive summarization typically uses sentences as summarization units . in contrast , joint compression and summarization can use smaller units such as words and phrases , resulting in summaries containing more information . the goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint . we propose an efficient decoding algorithm for fast compressive summarization using graph cuts . our approach first relaxes the length constraint using lagrangian relaxation . then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem , which can be solved efficiently using graph max-flow/min-cut . since finding the tightest lower bound suffers from local optimality , we use convex relaxation for initialization . experimental results on tac2008 dataset demonstrate our method achieves competitive rouge score and has good readability , while is much faster than the integer linear programming ( ilp ) method .

investigating automatic alignment methods for slide generation from
in this paper we investigate the task of automatic generation of slide presentations from academic papers , focusing initially on slide to paper alignment . we compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches , such as tf-idf term weighting and query expansion . our best aligner achieves an accuracy of 75 % and our findings show that for this application , average tf-idf scoring performs more poorly than a simpler method based on the number of matched terms , and query expansion degrades aligner performance .

identifying segment topics in medical dictations and artificial intelligence
in this paper , we describe the use of lexical and semantic features for topic classification in dictated medical reports . first , we employ svm classification to assign whole reports to coarse work-type categories . afterwards , text segments and their topic are identified in the output of automatic speech recognition . this is done by assigning work-type-specific topic labels to each word based on features extracted from a sliding context window , again using svm classification utilizing semantic features . classifier stacking is then used for a posteriori error correction , yielding a further improvement in classification accuracy .

automatic generation of inter-passage links based on semantic similarity
this paper investigates the use and the prediction potential of semantic similarity measures for automatic generation of links across different documents and passages . first , the correlation between the way people link content and the results produced by standard semantic similarity measures is investigated . the relation between semantic similarity and the length of the documents is then also analysed . based on these findings a new method for link generation is formulated and tested .

study of some distance measures for language and encoding anil kumar singh
to determine how close two language models ( e.g. , n-grams models ) are , we can use several distance measures . if we can represent the models as distributions , then the similarity is basically the similarity of distributions . and a number of measures are based on information theoretic approach . in this paper we present some experiments on using such similarity measures for an old natural language processing ( nlp ) problem . one of the measures considered is perhaps a novel one , which we have called mutual cross entropy . other measures are either well known or based on well known measures , but the results obtained with them vis-avis one-another might help in gaining an insight into how similarity measures work in practice . the first step in processing a text is to identify the language and encoding of its contents . this is a practical problem since for many languages , there are no universally followed text encoding standards . the method we have used in this paper for language and encoding identification uses pruned character n-grams , alone as well augmented with word n-grams . this method seems to give results comparable to other methods .

who wrote what where : analyzing the content of human and automatic summaries
abstractive summarization has been a longstanding and long-term goal in automatic summarization , because systems that can generate abstracts demonstrate a deeper understanding of language and the meaning of documents than systems that merely extract sentences from those documents . genest ( 2009 ) showed that summaries from the top automatic summarizers are judged as comparable to manual extractive summaries , and both are judged to be far less responsive than manual abstracts , as the state of the art approaches the limits of extractive summarization , it becomes even more pressing to advance abstractive summarization . however , abstractive summarization has been sidetracked by questions of what qualifies as important information , and how do we find it the guided summarization task introduced at the text analysis conference 2010 attempts to neutralize both of these problems by introducing topic categories and lists of aspects that a responsive summary should address . this design results in more similar human models , giving the automatic summarizers a more focused target to pursue , and also provides detailed diagnostics of summary content , which can can help build better meaningoriented summarization systems .

a high-performance syntactic and semantic dependency parser anders bjorkelund bernd bohnet love hafdell pierre nugues
this demonstration presents a highperformance syntactic and semantic dependency parser . the system consists of a pipeline of modules that carry out the tokenization , lemmatization , part-of-speech tagging , dependency parsing , and semantic role labeling of a sentence . the systems two main components draw on improved versions of a state-of-the-art dependency parser ( bohnet , 2009 ) and semantic role labeler ( bjorkelund et al , 2009 ) developed independently by the authors . the system takes a sentence as input and produces a syntactic and semantic annotation using the conll 2009 format . the processing time needed for a sentence typically ranges from 10 to 1000 milliseconds . the predicateargument structures in the final output are visualized in the form of segments , which are more intuitive for a user .

bart : a multilingual anaphora resolution system
bart ( versley et al , 2008 ) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering . for the semeval task 1 on coreference resolution , bart runs have been submitted for german , english , and italian . bart relies on a maximum entropy-based classifier for pairs of mentions . a novel entitymention approach based on semantic trees is at the moment only supported for english .

al-bayan : an arabic question answering system for the holy quran heba abdelnasser reham mohamed maha ragab alaa mohamed bassant farouk nagwa el-makky
recently , question answering ( qa ) has been one of the main focus of natural language processing research . however , arabic question answering is still not in the mainstream . the challenges of the arabic language and the lack of resources have made it difficult to provide arabic qa systems with high accuracy . while low accuracies may be accepted for general purpose systems , it is critical in some fields such as religious affairs . therefore , there is a need for specialized accurate systems that target these critical fields . in this paper , we propose al-bayan , a new arabic qa system specialized for the holy quran . the system accepts an arabic question about the quran , retrieves the most relevant quran verses , then extracts the passage that contains the answer from the quran and its interpretation books ( tafseer ) . evaluation results on a collected dataset show that the overall system can achieve 85 % accuracy using the top-3 results .

graph-based clustering for semantic classication of onomatopoetic
this paper presents a method for semantic classication of onomatopoetic words like ( hum ) and ( clip clop ) which exist in every language , especially japanese being rich in onomatopoetic words . we used a graph-based clustering algorithm called newman clustering . the algorithm calculates a simple quality function to test whether a particular division is meaningful . the quality function is calculated based on the weights of edges between nodes . we combined two different similarity measures , distributional similarity , and orthographic similarity to calculate weights . the results obtained by using the web data showed a 9.0 % improvement over the baseline single distributional similarity measure .

entities ' sentiment relevance
sentiment relevance detection problems occur when there is a sentiment expression in a text , and there is the question of whether or not the expression is related to a given entity or , more generally , to a given situation . the paper discusses variants of the problem , and shows that it is distinct from other somewhat similar problems occurring in the field of sentiment analysis and opinion mining . we experimentally demonstrate that using the information about relevancy significantly affects the final sentiment evaluation of the entities . we then compare a set of different algorithms for solving the relevance detection problem . the most accurate results are achieved by algorithms that use certain document-level information about the target entities . we show that this information can be accurately extracted using supervised classification methods .

bridging the gap between underspecification formalisms : minimal recursion semantics as dominance constraints
minimal recursion semantics ( mrs ) is the standard formalism used in large-scale hpsg grammars to model underspecified semantics . we present the first provably efficient algorithm to enumerate the readings of mrs structures , by translating them into normal dominance constraints .

n-gram-based statistical machine translation versus syntax augmented machine translation : comparison and system combination
in this paper we compare and contrast two approaches to machine translation ( mt ) : the cmu-uka syntax augmented machine translation system ( samt ) and upc-talp n-gram-based statistical machine translation ( smt ) . samt is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree . in n-gram-based smt , the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximumentropy framework . we provide a stepby-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources for a smaller arabic-to-english translation task ( 1.5m tokens in the training corpus ) . human error analysis clarifies advantages and disadvantages of the systems under consideration . finally , we combine the output of both systems to yield significant improvements in translation quality .

a three-step transition-based system for non-projective dependency parsing
this paper presents a non-projective dependency parsing system that is transition-based and operates in three steps . the three steps include one classical method for projective dependency parsing and two inverse methods predicting separately the right and left non-projective dependencies . splitting the parsing allows to increase the scores on both projective and non-projective dependencies compared to state-of-the-art non-projective dependency parsing . moreover , each step is performed in linear time .

learning greek verb complements : addressing the class imbalance
imbalanced training sets , where one class is heavily underrepresented compared to the others , have a bad effect on the classification of rare class instances . we apply one-sided sampling for the first time to a lexical acquisition task ( learning verb complements from modern greek corpora ) to remove redundant and misleading training examples of verb nondependents and thereby balance our training set . we experiment with well-known learning algorithms to classify new examples . performance improves up to 22 % in recall and 15 % in precision after balancing the dataset1 .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

mining new word translations from comparable corpora
new words such as names , technical terms , etc appear frequently . as such , the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations . comparable corpora such as news documents of the same period from different news agencies are readily available . in this paper , we present a new approach to mining new word translations from comparable corpora , by using context information to complement transliteration information . we evaluated our approach on six months of chinese and english gigaword corpora , with encouraging results .

a multilingual multimedia indian sign language dictionary tool
this paper presents a cross platform multilingual multimedia indian sign language ( isl ) dictionary building tool . isl is a linguistically under-investigated language with no source of well documented electronic data . research on isl linguistics also gets hindered due to a lack of isl knowledge and the unavailability of any educational tools . our system can be used to associate signs corresponding to a given text . the current system also facilitates the phonological annotation of indian signs in the form of hamnosys structure . the generated hamnosys string can be given as input to an avatar module to produce an animated sign representation .

evaluation of unsupervised emotion models to textual affect recognition sunghwan mac kim and information engineering and information engineering
in this paper we present an evaluation of new techniques for automatically detecting emotions in text . the study estimates categorical model and dimensional model for the recognition of four affective states : anger , fear , joy , and sadness that are common emotions in three datasets : semeval-2007 affective text , isear ( international survey on emotion antecedents and reactions ) , and childrens fairy tales . in the first model , wordnetaffect is used as a linguistic lexical resource and three dimensionality reduction techniques are evaluated : latent semantic analysis ( lsa ) , probabilistic latent semantic analysis ( plsa ) , and non-negative matrix factorization ( nmf ) . in the second model , anew ( affective norm for english words ) , a normative database with affective terms , is employed . experiments show that a categorical model using nmf results in better performances for semeval and fairy tales , whereas a dimensional model performs better with isear .

adaptation data selection using neural language models : experiments in machine translation
data selection is an effective approach to domain adaptation in statistical machine translation . the idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora , which are then incorporated into the training data . substantial gains have been demonstrated in previous works , which employ standard ngram language models . here , we explore the use of neural language models for data selection . we hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts , which are prevalent in general-domain text . in a comprehensive evaluation of 4 language pairs ( english to german , french , russian , spanish ) , we found that neural language models are indeed viable tools for data selection : while the improvements are varied ( i.e . 0.1 to 1.7 gains in bleu ) , they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams .

learning n-best correction models from implicit user feedback in a multi-modal local search application
we describe a novel n-best correction model that can leverage implicit user feedback ( in the form of clicks ) to improve performance in a multi-modal speech-search application . the proposed model works in two stages . first , the n-best list generated by the speech recognizer is expanded with additional candidates , based on confusability information captured via user click statistics . in the second stage , this expanded list is rescored and pruned to produce a more accurate and compact n-best list . results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline , as well as other traditional n-best rescoring approaches .

exploiting language models for visual recognition
the problem of learning language models from large text corpora has been widely studied within the computational linguistic community . however , little is known about the performance of these language models when applied to the computer vision domain . in this work , we compare representative models : a window-based model , a topic model , a distributional memory and a commonsense knowledge database , conceptnet , in two visual recognition scenarios : human action recognition and object prediction . we examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images . we determine the usefulness of different language models in aiding the two visual recognition tasks . the study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset .

language computer corporation
logic forms , particular powerful logic representations presented in moldovan and rus ( 2001 ) , are simple yet highly effective . in this paper , the structure of logic forms and their generation from input text are described . the results of an evaluation comparing the logic forms generated by hand with those generated automatically are also reported . finally , we suggest some improvements to the representation used in the lfi task based on our results .

low-cost enrichment of spanish wordnet with automatically translated glosses : combining general and specialized models
this paper studies the enrichment of spanish wordnet with synset glosses automatically obtained from the english wordnet glosses using a phrase-based statistical machine translation system . we construct the english-spanish translation system from a parallel corpus of proceedings of the european parliament , and study how to adapt statistical models to the domain of dictionary definitions . we build specialized language and translation models from a small set of parallel definitions and experiment with robust manners to combine them . a statistically significant increase in performance is obtained . the best system is finally used to generate a definition for all spanish synsets , which are currently ready for a manual revision . as a complementary issue , we analyze the impact of the amount of in-domain data needed to improve a system trained entirely on out-of-domain data .

reducing dimensions of tensors in type-driven distributional semantics
compositional distributional semantics is a subfield of computational linguistics which investigates methods for representing the meanings of phrases and sentences . in this paper , we explore implementations of a framework based on combinatory categorial grammar ( ccg ) , in which words with certain grammatical types have meanings represented by multilinear maps ( i.e . multi-dimensional arrays , or tensors ) . an obstacle to full implementation of the framework is the size of these tensors . we examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task . we find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .

convolutional neural networks for sentence classification
we report on a series of experiments with convolutional neural networks ( cnn ) trained on top of pre-trained word vectors for sentence-level classification tasks . we show that a simple cnn with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task-specific vectors through fine-tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .

enhancing the expression of contrast in the sparky restaurant corpus
we show that nakatsu & whites ( 2010 ) proposed enhancements to the sparky restaurant corpus ( src ; walker et al , 2007 ) for better expressing contrast do indeed make it possible to generate better texts , including ones that make effective and varied use of contrastive connectives and discourse adverbials . after first presenting a validation experiment for naturalness ratings of src texts gathered using amazons mechanical turk , we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them . we conclude with a discussion of possible ways of improving the ranker in future work .

evaluating the pairwise string alignment of pronunciations
pairwise string alignment ( psa ) is an important general technique for obtaining a measure of similarity between two strings , used e.g. , in dialectology , historical linguistics , transliteration , and in evaluating name distinctiveness . the current study focuses on evaluating different psa methods at the alignment level instead of via the distances it induces . about 3.5 million pairwise alignments of bulgarian phonetic dialect data are used to compare four algorithms with a manually corrected gold standard . the algorithms evaluated include three variants of the levenshtein algorithm as well as the pair hidden markov model . our results show that while all algorithms perform very well and align around 95 % of all alignments correctly , there are specific qualitative differences in the ( mis ) alignments of the different algorithms .

offspring from reproduction problems : what replication failure teaches us antske fokkens and marieke van erp the european library
repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work . we present two concrete use cases involving key techniques in the nlp domain for which we show that reproducing results is still difficult . we show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted . moreover , investigating these deviations provides new insights and a deeper understanding of the examined techniques . we identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers . our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field .

usna : a dual-classifier approach to contextual sentiment analysis
this paper describes a dual-classifier approach to contextual sentiment analysis at the semeval-2013 task 2. contextual analysis of polarity focuses on a word or phrase , rather than the broader task of identifying the sentiment of an entire text . the task 2 definition includes target word spans that range in size from a single word to entire sentences . however , the context of a single word is dependent on the words surrounding syntax , while a phrase contains most of the polarity within itself . we thus describe separate treatment with two independent classifiers , outperforming the accuracy of a single classifier . our system ranked 6th out of 19 teams on sms message classification , and 8th of 23 on twitter data . we also show a surprising result that a very small amount of word context is needed for high-performance polarity extraction .

visualizing the evaluation of distance measures
this paper describes the development and use of an interface for visually evaluating distance measures . the combination of multidimensional scaling plots , histograms and tables allows for different stages of overview and detail . the interdisciplinary project rule-based search in text databases with nonstandard orthography develops a fuzzy full text search engine and uses distance measures for historical text document retrieval . this engine should provide easier text access for experts as well as interested amateurs .

principles of non-stationary hidden markov model
models . to improve its predictive power , one of hmm hypotheses , named limited history hypothesis , is usually relaxed . then higher-order hmm is built up . but there are several severe problems hampering the applications of highorder hmm , such as the problem of parameter space explosion , data sparseness problem and system resource exhaustion problem . from another point of view , this paper relaxes the other hmm hypothesis , named stationary ( time invariant ) hypothesis , makes use of time information and proposes a non-stationary hmm ( nshmm ) . this paper describes nshmm in detail , including its definition , the representation of time information , the algorithms and the parameter space and so on . moreover , to further reduce the parameter space for mobile applications , this paper proposes a variant form of nshmm ( vnshmm ) . then nshmm and vnshmm are applied to two sequence labeling tasks : pos tagging and pinyin-tocharacter conversion . experiment results show that compared with hmm , nshmm and vnshmm can greatly reduce the error rate in both of the two tasks , which proves that they have much more predictive power than hmm does .

answering clinical questions with role identification
we describe our work in progress on natural language analysis in medical questionanswering in the context of a broader medical text-retrieval project . we analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems , and describe an alternative approach whose organizing principle is the identification of semantic roles in both question and answer texts that correspond to the fields of pico format .

automatic correction of arabic text : a cascaded approach
this paper describes the error correction model that we used for the automatic correction of arabic text shared task . we employed two correction models , namely a character-level model and a casespecific model , and two punctuation recovery models , namely a simple statistical model and a crf model . our results on the development set suggest that using a cascaded correction model yields the best results .

feature selection for sentiment analysis based on content and syntax models
recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans , to approaches that use general statistical measures where features are selected solely on empirical evidence . the advantage of statistical approaches is that they are fully automatic , however , they often fail to separate features that carry sentiment from those that do not . in this paper we propose a set of new feature selection schemes that use a content and syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities . by focusing only on the subjective expressions and ignoring the entities , we can choose more salient features for document-level sentiment analysis . the results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches .

ubc-zas : a k-nn based multiclassifier system to perform wsd in a reduced dimensional vector space
in this article a multiclassifier approach for word sense disambiguation ( wsd ) problems is presented , where a set of k-nn classifiers is used to predict the category ( sense ) of each word . in order to combine the predictions generated by the multiclassifier , bayesian voting is applied . through all the classification process , a reduced dimensional vector representation obtained by singular value decomposition ( svd ) is used . each word is considered an independent classification problem , and so different parameter setting , selected after a tuning phase , is applied to each word . the approach has been applied to the lexical sample wsd subtask of semeval 2007 ( task 17 ) .

the development of a multilingual collocation dictionary
in this paper we discuss the development of a multilingual collocation dictionary for translation purposes . by collocation we mean not only set or fixed expressions including idioms , simple cooccurrences of items and metaphorical uses , but also translators paraphrases . we approach this problem from two directions . firstly we identify certain linguistic phenomena and lexicographical requirements that need to be respected in the development of such dictionaries . the second and other direction concerns the development of such dictionaries in which linguistic phenomena and lexicographic attributes are themselves a means of access to the collocations . the linguistic phenomena and lexicographical requirements concern variously placing the sense of collocations rather than headwords or other access methods at the centre of interest , together with collocation synonymy and translation equivalence , polysemy and non-reversibility of the lexis , and other more lexicographic properties such as varieties of language and regionalisms , and types of translation .

chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation human language technology and pattern recognition
in this paper , we describe a sourceside reordering method based on syntactic chunks for phrase-based statistical machine translation . first , we shallow parse the source language sentences . then , reordering rules are automatically learned from source-side chunks and word alignments . during translation , the rules are used to generate a reordering lattice for each sentence . experimental results are reported for a chinese-to-english task , showing an improvement of 0.5 % 1.8 % bleu score absolute on various test sets and better computational efficiency than reordering during decoding . the experiments also show that the reordering at the chunk-level performs better than at the pos-level .

concreteness and subjectivity as dimensions of lexical meaning
we quantify the lexical subjectivity of adjectives using a corpus-based method , and show for the first time that it correlates with noun concreteness in large corpora . these cognitive dimensions together influence how word meanings combine , and we exploit this fact to achieve performance improvements on the semantic classification of adjective-noun pairs .

a localized prediction model for statistical machine translation
in this paper , we present a novel training method for a localized phrase-based prediction model for statistical machine translation ( smt ) . the model predicts blocks with orientation to handle local phrase re-ordering . we use a maximum likelihood criterion to train a log-linear block bigram model which uses realvalued features ( e.g . a language model score ) as well as binary features based on the block identities themselves , e.g . block bigram features . our training algorithm can easily handle millions of features . the best system obtains a % improvement over the baseline on a standard arabic-english translation task .

picking them up and figuring them out : and marco idiart
this paper investigates , in a first stage , some methods for the automatic acquisition of verb-particle constructions ( vpcs ) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles . given the limited coverage provided by lexical resources , such as dictionaries , and the constantly growing number of vpcs , possible ways of automatically identifying them are crucial for any nlp task that requires some degree of semantic interpretation . in a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given vpc . the results obtained show that such combination can successfully be used to detect vpcs and distinguish idiomatic from compositional cases .

terminal-aware synchronous binarization
we present an scfg binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side . we also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality .

learning to map text to graph-based meaning representations
we argue in favor of using a graph-based representation for language meaning and propose a novel learning method to map natural language text to its graph-based meaning representation . we present a grammar formalism , which combines syntax and semantics , and has ontology constraints at the rule level . these constraints establish links between language expressions and the entities they refer to in the real world . we present a relational learning algorithm that learns these grammars from a small representative set of annotated examples , and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations .

from neighborhood to parenthood : the advantages of dependency representation over bigrams in brown clustering
we present an effective modification of the popular brown et al . 1992 word clustering algorithm , using a dependency language model . by leveraging syntax-based context , resulting clusters are better when evaluated against a wordnet for dutch . the improvements are stable across parameters such as number of clusters , minimum frequency and granularity . further refinement is possible through dependency relation selection . our approach achieves a desired clustering quality with less data , resulting in a decrease in cluster creation times .

minimum cut model for spoken lecture segmentation
we consider the task of unsupervised lecture segmentation . we formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion . our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies . our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .

chinese unknown word identification using character-based tagging and goh chooi ling , masayuki asahara , yuji matsumoto
since written chinese has no space to delimit words , segmenting chinese texts becomes an essential task . during this task , the problem of unknown word occurs . it is impossible to register all words in a dictionary as new words can always be created by combining characters . we propose a unified solution to detect unknown words in chinese texts . first , a morphological analysis is done to obtain initial segmentation and pos tags and then a chunker is used to detect unknown words .

a multimodal home entertainment interface via a mobile device
we describe a multimodal dialogue system for interacting with a home entertainment center via a mobile device . in our working prototype , users may utilize both a graphical and speech user interface to search tv listings , record and play television programs , and listen to music . the developed framework is quite generic , potentially supporting a wide variety of applications , as we demonstrate by integrating a weather forecast application . in the prototype , the mobile device serves as the locus of interaction , providing both a small touchscreen display , and speech input and output ; while the tv screen features a larger , richer gui . the system architecture is agnostic to the location of the natural language processing components : a consistent user experience is maintained regardless of whether they run on a remote server or on the device itself .

dudley north visits north london : learning when to transliterate to arabic mahmoud azab houda bouamor behrang mohit kemal oflazer
we report the results of our work on automating the transliteration decision of named entities for english to arabic machine translation . we construct a classification-based framework to automate this decision , evaluate our classifier both in the limited news and the diverse wikipedia domains , and achieve promising accuracy . moreover , we demonstrate a reduction of translation error and an improvement in the performance of an english-to-arabic machine translation system .

factorization of synchronous context-free grammars in linear time
factoring a synchronous context-free grammar into an equivalent grammar with a smaller number of nonterminals in each rule enables synchronous parsing algorithms of lower complexity . the problem can be formalized as searching for the tree-decomposition of a given permutation with the minimal branching factor . in this paper , by modifying the algorithm of uno and yagiura ( 2000 ) for the closely related problem of finding all common intervals of two permutations , we achieve a linear time algorithm for the permutation factorization problem . we also use the algorithm to analyze the maximum scfg rule length needed to cover hand-aligned data from various language pairs .

style & topic language model adaptation using hmm-lda bo-june ( paul ) hsu , james glass
adapting language models across styles and topics , such as for lecture transcription , involves combining generic style models with topic-specific content relevant to the target document . in this work , we investigate the use of the hidden markov model with latent dirichlet allocation ( hmm-lda ) to obtain syntactic state and semantic topic assignments to word instances in the training corpus . from these context-dependent labels , we construct style and topic models that better model the target document , and extend the traditional bag-of-words topic models to n-grams . experiments with static model interpolation yielded a perplexity and relative word error rate ( wer ) reduction of 7.1 % and 2.1 % , respectively , over an adapted trigram baseline . adaptive interpolation of mixture components further reduced perplexity by 9.5 % and wer by a modest 0.3 % .

learning to rank answer candidates for automatic resolution of crossword puzzles
in this paper , we study the impact of relational and syntactic representations for an interesting and challenging task : the automatic resolution of crossword puzzles . automatic solvers are typically based on two answer retrieval modules : ( i ) a web search engine , e.g. , google , bing , etc . and ( ii ) a database ( db ) system for accessing previously resolved crossword puzzles . we show that learning to rank models based on relational syntactic structures defined between the clues and the answer can improve both modules above . in particular , our approach accesses the db using a search engine and reranks its output by modeling paraphrasing . this improves on the mrr of previous system up to 53 % in ranking answer candidates and greatly impacts on the resolution accuracy of crossword puzzles up to 15 % .

learning to compose effective strategies from a library of
this paper describes a method for automatically learning effective dialogue strategies , generated from a library of dialogue content , using reinforcement learning from user feedback . this library includes greetings , social dialogue , chit-chat , jokes and relationship building , as well as the more usual clarification and verification components of dialogue . we tested the method through a motivational dialogue system that encourages take-up of exercise and show that it can be used to construct good dialogue strategies with little effort .

from visualisation to hypothesis construction for second language acquisition
one research goal in second language acquisition ( sla ) is to formulate and test hypotheses about errors and the environments in which they are made , a process which often involves substantial effort ; large amounts of data and computational visualisation techniques promise help here . in this paper we have defined a new task for finding contexts for errors that vary with the native language of the speaker that are potentially useful for sla research . we propose four models for approaching this task , and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts , with an apparent trade-off between false positives and very specific contexts .

a sequential model for multi-class classification
many classification problems require decisions among a large number of competing classes . these tasks , however , are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion . we suggest a general approach a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining , with high probability , the presence of the true outcome in the candidates set . some theoretical and computational properties of the model are discussed and we argue that these are important in nlp-like domains . the advantages of the model are illustrated in an experiment in partof-speech tagging .

testing and performance evaluation of machine transliteration system for tamil language
machine translation ( mt ) is a science fiction that was converted into reality with the enormous contributions from the mt research community . we can not expect any text without named entities ( ne ) . such nes are crucial in deciding the quality of mt . nes are to be recognized from the text and transliterated accordingly into the target language in order to ensure the quality of mt . in the present paper we present various technical issues encountered during handling the shared task of ne transliteration for tamil .

exploring variant definitions of pointer length in mdl
within the information-theoretical framework described by ( rissanen , 1989 ; de marcken , 1996 ; goldsmith , 2001 ) , pointers are used to avoid repetition of phonological material . work with which we are familiar has assumed that there is only one way in which items could be pointed to . the purpose of this paper is to describe and compare several different methods , each of which satisfies mdls basic requirements , but which have different consequences for the treatment of linguistic phenomena . in particular , we assess the conditions under which these different ways of pointing yield more compact descriptions of the data , both from a theoretical and an empirical perspective .

automatic evaluation of summaries using n-gram
following the recent adoption by the machine translation community of automatic evaluation using the bleu/nist scoring process , we conduct an in-depth study of a similar idea for evaluating summaries . the results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations , based on various statistical metrics ; while direct application of the bleu evaluation procedure does not always give good results .

a method for automatically generating a mediatory summary to verify credibility of information on the web
in this paper , we propose a method for mediatory summarization , which is a novel technique for facilitating users assessments of the credibility of information on the web . a mediatory summary is generated by extracting a passage from web documents ; this summary is generated on the basis of its relevance to a given query , fairness , and density of keywords , which are features of the summaries constructed to determine the credibility of information on the web . we demonstrate the effectiveness of the generated mediatory summary in comparison with the summaries of web documents produced by web search engines .

automatically learning cognitive status for multi-document summarization of newswire
machine summaries can be improved by using knowledge about the cognitive status of news article referents . in this paper , we present an approach to automatically acquiring distinctions in cognitive status using machine learning over the forms of referring expressions appearing in the input . we focus on modeling references to people , both because news often revolve around people and because existing natural language tools for named entity identification are reliable . we examine two specific distinctionswhether a person in the news can be assumed to be known to a target audience ( hearer-old vs hearer-new ) and whether a person is a major character in the news story . we report on machine learning experiments that show that these distinctions can be learned with high accuracy , and validate our approach using human subjects .

improvement of a naive bayes sentiment classifier using mrs-based
this study explores the potential of using deep semantic features to improve binary sentiment classification of paragraphlength movie reviews from the imbd website . using a naive bayes classifier as a baseline , we show that features extracted from minimal recursion semantics representations in conjunction with back-off replacement of sentiment terms is effective in obtaining moderate increases in accuracy over the baselines n-gram features . although our results are mixed , our most successful feature combination achieves an accuracy of 89.09 % , which represents an increase of 0.76 % over the baseline performance and a 6.48 % reduction in error .

robust textual inference via graph matching
we present a system for deciding whether a given sentence can be inferred from text . each sentence is represented as a directed graph ( extracted from a dependency parser ) in which the nodes represent words or phrases , and the links represent syntactic and semantic relationships . we develop a learned graph matching approach to approximate entailment using the amount of the sentences semantic content which is contained in the text . we present results on the recognizing textual entailment dataset ( dagan et al , 2005 ) , and show that our approach outperforms bag-of-words and tf-idf models . in addition , we explore common sources of errors in our approach and how to remedy them .

training continuous space language models : some practical issues
using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling , with applications in speech recognition and statistical machine translation . however , training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available . in this work , we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms . the induced word embeddings for extreme cases are also analysed , thus providing insight into the convergence issues . a new initialization scheme and new training techniques are then introduced . these methods are shown to greatly reduce the training time and to significantly improve performance , both in terms of perplexity and on a large-scale translation task .

generating minimal definite descriptions
the incremental algorithm introduced in ( dale and reiter , 1995 ) for producing distinguishing descriptions does not always generate a minimal description . in this paper , i show that when generalised to sets of individuals and disjunctive properties , this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions . i then present an alternative , constraint-based algorithm and show that it builds on existing related algorithms in that ( i ) it produces minimal descriptions for sets of individuals using positive , negative and disjunctive properties , ( ii ) it straightforwardly generalises to n-ary relations and ( iii ) it is integrated with surface realisation .

towards building a competitive opinion summarization system :
this paper presents an overview of our participation in the tac 2008 opinion pilot summarization task , as well as the proposed and evaluated post-competition improvements . we first describe our opinion summarization system and the results obtained . further on , we identify the systems weak points and suggest several improvements , focused both on information content , as well as linguistic and readability aspects . we obtain encouraging results , especially as far as fmeasure is concerned , outperforming the competition results by approximately 80 % .

extracting ccg derivations from the penn chinese treebank
automated conversion has allowed the development of wide-coverage corpora for a variety of grammar formalisms without the expense of manual annotation . analysing new languages also tests formalisms , exposing their strengths and weaknesses . we present chinese ccgbank , a 760,000 word corpus annotated with combinatory categorial grammar ( ccg ) derivations , induced automatically from the penn chinese treebank ( pctb ) . we design parsimonious ccg analyses for a range of chinese syntactic constructions , and transform the pctb trees to produce them . our process yields a corpus of 27,759 derivations , covering 98.1 % of the pctb .

mplus : a probabilistic medical language understanding system
this paper describes the basic philosophy and implementation of mplus ( m+ ) , a robust medical text analysis tool that uses a semantic model based on bayesian networks ( bns ) . bns provide a concise and useful formalism for representing semantic patterns in medical text , and for recognizing and reasoning over those patterns . bns are noise-tolerant , and facilitate the training of m+ .

build chinese emotion lexicons using a graph-based algorithm and multiple resources
for sentiment analysis , lexicons play an important role in many related tasks . in this paper , aiming to build chinese emotion lexicons for public use , we adopted a graph-based algorithm which ranks words according to a few seed emotion words . the ranking algorithm exploits the similarity between words , and uses multiple similarity metrics which can be derived from dictionaries , unlabeled corpora or heuristic rules . to evaluate the adopted algorithm and resources , two independent judges were asked to label the top words of ranking list . it is observed that noise is almost unavoidable due to imprecise similarity metrics between words . so , to guarantee the quality of emotion lexicons , we use an iterative feedback to combine manual labeling and the automatic ranking algorithm above . we also compared our newly constructed chinese emotion lexicons ( happiness , anger , sadness , fear and surprise ) with existing counterparts , and related analysis is offered .

story tracking : linking similar news over time and across languages bruno pouliquen & ralf steinberger
the europe media monitor system ( emm ) gathers and aggregates an average of 50,000 newspaper articles per day in over 40 languages . to manage the information overflow , it was decided to group similar articles per day and per language into clusters and to link daily clusters over time into stories . a story automatically comes into existence when related groups of articles occur within a 7-day window . while cross-lingual links across 19 languages for individual news clusters have been displayed since 2004 as part of a freely accessible online application ( http : //press.jrc.it/newsexplorer ) , the newest development is work on linking entire stories across languages . the evaluation of the monolingual aggregation of historical clusters into stories and of the linking of stories across languages yielded mostly satisfying results .

efficient search for inversion transduction grammar
we develop admissible a* search heuristics for synchronous parsing with inversion transduction grammar , and present results both for bitext alignment and for machine translation decoding . we also combine the dynamic programming hook trick with a* search for decoding . these techniques make it possible to find optimal alignments much more quickly , and make it possible to find optimal translations for the first time . even in the presence of pruning , we are able to achieve higher bleu scores with the same amount of computation .

judging grammaticality with tree substitution grammar derivations
in this paper , we show that local features computed from the derivations of tree substitution grammars such as the identify of particular fragments , and a count of large and small fragments are useful in binary grammatical classification tasks . such features outperform n-gram features and various model scores by a wide margin . although they fall short of the performance of the hand-crafted feature set of charniak and johnson ( 2005 ) developed for parse tree reranking , they do so with an order of magnitude fewer features . furthermore , since the tsgs employed are learned in a bayesian setting , the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification . on the bllip dataset , we achieve an accuracy of 89.9 % in discriminating between grammatical text and samples from an n-gram language model .

quantifying the limits and success of extractive summarization systems
this paper analyzes the topic identification stage of single-document automatic text summarization across four different domains , consisting of newswire , literary , scientific and legal documents . we present a study that explores the summary space of each domain via an exhaustive search strategy , and finds the probability density function ( pdf ) of the rouge score distributions for each domain . we then use this pdf to calculate the percentile rank of extractive summarization systems . our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain .

evaluating content selection in summarization : the pyramid method
we present an empirically grounded method for evaluating content selection in summarization . it incorporates the idea that no single best model summary for a collection of documents exists . our method quantifies the relative importance of facts to be conveyed . we argue that it is reliable , predictive and diagnostic , thus improves considerably over the shortcomings of the human evaluation method currently used in the document understanding conference .

steps to excellence : simple inference with refined scoring of
much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions . in contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures . specifically , we introduce a sampling-based parser that can easily handle arbitrary global features . inspired by samplerank , we learn to take guided stochastic steps towards a high scoring parse . we introduce two samplers for traversing the space of trees , gibbs and metropolis-hastings with random walk . the model outperforms state-of-the-art results when evaluated on 14 languages of non-projective conll datasets . our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and pos correction . the resulting method outperforms the best reported results on the catib dataset , approaching performance of parsing with gold tags .

using emotion to gain rapport in a spoken dialog system
this paper describes research on automatically building rapport . this is done by adapting responses in a spoken dialog system to users emotions as inferred from nonverbal voice properties . emotions and their acoustic correlates will be extracted from a persuasive dialog corpus and will be used to implement an emotionally intelligent dialog system ; one that can recognize emotion , choose an optimal strategy for gaining rapport , and render a response that contains appropriate emotion , both lexically and auditory . in order to determine the value of emotion modeling for gaining rapport in a spoken dialog system , the final implementation will be evaluated using different configurations through a user study .

an ensemble of grapheme and phoneme for machine transliteration
or words in one alphabetical system for the corresponding characters in another alphabetical system . there has been increasing concern on machine transliteration as an assistant of machine translation and information retrieval . three machine transliteration models , including grapheme-based model , phonemebased model , and hybrid model , have been proposed . however , there are few works trying to make use of correspondence between source grapheme and phoneme , although the correspondence plays an important role in machine transliteration . furthermore there are few works , which dynamically handle source grapheme and phoneme . in this paper , we propose a new transliteration model based on an ensemble of grapheme and phoneme . our model makes use of the correspondence and dynamically uses source grapheme and phoneme . our method shows better performance than the previous works about 15~23 % in english-to-korean transliteration and about 15~43 % in english-to-japanese transliteration .

bayesian unsupervised topic segmentation
this paper describes a novel bayesian approach to unsupervised topic segmentation . unsupervised systems for this task are driven by lexical cohesion : the tendency of wellformed segments to induce a compact and consistent lexical distribution . we show that lexical cohesion can be placed in a bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment ; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation . this contrasts with previous approaches , which relied on hand-crafted cohesion metrics . the bayesian framework provides a principled way to incorporate additional features such as cue phrases , a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems . our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets . we also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the bayesian framework.1

rumor has it : identifying misinformation in microblogs
a rumor is commonly defined as a statement whose true value is unverifiable . rumors may spread misinformation ( false information ) or disinformation ( deliberately false information ) on a network of people . identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority . in this paper , we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features : content-based , network-based , and microblog-specific memes for correctly identifying rumors . moreover , we show how these features are also effective in identifying disinformers , users who endorse a rumor and further help it to spread . we perform our experiments on more than 10,000 manually annotated tweets collected from twitter and show how our retrieval model achieves more than 0.95 in mean average precision ( map ) . finally , we believe that our dataset is the first large-scale dataset on rumor detection . it can open new dimensions in analyzing online misinformation and other aspects of microblog conversations .

babelnet : building a very large multilingual semantic network dipartimento di informatica simone paolo ponzetto
in this paper we present babelnet a very large , wide-coverage multilingual semantic network . the resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from wordnet and wikipedia . in addition machine translation is also applied to enrich the resource with lexical information for all languages . we conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource .

from high heels to weed attics : a syntactic investigation of chick lit and literature
stylometric analysis of prose is typically limited to classification tasks such as authorship attribution . since the models used are typically black boxes , they give little insight into the stylistic differences they detect . in this paper , we characterize two prose genres syntactically : chick lit ( humorous novels on the challenges of being a modern-day urban female ) and high literature . first , we develop a top-down computational method based on existing literary-linguistic theory . using an off-the-shelf parser we obtain syntactic structures for a dutch corpus of novels and measure the distribution of sentence types in chick-lit and literary novels . the results show that literature contains more complex ( subordinating ) sentences than chick lit . secondly , a bottom-up analysis is made of specific morphological and syntactic features in both genres , based on the parsers output . this shows that the two genres can be distinguished along certain features . our results indicate that detailed insight into stylistic differences can be obtained by combining computational linguistic analysis with literary theory .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

annotating dialogue acts to construct dialogue systems for consulting
this paper introduces a new corpus of consulting dialogues , which is designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus . we have collected 130 h of consulting dialogues in the tourist guidance domain . this paper outlines our taxonomy of dialogue act annotation that can describe two aspects of an utterances : the communicative function ( speech act ) , and the semantic content of the utterance . we provide an overview of the kyoto tour guide dialogue corpus and a preliminary analysis using the dialogue act tags .

a simple and generic belief tracking mechanism for the dialog state tracking challenge : on the believability of observed information
this paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules , using basic probability operations . the rules apply to observed system actions and partially observable user acts , without using any knowledge obtained from external resources ( i.e . without requiring training data ) . the core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself , so as to better lowerbound ones expectations on the performance of more advanced statistical techniques for the task . the proposed method is evaluated in the dialog state tracking challenge , where it achieves comparable performance in hypothesis accuracy to machine learning based systems . consequently , with respect to different scenarios for the belief tracking problem , the potential superiority and weakness of machine learning approaches in general are investigated .

model-portability experiments for textual temporal analysis
we explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains : we generate additional training examples by substituting temporal expression words with potential synonyms . we explore using synonyms both from wordnet and from the latent words language model ( lwlm ) , which predicts synonyms in context using an unsupervised approach . we evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from tempeval 2010 , reuters and wikipedia . we find that the lwlm provides substan-tial improvements on the reuters corpus , and smaller improvements on the wikipedia corpus . we find that wordnet alne never improves performance , though intersecting the examples from the lwlm and wordnet provides more stable results for wikipedia .

a character-based joint model for chinese word segmentation behavior design corporation
the character-based tagging approach is a dominant technique for chinese word segmentation , and both discriminative and generative models can be adopted in that framework . however , generative and discriminative character-based approaches are significantly different and complement each other . a simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches . experiments on the second sighan bakeoff show that this joint approach achieves 21 % relative error reduction over the discriminative model and 14 % over the generative one . in addition , closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best fscore in four out of five corpora .

wrapping up a summary : from representation to generation josef steinberger and marco turchi and
the main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries . this is motivated by empirical evidence from tac 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries . we report encouraging preliminary results comparable to those attained by participating systems at tac 2009 .

discretization based learning approach to information retrieval dmitri roussinov weiguo fan
we approached the problem as learning how to order documents by estimated relevance with respect to a user query . our support vector machines based classifier learns from the relevance judgments available with the standard test collections and generalizes to new , previously unseen queries . for this , we have designed a representation scheme , which is based on the discrete representation of the local ( lw ) and global ( gw ) weighting functions , thus is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf , bm25 or those based on language models . our tests with the standard test collections have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely from the labeled examples and without taking advantage of knowing those functions or their important properties or parameters .

using similarity scoring to improve the bilingual dictionary for word
we describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment , and evaluate the improved dictionary using a version of the competitive linking algorithm . we demonstrate a problem faced by the competitive linking algorithm and present an approach to ameliorate it . in particular , we rebuild the bilingual dictionary by clustering similar words in a language and assigning them a higher cooccurrence score with a given word in the other language than each single word would have otherwise . experimental results show a significant improvement in precision and recall for word alignment when the improved dicitonary is used .

learning structured models for phone recognition
we present a maximally streamlined approach to learning hmm-based acoustic models for automatic speech recognition . in our approach , an initial monophone hmm is iteratively refined using a split-merge em procedure which makes no assumptions about subphone structure or context-dependent structure , and which uses only a single gaussian per hmm state . despite the much simplified training process , our acoustic model achieves state-of-the-art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard cd triphone / subphone / gmm approaches ) . we also present an analysis of what is and is not learned by our system .

alignment model adaptation for domain-specific word alignment
this paper proposes an alignment adaptation approach to improve domain-specific ( in-domain ) word alignment . the basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results . in this paper , we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively , and then interpolate these two models to improve the domain-specific word alignment . experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .

using fmri activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora colin kelly & anna korhonen
we present a series of methods for deriving conceptual representations from corpora and investigate the usefulness of the fmri data and machine learning methodology of mitchell et al ( 2008 ) as a basis for evaluating the different models . within this framework , the quality of a semantic model is quantified by its ability to predict the fmri activation associated with conceptual stimuli . mitchell et al used a manually-acquired set of verbs as the basis for their semantic model ; in this paper , we also consider automatically acquired feature-norm-like semantic representations . these models make different assumptions about the kinds of information available in corpora that is relevant to representing conceptual knowledge . our results indicate that automatically-acquired representations can make equally powerful predictions about the brain activity associated with the stimuli .

learning multi character alignment rules and classification of training data for transliteration
we address the issues of transliteration between indian languages and english , especially for named entities . we use an em algorithm to learn the alignment between the languages . we find that there are lot of ambiguities in the rules mapping the characters in the source language to the corresponding characters in the target language . some of these ambiguities can be handled by capturing context by learning multi-character based alignments and use of character n-gram models . we observed that a word in the source script may have actually originated from different languages . instead of learning one model for the language pair , we propose that one may use multiple models and a classifier to decide which model to use . a contribution of this work is that the models and classifiers are learned in a completely unsupervised manner . using our system we were able to get quite accurate transliteration models .

a new methodology for the construction and validation of information resources for consumer health and medical information science
a consumer health information system must be able to comprehend both expert and nonexpert medical vocabulary and to map between the two . we describe an ongoing project to create a new lexical database called medical wordnet ( mwn ) , consisting of medically relevant terms used by and intelligible to non-expert subjects and supplemented by a corpus of natural-language sentences that is designed to provide medically validated contexts for mwn terms . the corpus derives primarily from online health information sources targeted to consumers , and involves two sub-corpora , called medical factnet ( mfn ) and medical beliefnet ( mbn ) , respectively . the former consists of statements accredited as true on the basis of a rigorous process of validation , the latter of statements which non-experts believe to be true . we summarize the mwn / mfn / mbn project , and describe some of its applications .

inferring semantic roles using sub-categorization frames and maximum entropy model
in this paper , we propose an approach for inferring semantic role using subcategorization frames and maximum entropy model . our approach aims to use the sub-categorization information of the verb to label the mandatory arguments of the verb in various possible ways . the ambiguity between the assignment of roles to mandatory arguments is resolved using the maximum entropy model . the unlabelled mandatory arguments and the optional arguments are labelled directly using the maximum entropy model such that their labels are not one among the frame elements of the sub-categorization frame used . maximum entropy model is preferred because of its novel approach of smoothing . using this approach , we obtained an f-measure of 68.14 % on the development set of the data provided for the conll-2005 shared task . we show that this approach performs well in comparison to an approach which uses only the maximum entropy model .

faster phrase-based decoding by refining feature state
we contribute a faster decoding algorithm for phrase-based machine translation . translation hypotheses keep track of state , such as context for the language model and coverage of words in the source sentence . most features depend upon only part of the state , but traditional algorithms , including cube pruning , handle state atomically . for example , cube pruning will repeatedly query the language model with hypotheses that differ only in source coverage , despite the fact that source coverage is irrelevant to the language model . our key contribution avoids this behavior by placing hypotheses into equivalence classes , masking the parts of state that matter least to the score . moreover , we exploit shared words in hypotheses to iteratively refine language model scores rather than handling language model state atomically . since our algorithm and cube pruning are both approximate , improvement can be used to increase speed or accuracy . when tuned to attain the same accuracy , our algorithm is 4.07.7 times as fast as the moses decoder with cube pruning .

towards automatic addressee identification in multi-party dialogues rieks op den akker
the paper is about the issue of addressing in multi-party dialogues . analysis of addressing behavior in face to face meetings results in the identification of several addressing mechanisms . from these we extract several utterance features and features of non-verbal communicative behavior of a speaker , like gaze and gesturing , that are relevant for observers to identify the participants the speaker is talking to . a method for the automatic prediction of the addressee of speech acts is discussed .

a geo-coding service encompassing a geo-parsing tool and integrated digital main library building main library building
we describe a basic geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service . the development of a geo-parser comes from the need to explicitly georeference large resource collections such as the statistical accounts of scotland which currently only contain implicit georeferences in the form of placennames thus making such collections inherently geographically searchable .

an annotation type system for a data-driven nlp pipeline
we introduce an annotation type system for a data-driven nlp core system . the specifications cover formal document structure and document meta information , as well as the linguistic levels of morphology , syntax and semantics . the type system is embedded in the framework of the unstructured information management architecture ( uima ) .

dramatically reducing training data size through vocabulary
our field has seen significant improvements in the quality of machine translation systems over the past several years . the single biggest factor in this improvement has been the accumulation of ever larger stores of data . however , we now find ourselves the victims of our own success , in that it has become increasingly difficult to train on such large sets of data , due to limitations in memory , processing power , and ultimately , speed ( i.e. , data to models takes an inordinate amount of time ) . some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets , domain adaptation to arrive at data more suited to the task at hand , or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g. , ( eck et al , 2005 ) . this paper focuses on techniques related to the latter efforts . we have developed a very simple n-gram counting method that reduces the size of data sets dramatically , as much as 90 % , and is applicable independent of specific dev and test data . at the same time it reduces model sizes , improves training times , and , because it attempts to preserve contexts for all n-grams in a corpus , the cost in quality is minimal ( as measured by bleu ) . further , unlike other methods created specifically for data reduction that have similar effects on the data , our method scales to very large data , up to tens to hundreds of millions of parallel sentences .

a unified approach in speech-to-speech translation : integrating features of speech recognition and machine translation taro watanabe and frank soong and wai kit lo
based upon a statistically trained speech translation system , in this study , we try to combine distinctive features derived from the two modules : speech recognition and statistical machine translation , in a loglinear model . the translation hypotheses are then rescored and translation performance is improved . the standard translation evaluation metrics , including bleu , nist , multiple reference word error rate and its position independent counterpart , were optimized to solve the weights of the features in the log-linear model . the experimental results have shown significant improvement over the baseline ibm model 4 in all automatic translation evaluation metrics . the largest was for bleu , by 7.9 % absolute .

an incremental architecture for the semantic annotation of dialogue lina maria rojas-barahona and matthieu quignard
the semantic annotation of dialogue corpora permits building efficient language understanding applications for supporting enjoyable and effective human-machine interactions . nevertheless , the annotation process could be costly , time-consuming and complicated , particularly the more expressive is the semantic formalism . in this work , we propose a bootstrapping architecture for the semantic annotation of dialogue corpora with rich structures , based on dependency syntax and frame semantics .

relation detection between named entities : report of a shared task hugo goncalo oliveira cisuc , dei - fctuc
in this paper we describe the first evaluation contest ( track ) for portuguese whose goal was to detect and classify relations between named entities in running text , called rerelem . given a collection annotated with named entities belonging to ten different semantic categories , we marked all relationships between them within each document . we used the following fourfold relationship classification : identity , included-in , located-in , and other ( which was later on explicitly detailed into twenty different relations ) . we provide a quantitative description of this evaluation resource , as well as describe the evaluation architecture and summarize the results of the participating systems in the track .

adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification
polarity lexicons have been a valuable resource for sentiment analysis and opinion mining . there are a number of such lexical resources available , but it is often suboptimal to use them as is , because general purpose lexical resources do not reflect domain-specific lexical usage . in this paper , we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristics of the data more directly . in particular , our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item ( positive , neutral , negative , or negator ) for the given domain . experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification .

zipfian corruptions for robust pos tagging
inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech ( pos ) tagging that is less sensitive to domain shifts . the objective of our method is to minimize average loss under random distribution shifts . we restrict the possible target distributions to mixtures of the source distribution and random zipfian distributions . our algorithm is used for pos tagging and evaluated on the english web treebank and the danish dependency treebank with an average 4.4 % error reduction in tagging accuracy .

a first order semantic approach to adjectival inference
as shown in the formal semantics literature , adjectives can display very different inferential patterns depending on whether they are intersective , privative , subsective or plain non-subsective . moreover , many of these classes are often described using second order constructs . in this paper , we adopt hobbss ontologically promiscuous approach and present a first order treatment of adjective semantics which opens the way for a sophisticated treatment of adjectival inference . the approach was implemented and tested using first order automated reasoners .

cross-lingual wsd for translation extraction from comparable corpora
we propose a data-driven approach to enhance translation extraction from comparable corpora . instead of resorting to an external dictionary , we translate source vector features by using a cross-lingual word sense disambiguation method . the candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature . the translations found in the disambiguation output convey the sense of the features in the source vector , while the use of translation clusters permits to expand their translation with several variants . as a consequence , the translated vectors are less noisy and richer , and allow for the extraction of higher quality lexicons compared to simpler methods .

parsing arabic dialects
the arabic language is a collection of spoken dialects with important phonological , morphological , lexical , and syntactic differences , along with a standard written language , modern standard arabic ( msa ) . since the spoken dialects are not officially written , it is very costly to obtain adequate corpora to use for training dialect nlp tools such as parsers . in this paper , we address the problem of parsing transcribed spoken levantine arabic ( la ) .we do not assume the existence of any annotated la corpus ( except for development and testing ) , nor of a parallel corpus lamsa . instead , we use explicit knowledge about the relation between la and msa .

in online discussions with a socially-tuned sentiment lexicon
we study the problem of agreement and disagreement detection in online discussions . an isotonic conditional random fields ( isotonic crf ) based sequential model is proposed to make predictions on sentence- or segment-level . we automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance . we evaluate our agreement and disagreement tagging model on two disparate online discussion corpora wikipedia talk pages and online debates . our model is shown to outperform the state-of-the-art approaches in both datasets . for example , the isotonic crf model achieves f1 scores of 0.74 and 0.67 for agreement and disagreement detection , when a linear chain crf obtains 0.58 and 0.56 for the discussions on wikipedia talk pages .

length-incremental phrase training for smt
we present an iterative technique to generate phrase tables for smt , which is based on force-aligning the training data with a modified translation decoder . different from previous work , we completely avoid the use of a word alignment or phrase extraction heuristics , moving towards a more principled phrase generation and probability estimation . during training , we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration . experiments are carried out on the iwslt 2011 arabic-english task , where we are able to reach moderate improvements on a state-of-the-art baseline with our training method . the resulting phrase table shows only a small overlap with the heuristically extracted one , which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics . by interpolating the heuristic and the trained phrase table , we can improve over the baseline by 0.5 % bleu and 0.5 % ter .

a classification algorithm for predicting the structure of summaries
we investigate the problem of generating the structure of short domain independent abstracts . we apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool . we design a set of features for learning inspired from past research in content selection , information ordering , and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts . the proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60 % of the cases .

a virtual manipulative for learning log-linear models
we present an open-source virtual manipulative for conditional log-linear models . this web-based interactive visualization lets the user tune the probabilities of various shapeswhich grow and shrink accordinglyby dragging sliders that correspond to feature weights . the visualization displays a regularized training objective ; it supports gradient ascent by optionally displaying gradients on the sliders and providing step and solve buttons . the user can sample parameters and datasets of different sizes and compare their own parameters to the truth . our website , http : //cs.jhu.edu/jason/ tutorials/loglin/ , guides the user through a series of interactive lessons and provides auxiliary readings , explanations , practice problems and resources .

word segmentation of informal arabic with domain adaptation
segmentation of clitics has been shown to improve accuracy on a variety of arabic nlp tasks . however , state-of-the-art arabic word segmenters are either limited to formal modern standard arabic , performing poorly on arabic text featuring dialectal vocabulary and grammar , or rely on linguistic knowledge that is hand-tuned for each dialect . we extend an existing msa segmenter with a simple domain adaptation technique and new features in order to segment informal and dialectal arabic text . experiments show that our system outperforms existing systems on newswire , broadcast news and egyptian dialect , improving segmentation f 1 score on a recently released egyptian arabic corpus to 95.1 % , compared to 90.8 % for another segmenter designed specifically for egyptian arabic .

stanfords multi-pass sieve coreference resolution system at the
this paper details the coreference resolution system submitted by stanford at the conll2011 shared task . our system is a collection of deterministic coreference resolution models that incorporate lexical , syntactic , semantic , and discourse information . all these models use global document-level information by sharing mention attributes , such as gender and number , across mentions in the same cluster . we participated in both the open and closed tracks and submitted results using both predicted and gold mentions . our system was ranked first in both tracks , with a score of 57.8 in the closed track and 58.3 in the open track .

multi-word expression identification using sentence surface features
much nlp research on multi-word expressions ( mwes ) focuses on the discovery of new expressions , as opposed to the identification in texts of known expressions . however , mwe identification is not trivial because many expressions allow variation in form and differ in the range of variations they allow . we show that simple rule-based baselines do not perform identification satisfactorily , and present a supervised learning method for identification that uses sentence surface features based on expressions canonical form . to evaluate the method , we have annotated 3350 sentences from the british national corpus , containing potential uses of 24 verbal mwes . the method achieves an f-score of 94.86 % , compared with 80.70 % for the leading rule-based baseline . our method is easily applicable to any expression type . experiments in previous research have been limited to the compositional/non-compositional distinction , while we also test on sentences in which the words comprising the mwe appear but not as an expression .

recognizing sublanguages in scientific journal articles through closure properties computational bioscience program
it has long been realized that sublanguages are relevant to natural language processing and text mining . however , practical methods for recognizing or characterizing them have been lacking . this paper describes a publicly available set of tools for sublanguage recognition . closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model . scientific journal articles are compared to general english text , and it is shown that the journal articles fit the sublanguage model , while the general english text does not . a number of examples of implications of the sublanguage characteristics for natural language processing are pointed out . the software is made publicly available at [ edited for anonymization ] .

semantic role tagging for chinese at the lexical level
the absence of a parser . we investigated the effect of using only lexical information in statistical training ; and proposed to identify the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled . experiments were done on a textbook corpus and a news corpus , representing simple data and complex data respectively . results suggested that in chinese , simple lexical features are useful enough when constituent boundaries are known , while parse information might be more important for complicated sentences than simple ones . several ways to improve the headword identification results were suggested , and we also plan to explore some class-based techniques for the task , with reference to existing semantic lexicons .

speakers intention prediction using statistics of multi-level features in a schedule management domain
speakers intention prediction modules can be widely used as a pre-processor for reducing the search space of an automatic speech recognizer . they also can be used as a preprocessor for generating a proper sentence in a dialogue system . we propose a statistical model to predict speakers intentions by using multi-level features . using the multi-level features ( morpheme-level features , discourselevel features , and domain knowledge-level features ) , the proposed model predicts speakers intentions that may be implicated in next utterances . in the experiments , the proposed model showed better performances ( about 29 % higher accuracies ) than the previous model . based on the experiments , we found that the proposed multi-level features are very effective in speakers intention prediction .

divide and translate : improving long distance reordering in statistical
this paper proposes a novel method for long distance , clause-level reordering in statistical machine translation ( smt ) . the proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals . the nonterminals are placeholders of embedded clauses , by which we reduce complicated clause-level reordering into simple wordlevel reordering . its translation model is trained using a bilingual corpus with clause-level alignment , which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language . we achieved significant improvements of 1.4 % in bleu and 1.3 % in ter by using moses , and 2.2 % in bleu and 3.5 % in ter by using our hierarchical phrase-based smt , for the english-to-japanese translation of research paper abstracts in the medical domain .

understanding temporal expressions in emails
recent years have seen increasing research on extracting and using temporal information in natural language applications . however most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts . in this paper we report our work on anchoring temporal expressions in a novel genre , emails . the highly under-specified nature of these expressions fits well with our constraintbased representation of time , time calculus for natural language ( tcnl ) . we have developed and evaluated a temporal expression anchoror ( tea ) , and the result shows that it performs significantly better than the baseline , and compares favorably with some of the closely related work .

analysing lexical consistency in translation
a number of approaches have been taken to improve lexical consistency in statistical machine translation . however , little has been written on the subject of where and when to encourage consistency . i present an analysis of human authored translations , focussing on words belonging to different parts-of-speech across a number of different genres .

a high accuracy method for semi-supervised information extraction
customization to specific domains of discourse and/or user requirements is one of the greatest challenges for todays information extraction ( ie ) systems . while demonstrably effective , both rule-based and supervised machine learning approaches to ie customization pose too high a burden on the user . semisupervised learning approaches may in principle offer a more resource effective solution but are still insufficiently accurate to grant realistic application . we demonstrate that this limitation can be overcome by integrating fully-supervised learning techniques within a semisupervised ie approach , without increasing resource requirements .

evaluation of several phonetic similarity algorithms on the task of cognate identification
we investigate the problem of measuring phonetic similarity , focusing on the identification of cognates , words of the same origin in different languages . we compare representatives of two principal approaches to computing phonetic similarity : manually-designed metrics , and learning algorithms . in particular , we consider a stochastic transducer , a pair hmm , several dbn models , and two constructed schemes . we test those approaches on the task of identifying cognates among indoeuropean languages , both in the supervised and unsupervised context . our results suggest that the averaged context dbn model and the pair hmm achieve the highest accuracy given a large training set of positive examples .

a parallel proposition bank ii for chinese and english
the proposition bank ( propbank ) project is aimed at creating a corpus of text annotated with information about semantic propositions . the second phase of the project , propbank ii adds additional levels of semantic annotation which include eventuality variables , co-reference , coarse-grained sense tags , and discourse connectives . this paper presents the results of the parallel propbank ii project , which adds these richer layers of semantic annotation to the first 100k of the chinese treebank and its english translation . our preliminary analysis supports the hypothesis that this additional annotation reconciles many of the surface differences between the two languages .

unsupervised multilingual grammar induction
we investigate the task of unsupervised constituency parsing from bilingual parallel corpora . our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data . we formulate a generative bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters . to this end , we adapt a formalism known as unordered tree alignment to our probabilistic setting . using this formalism , our model loosely binds parallel trees while allowing language-specific syntactic structure . we perform inference under this model using markov chain monte carlo and dynamic programming . applying this model to three parallel corpora ( korean-english , urdu-english , and chinese-english ) we find substantial performance gains over the ccm model , a strong monolingual baseline . on average , across a variety of testing scenarios , our model achieves an 8.8 absolute gain in f-measure .

multi-document biography summarization
in this paper we describe a biography summarization system using sentence classification and ideas from information retrieval . although the individual techniques are not new , assembling and applying them to generate multi-document biographies is new . our system was evaluated in duc2004 . it is among the top performers in task 5short summaries focused by person questions .

on using articulatory features for discriminative speaker adaptation
this paper presents a way to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup , which included acoustic models for articulatory features such as rounded or voiced . we present supervised speaker adaptation experiments on a spontaneous speech task and compare the above stream-based approach to conventional approaches , in which the models , and not stream combination weights , are being adapted . in the approach we present , stream weights model the importance of features such as voiced for word discrimination , which offers a descriptive interpretation of the adaptation parameters .

predicting risk from financial reports with regression
we address a text regression problem : given a piece of text , predict a real-world continuous quantity associated with the texts meaning . in this work , the text is an sec-mandated financial report published annually by a publiclytraded company , and the quantity to be predicted is volatility of stock returns , an empirical measure of financial risk . we apply wellknown regression techniques to a large corpus of freely available financial reports , constructing regression models of volatility for the period following a report . our models rival past volatility ( a strong baseline ) in predicting the target variable , and a single model that uses both can significantly outperform past volatility . interestingly , our approach is more accurate for reports after the passage of the sarbanes-oxley act of 2002 , giving some evidence for the success of that legislation in making financial reports more informative .

offline strategies for online question answering : answering questions before they are asked
recent work in question answering has focused on web-based systems that extract answers using simple lexicosyntactic patterns . we present an alternative strategy in which patterns are used to extract highly precise relational information offline , creating a data repository that is used to efficiently answer questions . we evaluate our strategy on a challenging subset of questions , i.e . who is questions , against a state of the art web-based question answering system . results indicate that the extracted relations answer 25 % more questions correctly and do so three orders of magnitude faster than the state of the art system .

japanese-spanish thesaurus construction using english as a pivot jessica ramrez , masayuki asahara , yuji matsumoto
we present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of wikipedia and wordnet . our goal is to increase resources for natural language processing tasks such as machine translation targeting the japanese-spanish language pair . given the scarcity of resources , we use existing english resources as a pivot for creating a trilingual japanesespanish-english thesaurus . our approach consists of extracting the translation tuples from wikipedia , disambiguating them by mapping them to wordnet word senses . we present results comparing two methods of disambiguation , the first using vsm on wikipedia article texts and wordnet definitions , and the second using categorical information extracted from wikipedia , we find that mixing the two methods produces favorable results . using the proposed method , we have constructed a multilingual spanish-japanese-english thesaurus consisting of 25,375 entries . the same method can be applied to any pair of languages that are linked to english in wikipedia .

a tabulation-based parsing method that reduces copying
this paper presents a new bottom-up chart parsing algorithm for prolog along with a compilation procedure that reduces the amount of copying at run-time to a constant number ( 2 ) per edge . it has applications to unification-based grammars with very large partially ordered categories , in which copying is expensive , and can facilitate the use of more sophisticated indexing strategies for retrieving such categories that may otherwise be overwhelmed by the cost of such copying . it also provides a new perspective on quick-checking and related heuristics , which seems to confirm that forcing an early failure ( as opposed to seeking an early guarantee of success ) is in fact the best approach to use . a preliminary empirical evaluation of its performance is also provided .

acquiring an ontology for a fundamental vocabulary
in this paper we describe the extraction of thesaurus information from parsed dictionary definition sentences . the main data for our experiments comes from lexeed , a japanese semantic dictionary , and the hinoki treebank built on it . the dictionary is parsed using a head-driven phrase structure grammar of japanese . knowledge is extracted from the semantic representation ( minimal recursion semantics ) . this makes the extraction process language independent .

human gene name normalization using text matching with automatically extracted synonym dictionaries division of oncology , childrens hospital of philadelphia
the identification of genes in biomedical text typically consists of two stages : identifying gene mentions and normalization of gene names . we have created an automated process that takes the output of named entity recognition ( ner ) systems designed to identify genes and normalizes them to standard referents . the system identifies human gene synonyms from online databases to generate an extensive synonym lexicon . the lexicon is then compared to a list of candidate gene mentions using various string transformations that can be applied and chained in a flexible order , followed by exact string matching or approximate string matching . using a gold standard of medline abstracts manually tagged and normalized for mentions of human genes , a combined tagging and normalization system achieved 0.669 f-measure ( 0.718 precision and 0.626 recall ) at the mention level , and 0.901 f-measure ( 0.957 precision and 0.857 recall ) at the document level for documents used for tagger training .

a diachronic approach for schwa deletion in indo aryan languages
schwa deletion is an important issue in grapheme-to-phoneme conversion for indoaryan languages ( ial ) . in this paper , we describe a syllable minimization based algorithm for dealing with this that outperforms the existing methods in terms of efficiency and accuracy . the algorithm is motivated by the fact that deletion of schwa is a diachronic and sociolinguistic phenomenon that facilitates faster communication through syllable economy . the contribution of the paper is not just a better algorithm for schwa deletion ; rather we describe here a constrained optimization based framework that can partly model the evolution of languages , and hence , can be used for solving many problems in computational linguistics that call for diachronic explanations .

cross language dependency parsing using a bilingual lexicon
this paper proposes an approach to enhance dependency parsing in a language by using a translated treebank from another language . a simple statistical machine translation method , word-by-word decoding , where not a parallel corpus but a bilingual lexicon is necessary , is adopted for the treebank translation . using an ensemble method , the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language . the proposed method is evaluated in english and chinese treebanks . it is shown that a translated english treebank helps a chinese parser obtain a state-ofthe-art result .

learning verb-noun relations to improve parsing
the verb-noun sequence in chinese often creates ambiguities in parsing . these ambiguities can usually be resolved if we know in advance whether the verb and the noun tend to be in the verb-object relation or the modifier-head relation . in this paper , we describe a learning procedure whereby such knowledge can be automatically acquired . using an existing ( imperfect ) parser with a chart filter and a tree filter , a large corpus , and the log-likelihood-ratio ( llr ) algorithm , we were able to acquire verb-noun pairs which typically occur either in verbobject relations or modifier-head relations . the learned pairs are then used in the parsing process for disambiguation . evaluation shows that the accuracy of the original parser improves significantly with the use of the automatically acquired knowledge .

automatically generating wikipedia articles : a structure-aware approach
in this paper , we investigate an approach for creating a comprehensive textual overview of a subject composed of information drawn from the internet . we use the high-level structure of human-authored texts to automatically induce a domainspecific template for the topic structure of a new overview . the algorithmic innovation of our work is a method to learn topicspecific extractors for content selection jointly for the entire template . we augment the standard perceptron algorithm with a global integer linear programming formulation to optimize both local fit of information into each topic and global coherence across the entire overview . the results of our evaluation confirm the benefits of incorporating structural information into the content selection process .

applying semantic frame theory to automate natural language template generation from ontology statements
today there exist a growing number of framenet-like resources offering semantic and syntactic phrase specifications that can be exploited by natural language generation systems . in this paper we present on-going work that provides a starting point for exploiting framenet information for multilingual natural language generation . we describe the kind of information offered by modern computational lexical resources and discuss how template-based generation systems can benefit from them .

using part-of-speech reranking to improve chinese word segmentation
chinese word segmentation and part-ofspeech ( pos ) tagging have been commonly considered as two separated tasks . in this paper , we present a system that performs chinese word segmentation and pos tagging simultaneously . we train a segmenter and a tagger model separately based on linear-chain conditional random fields ( crf ) , using lexical , morphological and semantic features . we propose an approximated joint decoding method by reranking the n-best segmenter output , based pos tagging information . experimental results on sighan bakeoff dataset and penn chinese treebank show that our reranking method significantly improve both segmentation and pos tagging accuracies .

combining orthogonal monolingual and multilingual sources of evidence for all words wsd
word sense disambiguation remains one of the most complex problems facing computational linguists to date . in this paper we present a system that combines evidence from a monolingual wsd system together with that from a multilingual wsd system to yield state of the art performance on standard all-words data sets . the monolingual system is based on a modification of the graph based state of the art algorithm in-degree . the multilingual system is an improvement over an allwords unsupervised approach , salaam . salaam exploits multilingual evidence as a means of disambiguation . in this paper , we present modifications to both of the original approaches and then their combination . we finally report the highest results obtained to date on the senseval 2 standard data set using an unsupervised method , we achieve an overall f measure of 64.58 using a voting scheme .

cora : a web-based annotation tool for historical and other non-standard language data
we present cora , a web-based annotation tool for manual annotation of historical and other non-standard language data . it allows for editing the primary data and modifying token boundaries during the annotation process . further , it supports immediate retraining of taggers on newly annotated data .

extracting a verb lexicon for deep parsing from framenet
we examine the feasibility of harvesting a wide-coverage lexicon of english verbs from the framenet semantically annotated corpus , intended for use in a practical natural language understanding ( nlu ) system . we identify a range of constructions for which current annotation practice leads to problems in deriving appropriate lexical entries , for example imperatives , passives and control , and discuss potential solutions .

ensemble document clustering using weighted hypergraph generated by nmf
in this paper , we propose a new ensemble document clustering method . the novelty of our method is the use of non-negative matrix factorization ( nmf ) in the generation phase and a weighted hypergraph in the integration phase . in our experiment , we compared our method with some clustering methods . our method achieved the best results .

representation learning for text-level discourse parsing
text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features . in this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates rst discourse parsing . by combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features . the resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the rst treebank .

how well do semantic relatedness measures perform
various semantic relatedness , similarity , and distance measures have been proposed in the past decade and many nlp-applications strongly rely on these semantic measures . researchers compete for better algorithms and normally only few percentage points seem to suffice in order to prove a new measure outperforms an older one . in this paper we present a metastudy comparing various semantic measures and their correlation with human judgments . we show that the results are rather inconsistent and ask for detailed analyses as well as clarification . we argue that the definition of a shared task might bring us considerably closer to understanding the concept of semantic relatedness . 59 60 cramer

pseudo-projective dependency parsing
in order to realize the full potential of dependency-based syntactic parsing , it is desirable to allow non-projective dependency structures . we show how a datadriven deterministic dependency parser , in itself restricted to projective structures , can be combined with graph transformation techniques to produce non-projective structures . experiments using data from the prague dependency treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy . this leads to the best reported performance for robust non-projective parsing of czech .

the arabic online commentary dataset : an annotated dataset of informal arabic with high dialectal content
the written form of arabic , modern standard arabic ( msa ) , differs quite a bit from the spoken dialects of arabic , which are the true native languages of arabic speakers used in daily life . however , due to msas prevalence in written form , almost all arabic datasets have predominantly msa content . we present the arabic online commentary dataset , a 52m-word monolingual dataset rich in dialectal content , and we describe our long-term annotation effort to identify the dialect level ( and dialect itself ) in each sentence of the dataset . so far , we have labeled 108k sentences , 41 % of which as having dialectal content . we also present experimental results on the task of automatic dialect identification , using the collected labels for training and evaluation .

clause identification and classification in bengali
this paper reports about the development of clause identification and classification techniques for bengali language . a syntactic rule based model has been used to identify the clause boundary . for clause type identification a conditional random field ( crf ) based statistical model has been used . the clause identification system and clause classification system demonstrated 73 % and 78 % precision values respectively .

all words domain adapted wsd : finding a middle ground between
in spite of decades of research on word sense disambiguation ( wsd ) , all-words general purpose wsd has remained a distant goal . many supervised wsd systems have been built , but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern . therefore , attempts have been made to develop unsupervised and knowledge based techniques for wsd which do not need sense marked corpora . however such approaches have not proved effective , since they typically do not better wordnet first sense baseline accuracy . our research reported here proposes to stick to the supervised approach , but with far less demand on annotation . we show that if we have any sense marked corpora , be it from mixed domain or a specific domain , a small amount of annotation in any other domain can deliver the goods almost as if exhaustive sense marking were available in that domain . we have tested our approach across tourism and health domain corpora , using also the well known mixed domain semcor corpus . accuracy figures close to self domain training lend credence to the viability of our approach . our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised wsd . finally , our approach is not restricted to any specific set of target words , a departure from a commonly observed practice in domain specific wsd .

unsupervised alignment of comparable data and text resources anja belz eric kow
in this paper we investigate automatic datatext alignment , i.e . the task of automatically aligning data records with textual descriptions , such that data tokens are aligned with the word strings that describe them . our methods make use of log likelihood ratios to estimate the strength of association between data tokens and text tokens . we investigate datatext alignment at the document level and at the sentence level , reporting results for several methodological variants as well as baselines . we find that log likelihood ratios provide a strong basis for predicting data-text alignment .

automatic expansion of equivalent sentence set based on syntactic substitution
in this paper , we propose an automatic quantitative expansion method for a sentence set that contains sentences of the same meaning ( called an equivalent sentence set ) . this task is regarded as paraphrasing . the features of our method are : 1 ) the paraphrasing rules are dynamically acquired by hierarchical phrase alignment from the equivalent sentence set , and 2 ) a large equivalent sentence set is generated by substituting source syntactic structures . our experiments show that 561 sentences on average are correctly generated from 8.48 equivalent sentences .

mining large-scale parallel corpora from multilingual patents : an english-chinese example and its application to smt
in this paper , we demonstrate how to mine large-scale parallel corpora with multilingual patents , which have not been thoroughly explored before . we show how a large-scale english-chinese parallel corpus containing over 14 million sentence pairs with only 1-5 % wrong can be mined from a large amount of english-chinese bilingual patents . to our knowledge , this is the largest single parallel corpus in terms of sentence pairs . moreover , we estimate the potential for mining multilingual parallel corpora involving english , chinese , japanese , korean , german , etc. , which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing .

towards an optimal lexicalization in a natural-sounding portable natural language generator for dialog systems
in contrast to the latest progress in speech recognition , the state-of-the-art in natural language generation for spoken language dialog systems is lagging behind . the core dialog managers are now more sophisticated ; and natural-sounding and flexible output is expected , but not achieved with current simple techniques such as template-based systems . portability of systems across subject domains and languages is another increasingly important requirement in dialog systems . this paper presents an outline of legend , a system that is both portable and generates natural-sounding output . this goal is achieved through the novel use of existing lexical resources such as framenet and wordnet .

multilingual transliteration using feature based phonetic method
in this paper we investigate named entity transliteration based on a phonetic scoring method . the phonetic method is computed using phonetic features and carefully designed pseudo features . the proposed method is tested with four languages arabic , chinese , hindi and korean and one source language english , using comparable corpora . the proposed method is developed from the phonetic method originally proposed in tao et al ( 2006 ) . in contrast to the phonetic method in tao et al ( 2006 ) constructed on the basis of pure linguistic knowledge , the method in this study is trained using the winnow machine learning algorithm . there is salient improvement in hindi and arabic compared to the previous study . moreover , we demonstrate that the method can also achieve comparable results , when it is trained on language data different from the target language . the method can be applied both with minimal data , and without target language data for various languages .

aligning words in english-hindi parallel corpora niraj aswani robert gaizauskas
in this paper , we describe a word alignment algorithm for english-hindi parallel data . the system was developed to participate in the shared task on word alignment for languages with scarce resources at the acl 2005 workshop , on building and using parallel texts : data driven machine translation and beyond . our word alignment algorithm is based on a hybrid method which performs local word grouping on hindi sentences and uses other methods such as dictionary lookup , transliteration similarity , expected english words and nearest aligned neighbours . we trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in hindi sentences . the system scored 77.03 % precision and 60.68 % recall on the shared task unseen test data .

tbi-doc : generating patient & clinician reports from brain imaging
the tbi-doc prototype demonstrates the feasibility of automatically producing draft case reports for a new brain imaging technology , high definition fiber tracking ( hdft ) . here we describe the ontology for the hdft domain , the system architecture and our goals for future research and development .

active learning for post-editing based incrementally retrained mt
machine translation , in particular statistical machine translation ( smt ) , is making big inroads into the localisation and translation industry . in typical workflows ( s ) mt output is checked and ( where required ) manually post-edited by human translators . recently , a significant amount of research has concentrated on capturing human post-editing outputs as early as possible to incrementally update/modify smt models to avoid repeat mistakes . typically in these approaches , mt and post-edits happen sequentially and chronologically , following the way unseen data ( the translation job ) is presented . in this paper , we add to the existing literature addressing the question whether and if so , to what extent , this process can be improved upon by active learning , where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to ( whatever is ) the remaining data . we explore novel ( source side-only ) selection criteria and show performance increases of 0.67-2.65 points ter absolute on average on typical industry data sets compared to sequential pebased incrementally retrained smt .

building comparable corpora based on bilingual lda model
comparable corpora are important basic resources in cross-language information processing . however , the existing methods of building comparable corpora , which use intertranslate words and relative features , can not evaluate the topical relation between document pairs . this paper adopts the bilingual lda model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages . experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance .

conditional random fields for word hyphenation
finding allowable places in words to insert hyphens is an important practical problem . the algorithm that is used most often nowadays has remained essentially unchanged for 25 years . this method is the tex hyphenation algorithm of knuth and liang . we present here a hyphenation method that is clearly more accurate . the new method is an application of conditional random fields . we create new training sets for english and dutch from the celex european lexical resource , and achieve error rates for english of less than 0.1 % for correctly allowed hyphens , and less than 0.01 % for dutch . experiments show that both the knuth/liang method and a leading current commercial alternative have error rates several times higher for both languages .

unsupervised constraint driven learning for transliteration discovery
this paper introduces a novel unsupervised constraint-driven learning algorithm for identifying named-entity ( ne ) transliterations in bilingual corpora . the proposed method does not require any annotated data or aligned corpora . instead , it is bootstrapped using a simple resource a romanization table . we show that this resource , when used in conjunction with constraints , can efficiently identify transliteration pairs . we evaluate the proposed method on transliterating english nes to three different languages - chinese , russian and hebrew . our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models .

a human judgment corpus and a metric for arabic mt evaluation
we present a human judgments dataset and an adapted metric for evaluation of arabic machine translation . our mediumscale dataset is the first of its kind for arabic with high annotation quality . we use the dataset to adapt the bleu score for arabic . our score ( al-bleu ) provides partial credits for stem and morphological matchings of hypothesis and reference words . we evaluate bleu , meteor and al-bleu on our human judgments corpus and show that al-bleu has the highest correlation with human judgments . we are releasing the dataset and software to the research community .

biomedical event extraction from abstracts and full papers using
search-based structured prediction andreas vlachos and mark craven department of biostatistics and medical informatics university of wisconsin-madison { vlachos , craven } @ biostat.wisc.edu abstract in this paper we describe our approach to the bionlp 2011 shared task on biomedical event extraction from abstracts and full papers . we employ a joint inference system developed using the search-based structured prediction framework and show that it improves on a pipeline using the same features and it is better able to handle the domain shift from abstracts to full papers . in addition , we report on experiments using a simple domain adaptation method .

two improvements to left-to-right decoding for hierarchical phrase-based machine translation
left-to-right ( lr ) decoding ( watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( hiero ) that visits input spans in arbitrary order producing the output translation in left to right order . this leads to far fewer language model calls , but while lr decoding is more efficient than cky decoding , it is unable to capture some hierarchical phrase alignments reachable using cky decoding and suffers from lower translation quality as a result . this paper introduces two improvements to lr decoding that make it comparable in translation quality to cky-based hiero .

distributed asynchronous online learning for natural language processing carnegie mellon univeristy
recent speed-ups for training large-scale models like those found in statistical nlp exploit distributed computing ( either on multicore or cloud architectures ) and rapidly converging online learning algorithms . here we aim to combine the two . we focus on distributed , mini-batch learners that make frequent updates asynchronously ( nedic et al , 2001 ; langford et al , 2009 ) . we generalize existing asynchronous algorithms and experiment extensively with structured prediction problems from nlp , including discriminative , unsupervised , and non-convex learning scenarios . our results show asynchronous learning can provide substantial speedups compared to distributed and singleprocessor mini-batch algorithms with no signs of error arising from the approximate nature of the technique .

human effort and machine learnability in computer aided translation
analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections . however , this distinction is artificial in practice since the frontend and backend must work in concert . we present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : postediting and interactive machine translation ( mt ) . we describe a new translator interface , extensive modifications to a phrasebased mt system , and a novel objective function for re-tuning to human corrections . evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for french-english and englishgerman . however , re-tuning the mt system to interactive output leads to larger , statistically significant reductions in hter versus re-tuning to post-edit . analysis shows that tuning directly to hter results in fine-grained corrections to subsequent machine output .

building sentiment lexicons for all major languages
sentiment analysis in a multilingual world remains a challenging problem , because developing language-specific sentiment lexicons is an extremely resourceintensive process . such lexicons remain a scarce resource for most languages . in this paper , we address this lexicon gap by building high-quality sentiment lexicons for 136 major languages . we integrate a variety of linguistic resources to produce an immense knowledge graph . by appropriately propagating from seed words , we construct sentiment lexicons for each component language of our graph . our lexicons have a polarity agreement of 95.7 % with published lexicons , while achieving an overall coverage of 45.2 % . we demonstrate the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures wikipedia articles on 30 languages . despite cultural difference and the intended neutrality of wikipedia articles , our lexicons show an average sentiment correlation of 0.28 across all language pairs .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

fast recursive multi-class classification of pairs of text entities for biomedical event extraction
biomedical event extraction from articles has become a popular research topic driven by important applications , such as the automatic update of dedicated knowledge base . most existing approaches are either pipeline models of specific classifiers , usually subject to cascading errors , or joint structured models , more efficient but also more costly and more involved to train . we propose here a system based on a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities . such pairs are recursively provided to the classifier , so as to extract events involving other events as arguments . our model is more direct than the usual pipeline approaches , and speeds up inference compared to joint models . we report here the best results reported so far on the bionlp 2011 and 2013 genia tasks .

what decisions have you made : automatic decision detection in conversational speech
this study addresses the problem of automatically detecting decisions in conversational speech . we formulate the problem as classifying decision-making units at two levels of granularity : dialogue acts and topic segments . we conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts , and train maxent models using these features for the classification tasks . we find that models that combine lexical , prosodic , contextual and topical features yield the best results on both tasks , achieving 72 % and 86 % precision , respectively . the study also provides a quantitative analysis of the relative importance of the feature types .

discriminative reordering with chinese grammatical relations features
the prevalence in chinese of grammatical structures that translate into english in different word orders is an important cause of translation difficulty . while previous work has used phrase-structure parses to deal with such ordering problems , we introduce a richer set of chinese grammatical relations that describes more semantically abstract relations between words . using these chinese grammatical relations , we improve a phrase orientation classifier ( introduced by zens and ney ( 2006 ) ) that decides the ordering of two phrases when translated into english by adding path features designed over the chinese typed dependencies . we then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based mt system , and get significant bleu point gains on three test sets : mt02 ( +0.59 ) , mt03 ( +1.00 ) and mt05 ( +0.77 ) . our chinese grammatical relations are also likely to be useful for other nlp tasks .

sample selection for statistical parsers : cognitively driven
creating large amounts of manually annotated training data for statistical parsers imposes heavy cognitive load on the human annotator and is thus costly and error prone . it is hence of high importance to decrease the human efforts involved in creating training data without harming parser performance . for constituency parsers , these efforts are traditionally evaluated using the total number of constituents ( tc ) measure , assuming uniform cost for each annotated item . in this paper , we introduce novel measures that quantify aspects of the cognitive efforts of the human annotator that are not reflected by the tc measure , and show that they are well established in the psycholinguistic literature . we present a novel parameter based sample selection approach for creating good samples in terms of these measures . we describe methods for global optimisation of lexical parameters of the sample based on a novel optimisation problem , the constrained multiset multicover problem , and for cluster-based sampling according to syntactic parameters . our methods outperform previously suggested methods in terms of the new measures , while maintaining similar tc performance .

metonymy recognition using different kinds of context for a memory-based learner intelligent information and communication systems ( iics )
for the metonymy resolution task at semeval-2007 , the use of a memory-based learner to train classifiers for the identification of metonymic location names is investigated . metonymy is resolved on different levels of granularity , differentiating between literal and non-literal readings on the coarse level ; literal , metonymic , and mixed readings on the medium level ; and a number of classes covering regular cases of metonymy on a fine level . different kinds of context are employed to obtain different features : 1 ) a sequence of n1 synset ids representing subordination information for nouns and for verbs , 2 ) n2 prepositions , articles , modal , and main verbs in the same sentence , and 3 ) properties of n3 tokens in a context window to the left and to the right of the location name . different classifiers were trained on the mascara data set to determine which values for the context sizes n1 , n2 , and n3 yield the highest accuracy ( n1 = 4 , n2 = 3 , and n3 = 7 , determined with the leave-oneout method ) . results from these classifiers served as features for a combined classifier . in the training phase , the combined classifier achieved a considerably higher precision for the mascara data . in the semeval submission , an accuracy of 79.8 % on the coarse , 79.5 % on the medium , and 78.5 % on the fine level is achieved ( the baseline accuracy is 79.4 % ) .

dont worry about metaphor : affect extraction for conversational agents
we demonstrate one aspect of an affectextraction system for use in intelligent conversational agents . this aspect performs a degree of affective interpretation of some types of metaphorical utterance .

reducing annotation effort for quality estimation via active learning
quality estimation models provide feedback on the quality of machine translated texts . they are usually trained on humanannotated datasets , which are very costly due to its task-specific nature . we investigate active learning techniques to reduce the size of these datasets and thus annotation effort . experiments on a number of datasets show that with as little as 25 % of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets . in other words , our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors .

polly : a conversational system that uses a shared representation to
we present a demo of our conversational system polly ( politeness in language learning ) which uses a common planning representation to generate actions to be performed by embodied agents in a virtual environment and to generate spoken utterances for dialogues about the steps involved in completing the task . in order to generate socially appropriate dialogue , brown and levinsons theory of politeness is used to constrain the dialogue generation process .

weakly supervised morphology learning for agglutinating languages using small training sets
the paper describes a weakly supervised approach for decomposing words into all morphemes : stems , prefixes and suffixes , using wordforms with marked stems as training data . as we concentrate on under-resourced languages , the amount of training data is limited and we need some amount of supervision in the form of a small number of wordforms with marked stems . in the first stage we introduce a new supervised stem extraction algorithm ( sse ) . once stems have been extracted , an improved unsupervised segmentation algorithm gbums ( graphbased unsupervised morpheme segmentation ) is used to segment suffix or prefix sequences into individual suffixes and prefixes . the approach , experimentally validated on turkish and isizulu languages , gives high performance on test data and is comparable to a fully supervised method .

contradictions and justifications : extensions to the textual entailment task
the third pascal recognizing textual entailment challenge ( rte-3 ) contained an optional task that extended the main entailment task by requiring a system to make three-way entailment decisions ( entails , contradicts , neither ) and to justify its response . contradiction was rare in the rte-3 test set , occurring in only about 10 % of the cases , and systems found accurately detecting it difficult . subsequent analysis of the results shows a test set must contain many more entailment pairs for the three-way decision task than the traditional two-way task to have equal confidence in system comparisons . each of six human judges representing eventual end users rated the quality of a justification by assigning understandability and correctness scores . ratings of the same justification across judges differed significantly , signaling the need for a better characterization of the justification task .

reranking bilingually extracted paraphrases using monolingual distributional similarity tsz ping chan , chris callison-burch and benjamin van durme
this paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases . raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods . our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does . the results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection .

uniba : combining distributional semantic models and word sense disambiguation for textual similarity
this paper describes the uniba team participation in the cross-level semantic similarity task at semeval 2014. we propose to combine the output of different semantic similarity measures which exploit word sense disambiguation and distributional semantic models , among other lexical features . the integration of similarity measures is performed by means of two supervised methods based on gaussian process and support vector machine . our systems obtained very encouraging results , with the best one ranked 6 th out of 38 submitted systems .

using n-gram and word network features for native language identification shibamouli lahiri rada mihalcea
we report on the performance of two different feature sets in the native language identification shared task ( tetreault et al , 2013 ) . our feature sets were inspired by existing literature on native language identification and word networks . experiments show that word networks have competitive performance against the baseline feature set , which is a promising result . we also present a discussion of feature analysis based on information gain , and an overview on the performance of different word network features in the native language identification task .

learning to identify definitions using syntactic features
this paper describes an approach to learning concept definitions which operates on fully parsed text . a subcorpus of the dutch version of wikipedia was searched for sentences which have the syntactic properties of definitions . next , we experimented with various text classification techniques to distinguish actual definitions from other sentences . a maximum entropy classifier which incorporates features referring to the position of the sentence in the document as well as various syntactic features , gives the best results .

german particle verbs and pleonastic prepositions
this paper discusses the behaviour of german particle verbs formed by two-way prepositions in combination with pleonastic pps including the verb particle as a preposition . these particle verbs have a characteristic feature : some of them license directional prepositional phrases in the accusative , some only allow for locative pps in the dative , and some particle verbs can occur with pps in the accusative and in the dative . directional particle verbs together with directional pps present an additional problem : the particle and the preposition in the pp seem to provide redundant information . the paper gives an overview of the semantic verb classes influencing this phenomenon , based on corpus data , and explains the underlying reasons for the behaviour of the particle verbs . we also show how the restrictions on particle verbs and pleonastic pps can be expressed in a grammar theory like lexical functional grammar ( lfg ) .

norwegian sami parliament
this paper describes an annotation system for sami language corpora , which consists of structured , running texts . the annotation of the texts is fully automatic , starting from the original documents in different formats . the texts are first extracted from the original documents preserving the original structural markup . the markup is enhanced by a document-specific xslt script which contains document-specific formatting instructions . the overall maintenance is achieved by system-wide xslt scripts .

generalizing local and non-local word-reordering patterns for syntax-based machine translation
syntactic word reordering is essential for translations across different grammar structures between syntactically distant languagepairs . in this paper , we propose to embed local and non-local word reordering decisions in a synchronous context free grammar , and leverages the grammar in a chartbased decoder . local word-reordering is effectively encoded in hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to sourceside syntactic constituents . we demonstrate how these rules are learned from parallel corpora . our proposed shallow tree-to-string rules show significant improvements in translation quality across different test sets .

word sense induction using lexical chain based hypergraph model
word sense induction is a task of automatically finding word senses from large scale texts . it is generally considered as an unsupervised clustering problem . this paper introduces a hypergraph model in which nodes represent instances of contexts where a target word occurs and hyperedges represent higher-order semantic relatedness among instances . a lexical chain based method is used for discovering the hyperedges , and hypergraph clustering methods are used for finding word senses among the context instances . experiments show that this model outperforms other methods in supervised evaluation and achieves comparable performance with other methods in unsupervised evaluation .

itu treebank annotation tool
in this paper , we present a treebank annotation tool developed for processing turkish sentences . the tool consists of three different annotation stages ; morphological analysis , morphological disambiguation and syntax analysis . each of these stages are integrated with existing analyzers in order to guide human annotators . our semiautomatic treebank annotation tool is currently used both for creating new data sets and correcting the existing turkish treebank .

a pattern-based analyzer for french in the context of spoken language translation : first prototype and evaluation
in this paper , we describe a first prototype of a pattern-based analyzer developed in the context of a speech-to-speech translation project using a pivot-based approach ( the pivot is called if ) . the chosen situation involves a french client talking to an italian travel agent ( both in their own language ) to organize a stay in the trentino area . an if consists of a dialogue act , and a list , possibly empty , of argument values . the analyzer applies a `` phrase spotting '' mechanism on the output of the speech recognition module . it finds well-formed phrases corresponding to argument values . a dialogue act is then built according to the instantiated arguments and some other features of the input . the current version of the prototype has been involved in an evaluation campaign on an unseen corpus of four dialogues consisting of 235 speech turns . the results are given and commented in the last part of the paper . we think they pave the way for future enhancements to both the coverage and the development methodology . rsum dans cet article , nous dcrivons la premire version d'un analyseur fond sur des patrons dans le contexte d'un projet de traduction de parole utilisant une technique de traduction par pivot ( le pivot est appel if ) .

using encyclopedic knowledge for automatic topic identification
this paper presents a method for automatic topic identification using an encyclopedic graph derived from wikipedia . the system is found to exceed the performance of previously proposed machine learning algorithms for topic identification , with an annotation consistency comparable to human annotations .

crfs-based named entity recognition incorporated with heuristic entity list searching
chinese named entity recognition is one of the most important tasks in nlp . two kinds of challenges we confront are how to improve the performance in one corpus and keep its performance in another different corpus . we use a combination of statistical models , i.e . a language model to recognize person names and two crfs models to recognize location names and organization names respectively . we also incorporate an efficient heuristic named entity list searching process into the framework of statistical model in order to improve both the performance and the adaptability of the statistical ner system . we participate in the ner tests on open tracks of msra . the testing results show that our system can performs well .

online word alignment for online adaptive machine translation
a hot task in the computer assisted translation scenario is the integration of machine translation ( mt ) systems that adapt sentence after sentence to the postedits made by the translators . a main role in the mt online adaptation process is played by the information extracted from source and post-edited sentences , which in turn depends on the quality of the word alignment between them . in fact , this step is particularly crucial when the user corrects the mt output with words for which the system has no prior information . in this paper , we first discuss the application of popular state-of-the-art word aligners to this scenario and reveal their poor performance in aligning unknown words . then , we propose a fast procedure to refine their outputs and to get more reliable and accurate alignments for unknown words . we evaluate our enhanced word-aligner on three language pairs , namely english-italian , englishfrench , and english-spanish , showing a consistent improvement in aligning unknown words up to 10 % absolute fmeasure .

multilingual dependency parsing and domain adaptation
we describe our experiments using the desr parser in the multilingual and domain adaptation tracks of the conll 2007 shared task . desr implements an incremental deterministic shift/reduce parsing algorithm , using specific rules to handle non-projective dependencies . for the multilingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language . for the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain .

statistical bistratal dependency parsing
we present an inexact search algorithm for the problem of predicting a two-layered dependency graph . the algorithm is based on a k-best version of the standard cubictime search algorithm for projective dependency parsing , which is used as the backbone of a beam search procedure . this allows us to handle the complex nonlocal feature dependencies occurring in bistratal parsing if we model the interdependency between the two layers . we apply the algorithm to the syntactic semantic dependency parsing task of the conll-2008 shared task , and we obtain a competitive result equal to the highest published for a system that jointly learns syntactic and semantic structure .

linguistics in computational linguistics :
as my title suggests , this position paper focuses on the relevance of linguistics in nlp instead of asking the inverse question . although the question about the role of computational linguistics in the study of language may theoretically be much more interesting than the selected topic , i feel that my choice is more appropriate for the purpose and context of this workshop . this position paper starts with some retrospective observations clarifying my view on the ambivalent and multi-facetted relationship between linguistics and computational linguistics as it has evolved from both applied and theoretical research on language processing . in four brief points i will then strongly advocate a strengthened relationship from which both sides benefit . first , i will observe that recent developments in both deep linguistic processing and statistical nlp suggest a certain plausible division of labor between the two paradigms . second , i want to propose a systematic approach to research on hybrid systems which determines optimal combinations of the paradigms and continuously monitors the division of labor as both paradigm progress . concrete examples illustrating the proposal are taken from our own research . third , i will argue that a central vision of computational linguistics is still alive , the dream of a formalized reusable linguistic knowledge source embodying the core competence of a language that can be utilized for wide range of applications .

sentence-level rewriting detection
writers usually need iterations of revisions and edits during their writings . to better understand the process of rewriting , we need to know what has changed between the revisions . prior work mainly focuses on detecting corrections within sentences , which is at the level of words or phrases . this paper proposes to detect revision changes at the sentence level . looking at revisions at a higher level allows us to have a different understanding of the revision process . this paper also proposes an approach to automatically detect sentence revision changes . the proposed approach shows high accuracy in an evaluation using first and final draft essays from an undergraduate writing class .

exploring the potential of intractable parsers
we revisit the idea of history-based parsing , and present a history-based parsing framework that strives to be simple , general , and flexible . we also provide a decoder for this probability model that is linear-space , optimal , and anytime . a parser based on this framework , when evaluated on section 23 of the penn treebank , compares favorably with other stateof-the-art approaches , in terms of both accuracy and speed .

the importance of sub-utterance prosody in predicting level of certainty
we present an experiment aimed at understanding how to optimally use acoustic and prosodic information to predict a speakers level of certainty . with a corpus of utterances where we can isolate a single word or phrase that is responsible for the speakers level of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterances perceived level of certainty . our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole .

iscasa system for chinese word sense induction based on
this paper presents an unsupervised method for automatic chinese word sense induction . the algorithm is based on clustering the similar words according to the contexts in which they occur . first , the target word which needs to be disambiguated is represented as the vector of its contexts . then , reconstruct the matrix constituted by the vectors of target words through singular value decomposition ( svd ) method , and use the vectors to cluster the similar words . our system participants in clp2010 back off task4-chinese word sense induction .

left-corner transitions on dependency parsing
we propose a transition system for dependency parsing with a left-corner parsing strategy . unlike parsers with conventional transition systems , such as arc-standard or arc-eager , a parser with our system correctly predicts the processing difficulties people have , such as of center-embedding . we characterize our transition system by comparing its oracle behaviors with those of other transition systems on treebanks of 18 typologically diverse languages . a crosslinguistical analysis confirms the universality of the claim that a parser with our system requires less memory for parsing naturally occurring sentences .

cheap and easy entity evaluation ben hachey joel nothman will radford
the aida-yago dataset is a popular target for whole-document entity recognition and disambiguation , despite lacking a shared evaluation tool . we review evaluation regimens in the literature while comparing the output of three approaches , and identify research opportunities . this utilises our open , accessible evaluation tool . we exemplify a new paradigm of distributed , shared evaluation , in which evaluation software and standardised , versioned system outputs are provided online .

chasing hypernyms in vector spaces with entropy
in this paper , we introduce slqs , a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in distributional semantic models ( dsms ) . slqs is assessed through two tasks : ( i . ) identifying the hypernym in hyponym-hypernym pairs , and ( ii . ) discriminating hypernymy among various semantic relations . in both tasks , slqs outperforms other state-of-the-art measures .

an n-gram frequency database reference to handle mwe extraction in nlp
the identification and extraction of multiword expressions ( mwes ) currently deliver satisfactory results . however , the integration of these results into a wider application remains an issue . this is mainly due to the fact that the association measures ( ams ) used to detect mwes require a critical amount of data and that the mwe dictionaries can not account for all the lexical and syntactic variations inherent in mwes . in this study , we use an alternative technique to overcome these limitations . it consists in defining an n-gram frequency database that can be used to compute ams on-thefly , allowing the extraction procedure to efficiently process all the mwes in a text , even if they have not been previously observed .

towards a framework for learning structured shape models from
we present on-going work on the topic of learning translation models between image data and text ( english ) captions . most approaches to this problem assume a one-to-one or a flat , oneto-many mapping between a segmented image region and a word . however , this assumption is very restrictive from the computer vision standpoint , and fails to account for two important properties of image segmentation : 1 ) objects often consist of multiple parts , each captured by an individual region ; and 2 ) individual regions are often over-segmented into multiple subregions . moreover , this assumption also fails to capture the structural relations among words , e.g. , part/whole relations . we outline a general framework that accommodates a many-to-many mapping between image regions and words , allowing for structured descriptions on both sides . in this paper , we describe our extensions to the probabilistic translation model of brown et al ( 1993 ) ( as in duygulu et al ( 2002 ) ) that enable the creation of structured models of image objects . we demonstrate our work in progress , in which a set of annotated images is used to derive a set of labeled , structured descriptions in the presence of oversegmentation .

resumptive pronoun detection for modern standard arabic to english mt
many languages , including modern standard arabic ( msa ) , insert resumptive pronouns in relative clauses , whereas many others , such as english , do not , using empty categories instead . this discrepancy is a source of difficulty when translating between these languages because there are words in one language that correspond to empty categories in the other , and these words must either be inserted or deleteddepending on translation direction . in this paper , we first examine challenges presented by resumptive pronouns in msa-english translations and review resumptive pronoun translations generated by a popular online msa-english mt engine . we then present what is , to the best of our knowledge , the first system for automatic identification of resumptive pronouns . the system achieves 91.9 f1 and 77.8 f1 on arabic treebank data when using gold standard parses and automatic parses , respectively .

deep syntactic annotation : tectogrammatical representation and beyond and applied linguistics
the requirements of the depth and precision of annotation vary for different intended uses of the corpus but it has been commonly accepted nowadays that the standard annotations of surface structure are only the first steps in a more ambitious research program , aiming at a creation of advanced resources for most different systems of natural language processing and for testing and further enrichment of linguistic and computational theories . among the several possible directions in which we believe the standard annotation systems should go ( and in some cases already attempt to go ) beyond the pos tagging or shallow syntactic annotations , the following four are characterized in the present contribution : ( i ) predicateargument representation of the underlying syntactic relations as basically corresponding to a rooted tree that can be univocally linearized , ( ii ) the inclusion of the information structure using very simple means ( the left-to-right order of the nodes and three attribute values ) , ( iii ) relating this underlying structure ( rendering the linguistic meaning , i.e . the semantically relevant counterparts of the grammatical means of expression ) to certain central aspects of referential semantics ( reference assignment and coreferential relations ) , and ( iv ) handling of word sense disambiguation . the first three issues are documented in the present paper on the basis of our experience with the development of the structure and scenario of the prague dependency treebank which provides for syntactico-semantic annotation of large text segments from the czech national corpus and which is based on a solid theoretical framework .

precision-focused textual inference
this paper describes our system as used in the rte3 task . the system maps premise and hypothesis pairs into an abstract knowledge representation ( akr ) and then performs entailment and contradiction detection ( ecd ) on the resulting akrs . two versions of ecd were used in rte3 , one with strict ecd and one with looser ecd .

infoxtract location normalization : a hybrid approach to geographic references in information extraction
ambiguity is very high for location names . for example , there are 23 cities named buffalo in the u.s. based on our previous work , this paper presents a refined hybrid approach to geographic references using our information extraction engine infoxtract . the infoxtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses . multiple knowledge sources are used in a number of ways : ( i ) pattern matching driven by local context , ( ii ) maximum spanning tree search for discourse analysis , and ( iii ) applying default sense heuristics and extracting default senses from the web . the results are benchmarked with 96 % accuracy on our test collections that consist of both news articles and tourist guides . the performance contribution for each component of the module is also benchmarked and discussed .

time after time : representing time in literary texts
the representation of temporal information in text represents a significant computational challenge . the problem is particularly acute in the case of literary texts , which tend to be massively underspecified , often relying on a network of semantic relations to establish times and timings . this paper shows how a model based on threaded directed acyclic graphs makes it possible to capture a range of subtle temporal information in this type of text and argues for an onomasiological approach which places meaning before form .

mining opinion words and opinion targets in a two-stage framework
this paper proposes a novel two-stage method for mining opinion words and opinion targets . in the first stage , we propose a sentiment graph walking algorithm , which naturally incorporates syntactic patterns in a sentiment graph to extract opinion word/target candidates . then random walking is employed to estimate confidence of candidates , which improves extraction accuracy by considering confidence of patterns . in the second stage , we adopt a self-learning strategy to refine the results from the first stage , especially for filtering out high-frequency noise terms and capturing the long-tail terms , which are not investigated by previous methods . the experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods .

dr sentiment knows everything !
sentiment analysis is one of the hot demanding research areas since last few decades . although a formidable amount of research have been done , the existing reported solutions or available systems are still far from perfect or do not meet the satisfaction level of end users . the main issue is the various conceptual rules that govern sentiment and there are even more clues ( possibly unlimited ) that can convey these concepts from realization to verbalization of a human being . human psychology directly relates to the unrevealed clues and governs the sentiment realization of us . human psychology relates many things like social psychology , culture , pragmatics and many more endless intelligent aspects of civilization . proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem . in the present paper we propose a template based online interactive gaming technology , called dr sentiment to automatically create the psychosentiwordnet involving internet population . the psychosentiwordnet is an extension of sentiwordnet that presently holds human psychological knowledge on a few aspects along with sentiment knowledge .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

multi-lingual dependency parsing at naist
in this paper , we present a framework for multi-lingual dependency parsing . our bottom-up deterministic parser adopts nivres algorithm ( nivre , 2004 ) with a preprocessor . support vector machines ( svms ) are utilized to determine the word dependency attachments . then , a maximum entropy method ( maxent ) is used for determining the label of the dependency relation . to improve the performance of the parser , we construct a tagger based on svms to find neighboring attachment as a preprocessor . experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages . ( haji et al , 2004 ; simov et al , 2005 ; simov and osenova , 2003 ; chen et al , 2003 ; bhmov et al , 2003 ; kromann , 2003 ; van der beek et al , 2002 ; brants et al , 2002 ; kawata and bartels , 2000 ; afonso et al , 2002 ; deroski et al , 2006 ; civit and mart , 2002 ; nilsson et al , 2005 ; oflazer et al , 2003 ; atalay et al , 2003 ) .

naive bayes word sense induction do kook choe
we introduce an extended naive bayes model for word sense induction ( wsi ) and apply it to a wsi task . the extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense . the proposed model is very simple yet effective when evaluated on semeval-2010 wsi data .

a mechanism to provide language-encoding support and an nlp friendly anil kumar singh
many languages of the world ( some with very large numbers of native speakers ) are not yet supported on computers . in this paper we first present a simple method to provide an extra layer of easily customizable language-encoding support for less computerized languages . we then describe an editor called sanchay editor , which uses this method and also has many other facilities useful for those using less computerized languages for simple text editing or for natural language processing purposes , especially for annotation .

learning alignments and leveraging natural logic
we describe an approach to textual inference that improves alignments at both the typed dependency level and at a deeper semantic level . we present a machine learning approach to alignment scoring , a stochastic search procedure , and a new tool that finds deeper semantic alignments , allowing rapid development of semantic features over the aligned graphs . further , we describe a complementary semantic component based on natural logic , which shows an added gain of 3.13 % accuracy on the rte3 test set .

a new feature selection score for multinomial naive bayes text classification based on kl-divergence
we define a new feature selection score for text classification based on the kl-divergence between the distribution of words in training documents and their classes . the score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes . experiments on two standard data sets indicate that the new method outperforms mutual information , especially for smaller categories .

integrating logical representations with probabilistic information using markov logic
first-order logic provides a powerful and flexible mechanism for representing natural language semantics . however , it is an open question of how best to integrate it with uncertain , probabilistic knowledge , for example regarding word meaning . this paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of statistical relational ai . specifically , we show how discourse representation structures can be combined with distributional models for word meaning inside a markov logic network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context .

generation of vietnamese for french-vietnamese and englishvietnamese machine translation
this paper presents the implementation of the vietnamese generation module in its3 , a multilingual machine translation ( mt ) system based on the government & binding ( gb ) theory . despite well-designed generic mechanisms of the system , it turned out that the task of generating vietnamese posed non-trivial problems . we therefore had to deviate from the generic code and make new design and implementation in many important cases . by developing corresponding bilingual lexicons , we obtained prototypes of french-vietnamese and english-vietnamese mt , the former being the first known prototype of this kind . our experience suggests that in a principle-based generation system , the parameterized modules , which contain language-specific and lexicalized properties , deserve more attention , and the generic mechanisms should be flexible enough to facilitate the integration of these modules .

modeling of long distance context dependency in chinese
ngram modeling is simple in language modeling and has been widely used in many applications . however , it can only capture the short distance context dependency within an n-word window where the largest practical n for natural language is three . in the meantime , much of context dependency in natural language occurs beyond a three-word window . in order to incorporate this kind of long distance context dependency , this paper proposes a new mi-ngram modeling approach . the mi-ngram model consists of two components : an ngram model and an mi model . the ngram model captures the short distance context dependency within an n-word window while the mi model captures the long distance context dependency between the word pairs beyond the n-word window by using the concept of mutual information . it is found that mi-ngram modeling has much better performance than ngram modeling . evaluation on the xinhua new corpus of 29 million words shows that inclusion of the best 1,600,000 word pairs decreases the perplexity of the mi-trigram model by 20 percent compared with the trigram model . in the meanwhile , evaluation on chinese word segmentation shows that about 35 percent of errors can be corrected by using the mi-trigram model compared with the trigram model .

investigations on event evolution in tdt
topic detection and tracking approaches monitor broadcast news in order to spot new , previously unreported events and to track the development of the previously spotted ones . the dynamical nature of the events makes the use of state-of-the-art methods difficult . we present a new topic definition that has potential to model evolving events . we also discuss incorporating ontologies into the similarity measures of the topics , and illustrate a dynamic hierarchy that decreases the exhaustive computation performed in the tdt process . this is mainly work-in-progress .

ai azuma yuji matsumoto
in this paper , we describe a novel approach to cascaded learning and inference on sequences . we propose a weakly joint learning model on cascaded inference on sequences , called multilayer sequence labeling . in this model , inference on sequences is modeled as cascaded decision . however , the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them . it is not novel itself , but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses . we also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function . one of the dynamic programming algorithms resembles back propagation algorithm for multilayer feed-forward neural networks . the other is a generalized version of the forwardbackward algorithm . we also report experiments of cascaded part-of-speech tagging and chunking of english sentences and show effectiveness of the proposed method .

tracing actions helps in understanding interactions
integration of new utterances into context is a central task in any model for rational ( human-machine ) dialogues in natural language . in this paper , a pragmatics-first approach to specifying the meaning of utterances in terms of plans is presented . a rational dialogue is driven by the reaction of dialogue participants on how they find their expectations on changes in the environment satisfied by their observations of the outcome of performed actions . we present a computational model for this view on dialogues and illustrate it with examples from a real-world application .

using automatically acquired predominant senses for word sense
in word sense disambiguation ( wsd ) , the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed . the first ( or predominant ) sense heuristic assumes the availability of handtagged data . whilst there are hand-tagged corpora available for some languages , these are relatively small in size and many word forms either do not occur , or occur infrequently . in this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text . we evaluate on both the senseval-2 and senseval-3 english allwords data . for accurate wsd the first sense heuristic should be used only as a back-off , where the evidence from the context is not strong enough . in this paper however , we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from semcor outperformed many systems in senseval-2 .

learning to differentiate better from worse translations an shafiq joty llus m alessandro moschitti preslav nakov massimo nicosia
we present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference . we integrate several layers of linguistic information encapsulated in tree-based structures , making use of both the reference and the system output simultaneously , thus bringing our ranking closer to how humans evaluate translations . most importantly , instead of deciding upfront which types of features are important , we use the learning framework of preference re-ranking kernels to learn the features automatically . the evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures . also , we show our structural kernel learning ( skl ) can be a general framework for mt evaluation , in which syntactic and semantic information can be naturally incorporated .

priberam : a turbo semantic parser with second order features
this paper presents our contribution to the semeval-2014 shared task on broadcoverage semantic dependency parsing . we employ a feature-rich linear model , including scores for first and second-order dependencies ( arcs , siblings , grandparents and co-parents ) . decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition ( ad 3 ) . our system achieved the top score in the open challenge , and the second highest score in the closed track .

a large-scale inheritance-based morphological lexicon for russian
in this paper we describe the mapping of zaliznjaks ( 1977 ) morphological classes into the lexical representation language datr ( evans and gazdar 1996 ) . on the basis of the resulting datr theory a set of fully inflected forms together with their associated morphosyntax can automatically be generated from the electronic version of zaliznjaks dictionary ( ilola and mustajoki 1989 ) . from this data we plan to develop a wide-coverage morphosyntactic lemmatizer and tagger for russian .

using a corpus of sentence orderings defined by many experts to evaluate metrics of coherence for text structuring
this paper addresses two previously unresolved issues in the automatic evaluation of text structuring ( ts ) in natural language generation ( nlg ) . first , we describe how to verify the generality of an existing collection of sentence orderings defined by one domain expert using data provided by additional experts . second , a general evaluation methodology is outlined which investigates the previously unaddressed possibility that there may exist many optimal solutions for ts in the employed domain . this methodology is implemented in a set of experiments which identify the most promising candidate for ts among several metrics of coherence previously suggested in the literature.1

learning entailment rules for unary templates
most work on unsupervised entailment rule acquisition focused on rules between templates with two variables , ignoring unary rules - entailment rules between templates with a single variable . in this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method . the results show that the learned unary rule-sets outperform the binary rule-set . in addition , a novel directional similarity measure for learning entailment , termed balanced-inclusion , is the best performing measure .

mime - nlg in pre-hospital care
the cross-disciplinary mime project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians . nlg is used to produce a textual handover report at any time , summarising data from novel medical sensors , as well as observations and actions recorded by the carer . we describe the mime project with a focus on the nlg algorithm and an initial evaluation of the generated reports .

latent domain phrase-based models for adaptation
phrase-based models directly trained on mix-of-domain corpora can be sub-optimal . in this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus . we derive an em algorithm which alternates between inducing domain-focused phrase pair estimates , and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task . by embedding our latent domain phrase model in a sentence-level model and training the two in tandem , we are able to adapt all core translation components together phrase , lexical and reordering . we show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models , showing significant performance improvement in both tasks .

parsing arguments of nominalizations in english and chinese
in this paper , we use a machine learning framework for semantic argument parsing , and apply it to the task of parsing arguments of eventive nominalizations in the framenet database . we create a baseline system using a subset of features introduced by gildea and jurafsky ( 2002 ) , which are directly applicable to nominal predicates . we then investigate new features which are designed to capture the novelties in nominal argument structure and show a significant performance improvement using these new features . we also investigate the parsing performance of nominalizations in chinese and compare the salience of the features for the two languages .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

patterns of importance variation in spoken dialog
some things people say are more important , and some less so . importance varies from moment to moment in spoken dialog , and contextual prosodic features and patterns signal this . a simple linear regression model over such features gave estimates that correlated well , 0.83 , with human importance judgments .

a hybrid approach for named entity recognition in indian languages
in this paper we describe a hybrid system that applies maximum entropy model ( maxent ) , language specific rules and gazetteers to the task of named entity recognition ( ner ) in indian languages designed for the ijcnlp nersseal shared task . starting with named entity ( ne ) annotated corpora and a set of features we first build a baseline ner system . then some language specific rules are added to the system to recognize some specific ne classes . also we have added some gazetteers and context patterns to the system to increase the performance . as identification of rules and context patterns requires language knowledge , we were able to prepare rules and identify context patterns for hindi and bengali only . for the other languages the system uses the maxent model only . after preparing the one-level ner system , we have applied a set of rules to identify the nested entities . the system is able to recognize 12 classes of nes with 65.13 % f-value in hindi , 65.96 % f-value in bengali and 44.65 % , 18.74 % , and 35.47 % f-value in oriya , telugu and urdu respectively .

training mrf-based phrase translation models using gradient ascent
this paper presents a general , statistical framework for modeling phrase translation via markov random fields . the model allows for arbituary features extracted from a phrase pair to be incorporated as evidence . the parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an n-best list based expected bleu as the objective function . the model is easy to be incoporated into a standard phrase-based statistical machine translation system , requiring no code change in the runtime engine . evaluation is performed on two europarl translation tasks , germanenglish and french-english . results show that incoporating the markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system , leading to a gain of 0.8-1.3 bleu points .

optimizing features in active machine learning for complex qualitative content analysis jasy liew suet yan
we propose a semi-automatic approach for content analysis that leverages machine learning ( ml ) being initially trained on a small set of hand-coded data to perform a first pass in coding , and then have human annotators correct machine annotations in order to produce more examples to retrain the existing model incrementally for better performance . in this active learning approach , it is equally important to optimize the creation of the initial ml model given less training data so that the model is able to capture most if not all positive examples , and filter out as many negative examples as possible for human annotators to correct . this paper reports our attempt to optimize the initial ml model through feature exploration in a complex content analysis project that uses a multidimensional coding scheme , and contains codes with sparse positive examples . while different codes respond optimally to different combinations of features , we show that it is possible to create an optimal initial ml model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus .

ipa and stout : leveraging linguistic and source-based features for machine translation evaluation
this paper describes the upc submissions to the wmt14 metrics shared task : upcipa and upc-stout . these metrics use a collection of evaluation measures integrated in asiya , a toolkit for machine translation evaluation . in addition to some standard metrics , the two submissions take advantage of novel metrics that consider linguistic structures , lexical relationships , and semantics to compare both source and reference translation against the candidate translation . the new metrics are available for several target languages other than english . in the the official wmt14 evaluation , upc-ipa and upc-stout scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level .

question classification using head words and their hypernyms
question classification plays an important role in question answering . features are the key to obtain an accurate question classifier . in contrast to li and roth ( 2002 ) s approach which makes use of very rich feature space , we propose a compact yet effective feature set . in particular , we propose head word feature and present two approaches to augment semantic features of such head words using wordnet . in addition , lesks word sense disambiguation ( wsd ) algorithm is adapted and the depth of hypernym feature is optimized . with further augment of other standard features such as unigrams , our linear svm and maximum entropy ( me ) models reach the accuracy of 89.2 % and 89.0 % respectively over a standard benchmark dataset , which outperform the best previously reported accuracy of 86.2 % .

youve got answers : towards personalized models for predicting success in community question answering
question answering communities such as yahoo ! answers have emerged as a popular alternative to general-purpose web search . by directly interacting with other participants , information seekers can obtain specific answers to their questions . however , user success in obtaining satisfactory answers varies greatly . we hypothesize that satisfaction with the contributed answers is largely determined by the askers prior experience , expectations , and personal preferences . hence , we begin to develop personalized models of asker satisfaction to predict whether a particular question author will be satisfied with the answers contributed by the community participants . we formalize this problem , and explore a variety of content , structure , and interaction features for this task using standard machine learning techniques . our experimental evaluation over thousands of real questions indicates that indeed it is beneficial to personalize satisfaction predictions when sufficient prior user history exists , significantly improving accuracy over a one-size-fits-all prediction model .

modeling human inference process for textual entailment recognition
this paper aims at understanding what human think in textual entailment ( te ) recognition process and modeling their thinking process to deal with this problem . we first analyze a labeled rte-5 test set and find that the negative entailment phenomena are very effective features for te recognition . then , a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically . we evaluate the performance of using the negative entailment phenomena on both the english rte-5 dataset and chinese ntcir-9 rite dataset , and conclude the same findings .

sebastian riedel ivan meza-ruiz
this paper presents our system for the open track of the conll 2008 shared task ( surdeanu et al , 2008 ) in joint dependency parsing 1 and semantic role labelling . we use markov logic to define a joint srl model and achieve a semantic f-score of 74.59 % , the second best in the open track .

querying xml documents with multi-dimensional markup
xml documents annotated by different nlp tools accommodate multidimensional markup in a single hierarchy . to query such documents one has to account for different possible nesting structures of the annotations and the original markup of a document . we propose an expressive pattern language with extended semantics of the sequence pattern , supporting negation , permutation and regular patterns that is especially appropriate for querying xml annotated documents with multi-dimensional markup . the concept of fuzzy matching allows matching of sequences that contain textual fragments and known xml elements independently of how concurrent annotations and original markup are merged . we extend the usual notion of sequence as a sequence of siblings allowing matching of sequence elements on the different levels of nesting and abstract so from the hierarchy of the xml document . extended sequence semantics in combination with other language patterns allows more powerful and expressive queries than queries based on regular patterns .

deterministic shift-reduce parsing for unification-based grammars by using default unification
many parsing techniques including parameter estimation assume the use of a packed parse forest for efficient and accurate parsing . however , they have several inherent problems deriving from the restriction of locality in the packed parse forest . deterministic parsing is one of solutions that can achieve simple and fast parsing without the mechanisms of the packed parse forest by accurately choosing search paths . we propose ( i ) deterministic shift-reduce parsing for unification-based grammars , and ( ii ) best-first shift-reduce parsing with beam thresholding for unification-based grammars . deterministic parsing can not simply be applied to unification-based grammar parsing , which often fails because of its hard constraints . therefore , it is developed by using default unification , which almost always succeeds in unification by overwriting inconsistent constraints in grammars .

an agent-based approach to chinese named entity recognition
chinese ne ( named entity ) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure . this paper proposes the use of a rationality model in a multi-agent framework to tackle this problem . we employ a greedy strategy and use the ne rationality model to evaluate and detect all possible nes in the text . we then treat the process of selecting the best possible nes as a multi-agent negotiation problem . the resulting system is robust and is able to handle different types of ne effectively . our test on the met-2 test corpus indicates that our system is able to achieve high f1 values of above 92 % on all ne types .

building a shallow arabic morphological analyzer in one day
the paper presents a rapid method of developing a shallow arabic morphological analyzer . the analyzer will only be concerned with generating the possible roots of any given arabic word . the analyzer is based on automatically derived rules and statistics . for evaluation , the analyzer is compared to a commercially available arabic morphological analyzer .

outclassing wikipedia in open-domain information extraction : weakly-supervised acquisition of attributes over conceptual hierarchies
a set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy . besides a significant increase in the coverage of the class labels assigned to individual instances , the resulting resource of labeled classes is more effective than similar data derived from the manually-created wikipedia , in the task of attribute extraction over conceptual hierarchies .

how is meaning grounded in dictionary definitions
meaning can not be based on dictionary definitions all the way down : at some point the circularity of definitions must be broken in some way , by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution . this is the symbol grounding problem . we introduce the concept of a reachable set a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone , as long as the meanings of the smaller vocabulary are themselves already grounded . we provide simple algorithms to compute reachable sets for any given dictionary .

semantic-based multilingual document clustering via tensor modeling
a major challenge in document clustering research arises from the growing amount of text data written in different languages . previous approaches depend on language-specific solutions ( e.g. , bilingual dictionaries , sequential machine translation ) to evaluate document similarities , and the required transformations may alter the original document semantics . to cope with this issue we propose a new document clustering approach for multilingual corpora that ( i ) exploits a large-scale multilingual knowledge base , ( ii ) takes advantage of the multi-topic nature of the text documents , and ( iii ) employs a tensor-based model to deal with high dimensionality and sparseness . results have shown the significance of our approach and its better performance w.r.t . classic document clustering approaches , in both a balanced and an unbalanced corpus evaluation .

ntnu : measuring semantic similarity with sublexical feature representations and soft cardinality
the paper describes the approaches taken by the ntnu team to the semeval 2014 semantic textual similarity shared task . the solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from takelabs baseline system . the final ntnu system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance , being the best system on three of the datasets and third best overall ( on weighted mean over all six datasets ) .

hybrid deep belief networks for semi-supervised sentiment classification
in this paper , we develop a novel semi-supervised learning algorithm called hybrid deep belief networks ( hdbn ) , to address the semi-supervised sentiment classification problem with deep learning . first , we construct the previous several hidden layers using restricted boltzmann machines ( rbm ) , which can reduce the dimension and abstract the information of the reviews quickly . second , we construct the following hidden layers using convolutional restricted boltzmann machines ( crbm ) , which can abstract the information of reviews effectively . third , the constructed deep architecture is fine-tuned by gradient-descent based supervised learning with an exponential loss function . we did several experiments on five sentiment classification datasets , and show that hdbn is competitive with previous semi-supervised learning algorithm . experiments are also conducted to verify the effectiveness of our proposed method with different number of unlabeled reviews .

naturalli : natural logic inference for common sense reasoning
common-sense reasoning is important for ai applications , both in nlp and many vision and robotics tasks . we propose naturalli : a natural logic inference system for inferring common sense facts for instance , that cats have tails or tomatoes are round from a very large database of known facts . in addition to being able to provide strictly valid derivations , the system is also able to produce derivations which are only likely valid , accompanied by an associated confidence . we both show that our system is able to capture strict natural logic inferences on the fracas test suite , and demonstrate its ability to predict common sense facts with 49 % recall and 91 % precision .

the topology of synonymy and homonymy networks
semantic networks have been used successfully to explain access to the mental lexicon . topological analyses of these networks have focused on acquisition and generation . we extend this work to look at models that distinguish semantic relations . we find the scale-free properties of association networks are not found in synonymy-homonymy networks , and that this is consistent with studies of childhood acquisition of these relationships . we further find that distributional models of language acquisition display similar topological properties to these networks .

language engineering and the pathway to healthcare : a user-oriented view
this position paper looks critically at a number of aspects of current research into spoken language translation ( slt ) in the medical domain . we first discuss the user profile for medical slt , criticizing designs which assume that the doctor will necessarily need or want to control the technology . if patients are to be users on an equal standing , more attention must be paid to usability issues . we focus briefly on the issue of feedback in slt systems , pointing out the difficulties of relying on text-based paraphrases . we consider the delicate issue of evaluating medical slt systems , noting that some of the standard and much-used evaluation techniques for all aspects of the slt chain might not be suitable for use with real users , even if they are role-playing . finally , we discuss the idea that the pathway to healthcare involves much more than a face-to-face interview with a medical professional , and that different technologies including but not restricted to slt will be appropriate along this pathway .

a web-based demonstrator of a multi-lingual phrase-based itc-irst - centro per la ricerca scientifica e tecnologica
this paper describes a multi-lingual phrase-based statistical machine translation system accessible by means of a web page . the user can issue translation requests from arabic , chinese or spanish into english . the same phrase-based statistical technology is employed to realize the three supported language-pairs . new language-pairs can be easily added to the demonstrator . the web-based interface allows the use of the translation system to any computer connected to the internet .

exact decoding of syntactic translation models through lagrangian relaxation
we describe an exact decoding algorithm for syntax-based statistical translation . the approach uses lagrangian relaxation to decompose the decoding problem into tractable subproblems , thereby avoiding exhaustive dynamic programming . the method recovers exact solutions , with certificates of optimality , on over 97 % of test examples ; it has comparable speed to state-of-the-art decoders .

cogex : a logic prover for question answering language computer corporation
recent trec results have demonstrated the need for deeper text understanding methods . this paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a question answering system . the approach is to transform questions and answer passages into logic representations . world knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text . moreover , the trace of the proofs provide answer justifications . the results show that the prover boosts the performance of the qa system on trec questions by 30 % .

early deletion of fillers in processing conversational speech
this paper evaluates the benefit of deleting fillers ( e.g . you know , like ) early in parsing conversational speech . readability studies have shown that disfluencies ( fillers and speech repairs ) may be deleted from transcripts without compromising meaning ( jones et al , 2003 ) , and deleting repairs prior to parsing has been shown to improve its accuracy ( charniak and johnson , 2001 ) . we explore whether this strategy of early deletion is also beneficial with regard to fillers . reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser ( charniak , 2000 ) . while early deletion is found to yield only modest benefit for in-domain parsing , significant improvement is achieved for out-of-domain adaptation . this suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech .

optimal k-arization of synchronous tree-adjoining grammar and applied sciences and applied sciences
synchronous tree-adjoining grammar ( stag ) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics . current research in both of these areas is actively pursuing its incorporation . however , stag parsing is known to be np-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures . given a particular grammar , the polynomial degree of efficient stag parsing algorithms depends directly on the rank of the grammar : the maximum number of correspondences that appear within a single elementary structure . in this paper we present a compile-time algorithm for transforming a stag into a strongly-equivalent stag that optimally minimizes the rank , k , across the grammar . the algorithm performs ino ( |g|+ |y | l3 g ) time where l g is the maximum number of links in any single synchronous tree pair in the grammar and y is the set of synchronous tree pairs of g .

mapping source to target strings without alignment by analogical learning : a case study with transliteration
analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language . in this study , we revisit this learning paradigm and apply it to the transliteration task . we show that alone , it performs worse than a statistical phrase-based machine translation engine , but the combination of both approaches outperforms each one taken separately , demonstrating the usefulness of the information captured by a so-called formal analogy .

automating the creation of interactive glyph-supplemented scatterplots for visualizing algorithm results
ndaona is a matlab toolkit to create interactive three-dimensional models of data often found in nlp research , such as exploring the results of classification and dimensionality reduction algorithms . such models are useful for teaching , presentations and exploratory research ( such as showing where a classification algorithm makes mistakes ) . ndaona includes embedding and graphics parameter estimation algorithms , and generates files in the format of partiview ( levy , 2001 ) , an existing free open-source fast multidimensional data displayer that has traditionally been used in the planetarium community . partiview1 supports a number of enhancements to regular scatterplots that allow it to display more than three dimensions worth of information .

hultech : a general purpose system for cross-level semantic similarity based on anchor web counts
this paper describes the hultech team participation in task 3 of semeval-2014 . four different subtasks are provided to the participants , who are asked to determine the semantic similarity of cross-level test pairs : paragraphto-sentence , sentence-to-phrase , phrase-toword and word-to-sense . our system adopts a unified strategy ( general purpose system ) to calculate similarity across all subtasks based on word web frequencies . for that purpose , we define clueweb infosimba , a cross-level similarity corpus-based metric . results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems .

simple semi-supervised dependency parsing
we present a simple and effective semisupervised method for training dependency parsers . we focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated corpus . we demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the penn treebank and prague dependency treebank , and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions . for example , in the case of english unlabeled second-order parsing , we improve from a baseline accuracy of 92.02 % to 93.16 % , and in the case of czech unlabeled second-order parsing , we improve from a baseline accuracy of 86.13 % to 87.13 % . in addition , we demonstrate that our method also improves performance when small amounts of training data are available , and can roughly halve the amount of supervised data required to reach a desired level of performance .

structuring using a reliably annotated corpus
we use a reliably annotated corpus to compare metrics of coherence based on centering theory with respect to their potential usefulness for text structuring in natural language generation . previous corpus-based evaluations of the coherence of text according to centering did not compare the coherence of the chosen text structure with that of the possible alternatives . a corpusbased methodology is presented which distinguishes between centering-based metrics taking these alternatives into account , and represents therefore a more appropriate way to evaluate centering from a text structuring perspective .

japanese query alteration based on semantic similarity
we propose a unified approach to web search query alterations in japanese that is not limited to particular character types or orthographic similarity between a query and its alteration candidate . our model is based on previous work on english query correction , but makes some crucial improvements : ( 1 ) we augment the query-candidate list to include orthographically dissimilar but semantically similar pairs ; and ( 2 ) we use kernel-based lexical semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity . we also propose an efficient method for generating query-candidate pairs for model training and testing . we show that the proposed method achieves about 80 % accuracy on the query alteration task , improving over previously proposed methods that use semantic similarity .

phrasefix : statistical post-editing of tectomt
we present two english-to-czech systems that took part in the wmt 2013 shared task : tectomt and phrasefix . the former is a deep-syntactic transfer-based system , the latter is a more-or-less standard statistical post-editing ( spe ) applied on top of tectomt . in a brief survey , we put spe in context with other system combination techniques and evaluate spe vs. another simple system combination technique : using synthetic parallel data from tectomt to train a statistical mt system ( smt ) . we confirm that phrasefix ( spe ) improves the output of tectomt , and we use this to analyze errors in tectomt . however , we also show that extending data for smt is more effective .

credibility adjusted term frequency : a supervised term weighting scheme for sentiment analysis and text classification
we provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification . we compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks . the method is robust and works well on both snippets and longer documents .

machine translation systems
this paper describes meteor 1.3 , our submission to the 2011 emnlp workshop on statistical machine translation automatic evaluation metric tasks . new metric features include improved text normalization , higher-precision paraphrase matching , and discrimination between content and function words . we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrase-based urdu-english system .

e-assessment using latent semantic analysis in the computer science domain : a pilot study
latent semantic analysis ( lsa ) is a statistical natural language processing ( nlp ) technique for inferring meaning from a text . existing lsa-based applications focus on formative assessment in general domains . the suitability of lsa for summative assessment in the domain of computer science is not well known . the results from the pilot study reported in this paper encourage us to pursue further research in the use of lsa in the narrow , technical domain of computer science . this paper explains the theory behind lsa , describes some existing lsa applications , and presents some results using lsa for automatic marking of short essays for a graduate class in architectures of computing systems .

a hybrid feature set based maximum entropy hindi named entity sujan kumar saha
we describe our effort in developing a named entity recognition ( ner ) system for hindi using maximum entropy ( maxent ) approach . we developed a ner annotated corpora for the purpose . we have tried to identify the most relevant features for hindi ner task to enable us to develop an efficient ner from the limited corpora developed . apart from the orthographic and collocation features , we have experimented on the efficiency of using gazetteer lists as features . we also worked on semi-automatic induction of context patterns and experimented with using these as features of the maxent method . we have evaluated the performance of the system against a blind test set having 4 classes - person , organization , location and date . our system achieved a f-value of 81.52 % .

adapting an ner-system for german to the biomedical domain duisburg germany
in this paper , we report the adaptation of a named entity recognition ( ner ) system to the biomedical domain in order to participate in the shared task bio-entity recognition . the system is originally developed for german ner that shares characteristics with the biomedical task . to facilitate adaptability , the system is knowledge-poor and utilizes unlabeled data . investigating the adaptability of the single components and the enhancements necessary , we get insights into the task of bioentity recognition .

turker-assisted paraphrasing for english-arabic machine translation
this paper describes a semi-automatic paraphrasing task for english-arabic machine translation conducted using amazon mechanical turk . the method for automatically extracting paraphrases is described , as are several human judgment tasks completed by turkers . an ideal task type , revised specifically to address feedback from turkers , is shown to be sophisticated enough to identify and filter problem turkers while remaining simple enough for non-experts to complete . the results of this task are discussed along with the viability of using this data to combat data sparsity in mt .

hybrid processing for grammar and style checking
this paper presents an implemented hybrid approach to grammar and style checking , combining an industrial patternbased grammar and style checker with bidirectional , large-scale hpsg grammars for german and english . under this approach , deep processing is applied selectively based on the error hypotheses of a shallow system . we have conducted a comparative evaluation of the two components , supporting an integration scenario where the shallow system is best used for error detection , whereas the hpsg grammars add error correction for both grammar and controlled language style errors .

wordnet-based semantic relatedness measures in automatic speech recognition for meetings
this paper presents the application of wordnet-based semantic relatedness measures to automatic speech recognition ( asr ) in multi-party meetings . different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of n best lists . no significant improvements in terms of word-error-rate ( wer ) are achieved compared to a large word-based ngram baseline model . we discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks .

exact phrases in information retrieval for question answering
question answering ( qa ) is the task of finding a concise answer to a natural language question . the first stage of qa involves information retrieval . therefore , performance of an information retrieval subsystem serves as an upper bound for the performance of a qa system . in this work we use phrases automatically identified from questions as exact match constituents to search queries . our results show an improvement over baseline on several document and sentence retrieval measures on the web dataset . we get a 20 % relative improvement in mrr for sentence extraction on the web dataset when using automatically generated phrases and a further 9.5 % relative improvement when using manually annotated phrases . surprisingly , a separate experiment on the indexed aquaint dataset showed no effect on ir performance of using exact phrases .

a pac-bayesian approach to minimum perplexity language modeling
despite the overwhelming use of statistical language models in speech recognition , machine translation , and several other domains , few high probability guarantees exist on their generalization error . in this paper , we bound the test set perplexity of two popular language models the n-gram model and class-based n-grams using pac-bayesian theorems for unsupervised learning . we extend the bound to sequence clustering , wherein classes represent longer context such as phrases . the new bound is dominated by the maximum number of sequences represented by each cluster , which is polynomial in the vocabulary size . we show that we can still encourage small sample generalization by sparsifying the cluster assignment probabilities . we incorporate our bound into an efficient hmm-based sequence clustering algorithm and validate the theory with empirical results on the resource management corpus .

ranking help message candidates based on robust grammar verification results and utterance history in spoken dialogue systems
we address an issue of out-of-grammar ( oog ) utterances in spoken dialogue systems by generating help messages for novice users . help generation for oog utterances is a challenging problem because language understanding ( lu ) results based on automatic speech recognition ( asr ) results for such utterances are always erroneous as important words are often misrecognized or missed from such utterances . we first develop grammar verification for oog utterances on the basis of a weighted finite-state transducer ( wfst ) . it robustly identifies a grammar rule that a user intends to utter , even when some important words are missed from the asr result . we then adopt a ranking algorithm , rankboost , whose features include the grammar verification results and the utterance history representing the users experience .

text simplification for reading assistance : a project note kentaro inui atsushi fujita tetsuro takahashi ryu iida
this paper describes our ongoing research project on text simplification for congenitally deaf people . text simplification we are aiming at is the task of offering a deaf reader a syntactic and lexical paraphrase of a given text for assisting her/him to understand what it means . in this paper , we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task : readability assessment , paraphrase representation and post-transfer error detection .

challenges for annotating images for sense disambiguation cecilia ovesdotter alm
we describe an unusual data set of thousands of annotated images with interesting sense phenomena . natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text . these issues are discussed and illustrated , including the distinction between word senses and iconographic senses .

multi-faceted event recognition with bootstrapped dictionaries
identifying documents that describe a specific type of event is challenging due to the high complexity and variety of event descriptions . we propose a multi-faceted event recognition approach , which identifies documents about an event using event phrases as well as defining characteristics of the event . our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest . we present a bootstrapping algorithm that automatically acquires event phrases , agent terms , and purpose ( reason ) phrases from unannotated texts . we use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy .

paraquery : making sense of paraphrase collections educational testing service
pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition . although such pivoted paraphrase collections have been successfully used to improve the performance of several different nlp applications , it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections . we present paraquery , a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection , analyze its utility for a particular domain , and compare it to other popular lexical similarity resources all within a single interface .

open information extraction for spanish language based on syntactic constraints instituto politecnico nacional , instituto politecnico nacional ,
open information extraction ( open ie ) serves for the analysis of vast amounts of texts by extraction of assertions , or relations , in the form of tuples argument 1 ; relation ; argument 2. various approaches to open ie have been designed to perform in a fast , unsupervised manner . all of them require language specific information for their implementation . in this work , we introduce an approach to open ie based on syntactic constraints over pos tag sequences targeted at spanish language . we describe the rules specific for spanish language constructions and their implementation in extrhech , an open ie system for spanish . we also discuss language-specific issues of implementation . we compare extrhechs performance with that of reverb , a similar open ie system for english , on a parallel dataset and show that these systems perform at a very similar level . we also compare extrhechs performance on a dataset of grammatically correct sentences against its performance on a dataset of random texts extracted from the web , drastically different in their quality from the first dataset . the latter experiment shows robustness of extrhech on texts from the web .

entity linking for tweets
we study the task of entity linking for tweets , which tries to associate each mention in a tweet with a knowledge base entry . two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations . to address these challenges , we propose a collective inference method that simultaneously resolves a set of mentions . particularly , our model integrates three kinds of similarities , i.e. , mention-entry similarity , entry-entry similarity , and mention-mention similarity , to enrich the context for entity linking , and to address irregular mentions that are not covered by the entity-variation dictionary . we evaluate our method on a publicly available data set and demonstrate the effectiveness of our method .

coreference resolution using semantic relatedness information from automatically discovered patterns
semantic relatedness is a very important factor for the coreference resolution task . to obtain this semantic information , corpusbased approaches commonly leverage patterns that can express a specific semantic relation . the patterns , however , are designed manually and thus are not necessarily the most effective ones in terms of accuracy and breadth . to deal with this problem , in this paper we propose an approach that can automatically find the effective patterns for coreference resolution . we explore how to automatically discover and evaluate patterns , and how to exploit the patterns to obtain the semantic relatedness information . the evaluation on ace data set shows that the pattern based semantic information is helpful for coreference resolution .

robust and efficient chinese word dependency analysis with linear kernel support vector machines
data-driven learning based on shift reduce parsing algorithms has emerged dependency parsing and shown excellent performance to many treebanks . in this paper , we investigate the extension of those methods while considerably improved the runtime and training time efficiency via l2svms . we also present several properties and constraints to enhance the parser completeness in runtime . we further integrate root-level and bottom-level syntactic information by using sequential taggers . the experimental results show the positive effect of the root-level and bottom-level features that improve our parser from 81.17 % to 81.41 % and 81.16 % to 81.57 % labeled attachment scores with modified yamadas and nivres method , respectively on the chinese treebank . in comparison to well-known parsers , such as maltparser ( 80.74 % ) and mstparser ( 78.08 % ) , our methods produce not only better accuracy , but also drastically reduced testing time in 0.07 and 0.11 , respectively .

automatic chinese abbreviation generation using conditional random
this paper presents a new method for automatically generating abbreviations for chinese organization names . abbreviations are commonly used in spoken chinese , especially for organization names . the generation of chinese abbreviation is much more complex than english abbreviations , most of which are acronyms and truncations . the abbreviation generation process is formulated as a character tagging problem and the conditional random field ( crf ) is used as the tagging model . a carefully selected group of features is used in the crf model . after generating a list of abbreviation candidates using the crf , a length model is incorporated to re-rank the candidates . finally the full-name and abbreviation co-occurrence information from a web search engine is utilized to further improve the performance . we achieved top-10 coverage of 88.3 % by the proposed method .

cascading use of soft and hard matching pattern rules for weakly supervised information extraction
current rule induction techniques based on hard matching ( i.e. , strict slot-by-slot matching ) tend to fare poorly in extracting information from natural language texts , which often exhibit great variations . the reason is that hard matching techniques result in relatively high precision but low recall . to tackle this problem , we take advantage of the newly proposed soft pattern rules which offer high recall through the use of probabilistic matching . we propose a bootstrapping framework in which soft and hard matching pattern rules are combined in a cascading manner to realize a weakly supervised rule induction scheme . the system starts with a small set of hand-tagged instances . at each iteration , we first generate soft pattern rules and utilize them to tag new training instances automatically . we then apply hard pattern rule induction on the overall tagged data to generate more precise rules , which are used to tag the data again . the process can be repeated until satisfactory results are obtained . our experimental results show that our bootstrapping scheme with two cascaded learners approaches the performance of a fully supervised information extraction system while using much fewer handtagged instances .

type-supervised domain adaptation for joint segmentation and
we report an empirical investigation on type-supervised domain adaptation for joint chinese word segmentation and pos-tagging , making use of domainspecific tag dictionaries and only unlabeled target domain data to improve target-domain accuracies , given a set of annotated source domain sentences . previous work on pos-tagging of other languages showed that type-supervision can be a competitive alternative to tokensupervision , while semi-supervised techniques such as label propagation are important to the effectiveness of typesupervision . we report similar findings using a novel approach for joint chinese segmentation and pos-tagging , under a cross-domain setting . with the help of unlabeled sentences and a lexicon of 3,000 words , we obtain 33 % error reduction in target-domain tagging . in addition , combined type- and token-supervision can lead to improved cost-effectiveness .

towards a multidimensional semantics of discourse markers in spoken dialogue
the literature contains a wealth of theoretical and empirical analyses of discourse marker functions in human communication . some of these studies address the phenomenon that discourse markers are often multifunctional in a given context , but do not study this in systematic and formal ways . in this paper we show that the use of multiple dimensions in distinguishing and annotating semantic units supports a more accurate analysis of the meaning of discourse markers . we present an empirically-based analysis of the semantic functions of discourse markers in dialogue . we demonstrate that the multiple functions , which a discourse marker may have , are automatically recognizable from utterance surface-features using machine-learning techniques .

classifying amharic news text using self-organizing maps
the paper addresses using artificial neural networks for classification of amharic news items . amharic is the language for countrywide communication in ethiopia and has its own writing system containing extensive systematic redundancy . it is quite dialectally diversified and probably representative of the languages of a continent that so far has received little attention within the language processing field . the experiments investigated document clustering around user queries using selforganizing maps , an unsupervised learning neural network strategy . the best ann model showed a precision of 60.0 % when trying to cluster unseen data , and a 69.5 % precision when trying to classify it .

half : comparing a pure cdsm approach with a standard machine learning system for rte fabio massimo zanzotto
in this paper , we describe our submission to the shared task # 1. we tried to follow the underlying idea of the task , that is , evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models ( cdsms ) applied to this task . we thus submitted two runs : 1 ) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2 ) a system completely based on a cdsm that mixes structural and syntactic information by using distributed tree kernels . our analysis shows that , under the same conditions , the fully cdsm system is still far from being competitive with more complex methods .

improving name discrimination : a language salad approach
this paper describes a method of discriminating ambiguous names that relies upon features found in corpora of a more abundant language . in particular , we discriminate ambiguous names in bulgarian , romanian , and spanish corpora using information derived from much larger quantities of english data . we also mix together occurrences of the ambiguous name found in english with the occurrences of the name in the language in which we are trying to discriminate . we refer to this as a language salad , and find that it often results in even better performance than when only using english or the language itself as the source of information for discrimination .

automatic discovery of attribute words from
range of objects from japanese web documents . the method is a simple unsupervised method that utilizes the statistics of words , lexico-syntactic patterns , and html tags . to evaluate the attribute words , we also establish criteria and a procedure based on question-answerability about the candidate word .

geoname : a system for back-transliterating pinyin place names kui lam kwok
to be unambiguous about a chinese geographic name represented in english text as pinyin , one needs to recover the name in chinese characters . we present our approach to this back-transliteration problem based on processes such as bilingual geographic name lookup , name suggestion using place name character and pair frequencies , and confirmation via a collection of monolingual names or the www . evaluation shows that about 48 % to 72 % of the correct names can be recovered as the top candidate , and 82 % to 86 % within top ten , depending on the processes employed .

the stages of event extraction
event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty . in this paper , we present a simple , modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks , as well as to evaluate the impact on performance these sub-tasks have on the overall task .

statistical machine translation using coercive two-level syntactic
we define , implement and evaluate a novel model for statistical machine translation , which is based on shallow syntactic analysis ( part-of-speech tagging and phrase chunking ) in both the source and target languages . it is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language . we also examine aspects of lexical transfer , suggesting and exploring a concept of translation coercion across parts of speech , as well as a transfer model based on lemma-to-lemma translation probabilities , which holds promise for improving machine translation of low-density languages . experiments are performed in both arabic-to-english and french-to-english translation demonstrating the efficacy of the proposed techniques . performance is automatically evaluated via the bleu score metric .

chinese word sense disambiguation with pagerank and hownet and telecommunications
word sense disambiguation is a basic problem in natural language processing . this paper proposed an unsupervised word sense disambiguation method based pagerank and hownet . in the method , a free text is firstly represented as a sememe graph with sememes as vertices and relatedness of sememes as weighted edges based on hownet . then uw-pagerank is applied on the sememe graph to score the importance of sememes . score of each definition of one word can be computed from the score of sememes it contains . finally , the highest scored definition is assigned to the word . this approach is tested on senseval-3 and the experimental results prove practical and effective .

large and diverse language models for statistical machine translation
this paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart frenchenglish and arabicenglish machine translation system . we show gains of over 2 bleu points over a strong baseline by using continuous space language models in re-ranking .

constructing taxonomy of numerative classifiers for asian languages
numerative classifiers are ubiquitous in many asian languages . this paper proposes a method to construct a taxonomy of numerative classifiers based on a nounclassifier agreement database . the taxonomy defines superordinate-subordinate relation among numerative classifiers and represents the relations in tree structures . the experiments to construct taxonomies were conducted for evaluation by using data from three different languages : chinese , japanese and thai . we found that our method was promising for chinese and japanese , but inappropriate for thai . it confirms that there really is no hierarchy among thai classifiers .

using two translation models
classifying research papers into patent classification systems enables an exhaustive and effective invalidity search , prior art search , and technical trend analysis . however , it is very costly to classify research papers manually . therefore , we have studied automatic classification of research papers into a patent classification system . to classify research papers into patent classification systems , the differences in terms used in research papers and patents should be taken into account . this is because the terms used in patents are often more abstract or creative than those used in research papers in order to widen the scope of the claims . it is also necessary to do exhaustive searches and analyses that focus on classification of research papers written in various languages . to solve these problems , we propose some classification methods using two machine translation models . when translating english research papers into japanese , the performance of a translation model for patents is inferior to that for research papers due to the differences in terms used in research papers and patents . however , the model for patents is thought to be useful for our task because translation results by patent translation models tend to contain more patent terms than those for research papers . to confirm the effectiveness of our methods , we conducted some experiments using the data of the patent mining task in the ntcir-7 workshop .

monads as a solution for generalized opacity
in this paper we discuss a conservative extension of the simply-typed lambda calculus in order to model a class of expressions that generalize the notion of opaque contexts . our extension is based on previous work in the semantics of programming languages aimed at providing a mathematical characterization of computations that produce some kind of side effect ( moggi , 1989 ) , and is based on the notion of monads , a construction in category theory that , intuitively , maps a collection of simple values and simple functions into a more complex value space , in a canonical way . the main advantages of our approach with respect to traditional analyses of opacity are the fact that we are able to explain in a uniform way a set of different but related phenomena , and that we do so in a principled way that has been shown to also explain other linguistic phenomena ( shan , 2001 ) .

mining syntactically annotated corpora with xquery
this paper presents a uniform approach to data extraction from syntactically annotated corpora encoded in xml . xquery , which incorporates xpath , has been designed as a query language for xml . the combination of xpath and xquery offers flexibility and expressive power , while corpus specific functions can be added to reduce the complexity of individual extraction tasks . we illustrate our approach using examples from dependency treebanks for dutch .

improving statistical word alignment with ensemble
and cross-validation committees . on these two methods , both weighted voting and unweighted voting are compared under the word alignment task . in addition , we analyze the effect of different sizes of training sets on the bagging method . experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting . weighted voting performs consistently better than unweighted voting on different sizes of training sets .

on the complexity of alignment problems in two synchronous grammar
the alignment problem for synchronous grammars in its unrestricted form , i.e . whether for a grammar and a string pair the grammar induces an alignment of the two strings , reduces to the universal recognition problem , but restrictions may be imposed on the alignment sought , e.g . alignments may be 1 : 1 , island-free or sure-possible sorted . the complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntax-based machine translation , inversion transduction grammars ( itgs ) ( wu , 1997 ) and a restricted form of range concatenation grammars ( ( 2,2 ) -brcgs ) ( sgaard , 2008 ) , are investigated . the universal recognition problems , and therefore also the unrestricted alignment problems , of both formalisms can be solved in time o ( n6|g| ) . the complexities of the restricted alignment problems differ significantly , however .

top-down nearly-context-sensitive parsing
we present a new syntactic parser that works left-to-right and top down , thus maintaining a fully-connected parse tree for a few alternative parse hypotheses . all of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence . thus they only find a complete fully connected parse at the very end . in contrast , both subjective and experimental evidence show that people understand a sentence word-to-word as they go along , or close to it . the constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact . our parser achieves a new best result for topdown parsers of 89.4 % , a 20 % error reduction over the previous single-parser best result for parsers of this type of 86.8 % ( roark , 2001 ) . the improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming .

unsupervised personal name disambiguation
this paper presents a set of algorithms for distinguishing personal names with multiple real referents in text , based on little or no supervision . the approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts , which are automatically extracted via a language-independent bootstrapping process . the induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data . performance is evaluated based on both a test set of handlabeled multi-referent personal names and via automatically generated pseudonames .

cross-lingual information retrieval system for indian
this paper describes our first participation in the indian language sub-task of the main adhoc monolingual and bilingual track in clef competition . in this track , the task is to retrieve relevant documents from an english corpus in response to a query expressed in different indian languages including hindi , tamil , telugu , bengali and marathi . groups participating in this track are required to submit a english to english monolingual run and a hindi to english bilingual run with optional runs in rest of the languages . we had submitted a monolingual english run and a hindi to english crosslingual run . we used a word alignment table that was learnt by a statistical machine translation ( smt ) system trained on aligned parallel sentences , to map a query in source language into an equivalent query in the language of the target document collection . the relevant documents are then retrieved using a language modeling based retrieval algorithm . on clef 2007 data set , our official cross-lingual performance was 54.4 % of the monolingual performance and in the post submission experiments we found that it can be significantly improved up to 73.4 % .

extending a broad-coverage parser for a general nlp toolkit
with the rapid growth of real world applications for nlp systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific nlp systems . such a toolkit should have a parser that is general enough to be used across domains , and yet accurate enough for each specific application . in this paper , we describe a parser that extends a broad-coverage parser , minipar ( lin , 2001 ) , with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific nl problems . we test this parser on our corpus and the results show that the accuracy is significantly higher than a system that uses minipar alone .

an empirical study of semi-supervised structured conditional models for dependency parsing
this paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach . we describe an extension of semisupervised structured conditional models ( ss-scms ) to the dependency parsing problem , whose framework is originally proposed in ( suzuki and isozaki , 2008 ) . moreover , we introduce two extensions related to dependency parsing : the first extension is to combine ss-scms with another semi-supervised approach , described in ( koo et al , 2008 ) . the second extension is to apply the approach to secondorder parsing models , such as those described in ( carreras , 2007 ) , using a twostage semi-supervised learning approach . we demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections : the penn treebank for english , and the prague dependency treebank for czech . our best results on test data in the above datasets achieve 93.79 % parent-prediction accuracy for english , and 88.05 % for czech .

down-stream effects of tree-to-dependency conversions
dependency analysis relies on morphosyntactic evidence , as well as semantic evidence . in some cases , however , morphosyntactic evidence seems to be in conflict with semantic evidence . for this reason dependency grammar theories , annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions . most experiments for which constituent-based treebanks such as the penn treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes . this paper evaluates the down-stream effect of choice of conversion scheme , showing that it has dramatic impact on end results .

extracting information on pneumonia in infants using natural language processing of radiology reports
natural language processing ( nlp ) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports . many clinical applications require coded data to function appropriately , such as decision support and quality assurance applications . however , in order to be applicable in the clinical domain , performance of the nlp systems must be adequate . a valuable clinical application is the detection of infectious diseases , such as surveillance of healthcare-associated pneumonia in newborns ( e.g . neonates ) because it produces significant rates of morbidity and mortality , and manual surveillance of respiratory infection in these patients is a challenge . studies have already demonstrated that automated surveillance using nlp tools is a useful adjunct to manual clinical management , and is an effective tool for infection control practitioners . this paper presents a study aimed at evaluating the feasibility of an nlp-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neonates . we estimated sensitivity , specificity , and positive predictive value by comparing the detection with clinicians judgments and our results demonstrated that the automated method was indeed feasible . sensitivity ( recall ) was 87.5 % , and specificity ( true negative rates ) was 94.1 % .

the grec challenges 2010 : overview and evaluation results
there were three grec tasks at generation challenges 2010 : grec-ner required participating systems to identify all people references in texts ; for grecneg , systems selected coreference chains for all people entities in texts ; and grecfull combined the ner and neg tasks , i.e . systems identified and , if appropriate , replaced references to people in texts . five teams submitted 10 systems in total , and we additionally created baseline systems for each task . systems were evaluated automatically using a range of intrinsic metrics . in addition , systems were assessed by human judges using preference strength judgements . this report presents the evaluation results , along with descriptions of the three grec tasks , the evaluation methods , and the participating systems .

a formal model for information selection in multi-sentence text
selecting important information while accounting for repetitions is a hard task for both summarization and question answering . we propose a formal model that represents a collection of documents in a two-dimensional space of textual and conceptual units with an associated mapping between these two dimensions . this representation is then used to describe the task of selecting textual units for a summary or answer as a formal optimization task . we provide approximation algorithms and empirically validate the performance of the proposed model when used with two very different sets of features , words and atomic events .

adapting existing grammars : the xle experience
we report on the xle parser and grammar development platform ( maxwell and kaplan , 1993 ) and describe how a basic lexical functional grammar for english has been adapted to two different corpora ( newspaper text and copier repair tips ) .

learning verb classes in an incremental model
the ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs . such generalizations help children extend their learned knowledge of constructions to a novel verb , and use it appropriately in syntactic patterns previously unobserved for that verba key factor in language productivity . computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition . we present an incremental bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input . we show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes .

guaranteeing parsing termination of unication grammars
unification grammars are known to be turingequivalent ; given a grammar and a word , it is undecidable whether . in order to ensure decidability , several constraints on grammars , commonly known as off-line parsability ( olp ) were suggested . the recognition problem is decidable for grammars which satisfy olp . an open question is whether it is decidable if a given grammar satisfies olp . in this paper we investigate various definitions of olp , discuss their inter-relations and show that some of them are undecidable .

dependency parsing for weibo : an efficient probabilistic logic programming approach
dependency parsing is a core task in nlp , and it is widely used by many applications such as information extraction , question answering , and machine translation . in the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data . we present a new gfl/fudg-annotated chinese treebank with more than 18k tokens from sina weibo ( the chinese equivalent of twitter ) . we formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic firstorder logic to infer the dependency arc of a token in the sentence . in experiments , we show that the proposed model outperforms an off-the-shelf stanford chinese parser , as well as a strong maltparser baseline that is trained on the same in-domain data .

word level language identification in online multilingual communication
multilingual speakers switch between languages in online and spoken communication . analyses of large scale multilingual data require automatic language identification at the word level . for our experiments with multilingual online discussions , we first tag the language of individual words using language models and dictionaries . secondly , we incorporate context to improve the performance . we achieve an accuracy of 98 % . besides word level accuracy , we use two new metrics to evaluate this task .

bitext-based resolution of german subject-object ambiguities
we present a method for disambiguating syntactic subjects from syntactic objects ( a frequent ambiguity ) in german sentences taken from an english-german bitext . we exploit the fact that subject and object are usually easily determined in english . we show that a simple method disambiguates some subjectobject ambiguities in german , while making few errors . we view this procedure as the first step in automatically acquiring ( mostly ) correct labeled data . we also evaluate using it to improve a state of the art statistical parser .

phrase table training for precision and recall : what makes a good phrase and a good phrase pair yonggang deng , jia xu+ and yuqing gao
in this work , the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall . we present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance . multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs . experimental results demonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only .

adding noun phrase structure to the penn treebank
the penn treebank does not annotate within base noun phrases ( nps ) , committing only to flat structures that ignore the complexity of english nps . this means that tools trained on treebank data can not learn the correct internal structure of nps . this paper details the process of adding gold-standard bracketing within each noun phrase in the penn treebank . we then examine the consistency and reliability of our annotations . finally , we use this resource to determine np structure using several statistical approaches , thus demonstrating the utility of the corpus . this adds detail to the penn treebank that is necessary for many nlp applications .

evaluating a semantic network automatically constructed from lexical co-occurrence on a word sense disambiguation task
we describe the extension and objective evaluation of a network1 of semantically related noun senses ( or concepts ) that has been automatically acquired by analyzing lexical cooccurrence in wikipedia . the acquisition process makes no use of the metadata or links that have been manually built into the encyclopedia , and nouns in the network are automatically disambiguated to their corresponding noun senses without supervision . for this task , we use the noun sense inventory of wordnet 3.0. thus , this work can be conceived of as augmenting the wordnet noun ontologywith unweighted , undirected relatedto edges between synsets . our network contains 208,832 such edges . we evaluate our networks performance on a word sense disambiguation ( wsd ) task and show : a ) the network is competitive with wordnet when used as a stand-alone knowledge source for two wsd algorithms ; b ) combining our network with wordnet achieves disambiguation results that exceed the performance of either resource individually ; and c ) our network outperforms a similar resource that has been automatically derived from semantic annotations in the wikipedia corpus .

contextual phenomena and thematic relations in database qa dialogues : results from a wizard-of-oz experiment
considering data obtained from a corpus of database qa dialogues , we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus . we look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays a role in discourse structure .

discriminative syntactic language modeling for speech recognition
we describe a method for discriminative training of a language model that makes use of syntactic features . we follow a reranking approach , where a baseline recogniser is used to produce 1000-best output for each acoustic input , and a second reranking model is then used to choose an utterance from these 1000-best lists . the reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm . we describe experiments on the switchboard speech recognition task . the syntactic features provide an additional 0.3 % reduction in testset error rate beyond the model of ( roark et al. , 2004a ; roark et al , 2004b ) ( significant at p < 0.001 ) , which makes use of a discriminatively trained n-gram model , giving a total reduction of 1.2 % over the baseline switchboard system .

projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora
this paper presents the first step to project pos tags and dependencies from english and french to polish in aligned corpora . both the english and french parts of the corpus are analysed with a pos tagger and a robust parser . the english/polish bi-text and the french/polish bi-text are then aligned at the word level with the giza++ package . the intersection of ibm-4 viterbi alignments for both translation directions is used to project the annotations from english and french to polish . the results show that the precision of direct projection vary according to the type of induced annotations as well as the source language . moreover , the performances are likely to be improved by defining regular conversion rules among pos tags and dependencies .

implications of pragmatic and cognitive theories on the design of utterance-based aac systems and information sciences
utterance-based aac systems have the potential to significantly speed communication rate for someone who relies on a speech generating device for communication . at the same time , such systems pose interesting challenges including anticipating text needs , remembering what text is stored , and accessing desired text when needed . moreover , using such systems has profound pragmatic implications as a prestored message may or may not capture exactly what the user wishes to say in a particular discourse situation . in this paper we describe a prototype of an utterance-based aac system whose design choices are driven by findings from theoretically driven studies concerning pragmatic choices with which the user of such a system is faced . these findings are coupled with cognitive theories to make choices for system design .

identifying non-explicit citing sentences for citation-based
identifying background ( context ) information in scientific articles can help scholars understand major contributions in their research area more easily . in this paper , we propose a general framework based on probabilistic inference to extract such context information from scientific papers . we model the sentences in an article and their lexical similarities as a markov random field tuned to detect the patterns that context data create , and employ a belief propagation mechanism to detect likely context sentences . we also address the problem of generating surveys of scientific papers . our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone .

regularisation techniques for conditional random fields : parameterised versus
demonstrated the need for regularisation when applying these models to real-world nlp data sets . conventional approaches to regularising crfs has focused on using a gaussian prior over the model parameters . in this paper we explore other possibilities for crf regularisation . we examine alternative choices of prior distribution and we relax the usual simplifying assumptions made with the use of a prior , such as constant hyperparameter values across features . in addition , we contrast the effectiveness of priors with an alternative , parameter-free approach . specifically , we employ logarithmic opinion pools ( lops ) . our results show that a lop of crfs can outperform a standard unregularised crf and attain a performance level close to that of a regularised crf , without the need for intensive hyperparameter search .

otto : a transcription and management tool for historical texts
this paper presents otto , a transcription tool designed for diplomatic transcription of historical language data . the tool supports easy and fast typing and instant rendering of transcription in order to gain a look as close to the original manuscript as possible . in addition , the tool provides support for the management of transcription projects which involve distributed , collaborative working of multiple parties on collections of documents .

omnifluenttm english-to-french and russian-to-english systems for the science applications international corporation ( saic )
this paper describes omnifluenttm translate a state-of-the-art hybrid mt system capable of high-quality , high-speed translations of text and speech . the system participated in the english-to-french and russian-to-english wmt evaluation tasks with competitive results . the features which contributed the most to high translation quality were training data sub-sampling methods , document-specific models , as well as rule-based morphological normalization for russian . the latter improved the baseline russian-to-english bleu score from 30.1 to 31.3 % on a heldout test set .

scfg decoding without binarization
conventional wisdom dictates that synchronous context-free grammars ( scfgs ) must be converted to chomsky normal form ( cnf ) to ensure cubic time decoding . for arbitrary scfgs , this is typically accomplished via the synchronous binarization technique of ( zhang et al , 2006 ) . a drawback to this approach is that it inflates the constant factors associated with decoding , and thus the practical running time . ( denero et al , 2009 ) tackle this problem by defining a superset of cnf called lexical normal form ( lnf ) , which also supports cubic time decoding under certain implicit assumptions . in this paper , we make these assumptions explicit , and in doing so , show that lnf can be further expanded to a broader class of grammars ( called scope3 ) that also supports cubic-time decoding . by simply pruning non-scope-3 rules from a ghkm-extracted grammar , we obtain better translation performance than synchronous binarization .

invariants and variability of synonymy networks : self mediated agreement by confluence
edges of graphs that model real data can be seen as judgements whether pairs of objects are in relation with each other or not . so , one can evaluate the similarity of two graphs with a measure of agreement between judges classifying pairs of vertices into two categories ( connected or not connected ) . when applied to synonymy networks , such measures demonstrate a surprisingly low agreement between various resources of the same language . this seems to suggest that the judgements on synonymy of lexemes of the same lexicon radically differ from one dictionary editor to another . in fact , even a strong disagreement between edges does not necessarily mean that graphs model a completely different reality : although their edges seem to disagree , synonymy resources may , at a coarser grain level , outline similar semantics . to investigate this hypothesis , we relied on shared common properties of real world data networks to look at the graphs at a more global level by using random walks . they enabled us to reveal a much better agreement between dense zones than between edges of synonymy graphs . these results suggest that although synonymy resources may disagree at the level of judgements on single pairs of words , they may nevertheless convey an essentially similar semantic information .

grasp : grammar learning peter juel henrichsen
this paper presents the ongoing project computational models of first language acquisition , together with its current product , the learning algorithm grasp . grasp is designed specifically for inducing grammars from large , unlabelled corpora of spontaneous ( i.e . unscripted ) speech . the learning algorithm does not assume a predefined grammatical taxonomy ; rather the determination of categories and their relations is considered as part of the learning task . while grasp learning can be used for a range of practical tasks , the long-term goal of the project is to contribute to the debate of innate linguistic knowledge under the hypothesis that there is no such .

sort : an interactive source-rewriting tool for improved translation
the quality of automatic translation is affected by many factors . one is the divergence between the specific source and target languages . another lies in the source text itself , as some texts are more complex than others . one way to handle such texts is to modify them prior to translation . yet , an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used . in this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use . modifications are automatically generated and proposed for the users approval . such a system can reduce postediting effort , replacing it by cost-effective pre-editing that can be done by monolinguals .

imbalanced classification using dictionary-based prototypes and hierarchical decision rules for entity sense disambiguation
entity sense disambiguation becomes difficult with few or even zero training instances available , which is known as imbalanced learning problem in machine learning . to overcome the problem , we create a new set of reliable training instances from dictionary , called dictionarybased prototypes . a hierarchical classification system with a tree-like structure is designed to learn from both the prototypes and training instances , and three different types of classifiers are employed . in addition , supervised dimensionality reduction is conducted in a similarity-based space . experimental results show our system outperforms three baseline systems by at least 8.3 % as measured by macro f1 score .

proper name machine translation from japanese to japanese sign language
this paper describes machine translation of proper names from japanese to japanese sign language ( jsl ) . proper name transliteration is a kind of machine translation of proper names between spoken languages and involves character-tocharacter conversion based on pronunciation . however , transliteration methods can not be applied to japanese-jsl machine translation because proper names in jsl are composed of words rather than characters . our method involves not only pronunciation-based translation , but also sense-based translation , because kanji , which are ideograms that compose most japanese proper names , are closely related to jsl words . these translation methods are trained from parallel corpora . the sense-based translation part is trained via phrase alignment in sentence pairs in a japanese and jsl corpus . the pronunciation-based translation part is trained from a japanese proper name corpus and then post-processed with transformation rules . we conducted a series of evaluation experiments and obtained 75.3 % of accuracy rate , increasing from baseline method by 19.7 points . we also developed a japanese-jsl proper name translation system , in which the translated proper names are visualized with cg animations .

predicting chinese abbreviations with minimum semantic unit and
we propose a new chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally . different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework . to solve the character duplication problem in chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates . we use an integer linear programming ( ilp ) formulation with various constraints to globally decode the final abbreviation from the generated candidates . experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .

simultaneous similarity learning and feature-weight learning
a key problem in document classification and clustering is learning the similarity between documents . traditional approaches include estimating similarity between feature vectors of documents where the vectors are computed using tf-idf in the bag-of-words model . however , these approaches do not work well when either similar documents do not use the same vocabulary or the feature vectors are not estimated correctly . in this paper , we represent documents and keywords using multiple layers of connected graphs . we pose the problem of simultaneously learning similarity between documents and keyword weights as an edge-weight regularization problem over the different layers of graphs . unlike most feature weight learning algorithms , we propose an unsupervised algorithm in the proposed framework to simultaneously optimize similarity and the keyword weights . we extrinsically evaluate the performance of the proposed similarity measure on two different tasks , clustering and classification . the proposed similarity measure outperforms the similarity measure proposed by ( muthukrishnan et al , 2010 ) , a state-of-theart classification algorithm ( zhou and burges , 2007 ) and three different baselines on a variety of standard , large data sets .

a fully-lexicalized probabilistic model for japanese zero anaphora resolution and communication technology
this paper presents a probabilistic model for japanese zero anaphora resolution . first , this model recognizes discourse entities and links all mentions to them . zero pronouns are then detected by case structure analysis based on automatically constructed case frames . their appropriate antecedents are selected from the entities with high salience scores , based on the case frames and several preferences on the relation between a zero pronoun and an antecedent . case structure and zero anaphora relation are simultaneously determined based on probabilistic evaluation metrics .

modeling biological processes for reading comprehension abby vander linden and brittany harding
machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora . in this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document . the input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process . to answer the questions , we first predict a rich structure representing the process in the paragraph . then , we map the question to a formal query , which is executed against the predicted structure . we demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .

intensional summaries as cooperative responses in dialogue :
despite its long history , and a great deal of research producing many useful algorithms and observations , research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies , particularly within the spoken dialogue community . we hypothesize that a particular type of cooperative response , intensional summaries , are effective for when users are unfamiliar with the domain . we evaluate this hypothesis with two experiments with cruiser , a ds for in-car or mobile users to access restaurant information . first , we compare cruiser with a baseline system-initiative ds , and show that users prefer cruiser . then , we experiment with four algorithms for constructing intensional summaries in cruiser , and show that two summary types are equally effective : summaries that maximize domain coverage and summaries that maximize utility with respect to a user model .

play your way to an annotated corpus : games with a purpose and anaphoric annotation
the lack of large-scale corpora annotated with semantic information has been a serious bottleneck for computational semantics , slowing down not only the development of more advanced statistical methods , but also our empirical understanding of the phenomena . the creation of the ontonotes corpus will finally bring computational semantics to the point where computational syntax was in 1993 - but in the meantime , we have come to appreciate the limitations of that methodology both theoretically and as a way of gathering judgments . in this talk , i will discuss an ongoing effort to use the games with a purpose methodology to create a large-scale anaphorically annotated corpus in which multiple judgments are maintained about the interpretation of each anaphoric expression .

decoder integration and expected bleu training for recurrent neural network language models
neural network language models are often trained by optimizing likelihood , but we would prefer to optimize for a task specific metric , such as bleu in machine translation . we show how a recurrent neural network language model can be optimized towards an expected bleu loss instead of the usual cross-entropy criterion . furthermore , we tackle the issue of directly integrating a recurrent network into firstpass decoding under an efficient approximation . our best results improve a phrasebased statistical machine translation system trained on wmt 2012 french-english data by up to 2.0 bleu , and the expected bleu objective improves over a crossentropy trained model by up to 0.6 bleu in a single reference setup .

mining linguistically interpreted texts cassiana fagundes da silva , renata vieira , fernando santos osrio pipca - unisinos vora - portugal
this paper proposes and evaluates the use of linguistic information in the pre-processing phase of text mining tasks . we present several experiments comparing our proposal for selection of terms based on linguistic knowledge with usual techniques applied in the field . the results show that part of speech information is useful for the pre-processing phase of text categorization and clustering , as an alternative for stop words and stemming .

a paraphrase-based exploration of cohesiveness criteria and nogami masaru
this paper proposes an empirical approach to the development of a computational model for assessing texts according to cohesiveness . we argue that the nlg technologies for the generation of structural paraphrases can be used to efficiently create what we call a cohesion-variant parallel corpus , which would serve as a good resource for empirical acquisition of cohesiveness criteria . we also present our pilot case study , in which we took a particular type of paraphrasing that separates a relative clause from a sentence . we have so far created a cohesion-variant parallel corpus containing 499 cohesive instances and 841 incohesive instances . based on this corpus , we conducted a preliminary experiment on cohesion evaluation , obtaining encouraging results .

unsupervised induction of stochastic context-free grammars using
an algorithm is presented for learning a phrase-structure grammar from tagged text . it clusters sequences of tags together based on local distributional information , and selects clusters that satisfy a novel mutual information criterion . this criterion is shown to be related to the entropy of a random variable associated with the tree structures , and it is demonstrated that it selects linguistically plausible constituents . this is incorporated in a minimum description length algorithm . the evaluation of unsupervised models is discussed , and results are presented when the algorithm has been trained on 12 million words of the british national corpus .

multilingual semantic parsing : parsing multiple languages into semantic representations & technology of china information systems technology and design
we consider multilingual semantic parsing the task of simultaneously parsing semantically equivalent sentences from multiple different languages into their corresponding formal semantic representations . our model is built on top of the hybrid tree semantic parsing framework , where natural language sentences and their corresponding semantics are assumed to be generated jointly from an underlying generative process . we first introduce a variant of the joint generative process , which essentially gives us a new semantic parsing model within the framework . based on the different models that can be developed within the framework , we then investigate several approaches for performing the multilingual semantic parsing task . we present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families .

efficient retrieval of tree translation examples for syntax-based machine translation
we propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules . we also propose improvements of this algorithm allowing several kinds of flexible matchings .

comparing triggering policies for social behaviors
instructional efficacy of automated conversational agents designed to help small groups of students achieve higher learning outcomes can be improved by the use of social interaction strategies . these strategies help the tutor agent manage the attention of the students while delivering useful instructional content . two technical challenges involving the use of social interaction strategies include determining the appropriate policy for triggering these strategies and regulating the amount of social behavior performed by the tutor . in this paper , a comparison of six different triggering policies is presented . we find that a triggering policy learnt from human behavior in combination with a filter that keeps the amount of social behavior comparable to that performed by human tutors offers the most effective solution to the these challenges .

inferring tutorial dialogue structure with hidden markov modeling
the field of intelligent tutoring systems has seen many successes in recent years . a significant remaining challenge is the automatic creation of corpus-based tutorial dialogue management models . this paper reports on early work toward this goal . we identify tutorial dialogue modes in an unsupervised fashion using hidden markov models ( hmms ) trained on input sequences of manually-labeled dialogue acts and adjacency pairs . the two best-fit hmms are presented and compared with respect to the dialogue structure they suggest ; we also discuss potential uses of the methodology for future work .

an incremental decision list learner
we demonstrate a problem with the standard technique for learning probabilistic decision lists . we describe a simple , incremental algorithm that avoids this problem , and show how to implement it efficiently . we also show a variation that adds thresholding to the standard sorting algorithm for decision lists , leading to similar improvements . experimental results show that the new algorithm produces substantially lower error rates and entropy , while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm .

filling knowledge base gaps for distant supervision of relation extraction
distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus . however , previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases . to tackle this problem , we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features . we adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases , assuming entity pairs in top-ranked passages are more likely to express a relation . our proposed technique significantly improves the quality of distantly supervised relation extraction , boosting recall from 47.7 % to 61.2 % with a consistently high level of precision of around 93 % in the experiments .

disambiguating prepositional phrase attachment sites with sense information captured in contextualized distributional data
this work presents a supervised prepositional phrase ( pp ) attachment disambiguation system that uses contextualized distributional information as the distance metric for a nearest-neighbor classifier . contextualized word vectors constructed from the gigaword corpus provide a method for implicit word sense disambiguation ( wsd ) , whose reliability helps this system outperform baselines and achieve comparable results to those of systems with full wsd modules . this suggests that targeted wsd methods are preferable to ignoring sense information and also to implementing wsd as an independent module in a pipeline .

profiting from mark-up : hyper-text annotations for guided parsing
we show how web mark-up can be used to improve unsupervised dependency parsing . starting from raw bracketings of four common html tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints . conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus . we demonstrate that derived constraints aid grammar induction by training klein and mannings dependency model with valence ( dmv ) on this data set : parsing accuracy on section 23 ( all sentences ) of the wall street journal corpus jumps to 50.4 % , beating previous state-of-theart by more than 5 % . web-scale experiments show that the dmv , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data . our model , trained on a single blog , generalizes to 53.3 % accuracy out-of-domain , against the brown corpus nearly 10 % higher than the previous published best . the fact that web mark-up strongly correlates with syntactic structure may have broad applicability in nlp .

generating coherent event schemas at scale computer science & engineering
chambers and jurafsky ( 2009 ) demonstrated that event schemas can be automatically induced from text corpora . however , our analysis of their schemas identifies several weaknesses , e.g. , some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor . it is due in part to their pair-wise representation that treats subjectverb independently from verb-object . this often leads to subject-verb-object triples that are not meaningful in the real-world . we present a novel approach to inducing open-domain event schemas that overcomes these limitations . our approach uses cooccurrence statistics of semantically typed relational triples , which we call rel-grams ( relational n-grams ) . in a human evaluation , our schemas outperform chamberss schemas by wide margins on several evaluation criteria . both rel-grams and event schemas are freely available to the research community .

exploiting morphological , grammatical , and semantic correlates for improved text difficulty assessment
we present a low-resource , languageindependent system for text difficulty assessment . we replicate and improve upon a baseline by shen et al . ( 2013 ) on the interagency language roundtable ( ilr ) scale . our work demonstrates that the addition of morphological , information theoretic , and language modeling features to a traditional readability baseline greatly benefits our performance . we use the margin-infused relaxed algorithm and support vector machines for experiments on arabic , dari , english , and pashto , and provide a detailed analysis of our results .

is it really that difficult to parse german
this paper presents a comparative study of probabilistic treebank parsing of german , using the negra and tuba-d/z treebanks . experiments with the stanford parser , which uses a factored pcfg and dependency model , show that , contrary to previous claims for other parsers , lexicalization of pcfg models boosts parsing performance for both treebanks . the experiments also show that there is a big difference in parsing performance , when trained on the negra and on the tubad/z treebanks . parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser , when trained on the penn treebank . this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english .

domain adaptation for sentiment classification
automatic sentiment classification has been extensively studied and applied in recent years . however , sentiment is expressed differently in different domains , and annotating corpora for every possible domain of interest is impractical . we investigate domain adaptation for sentiment classifiers , focusing on online reviews for different types of products . first , we extend to sentiment classification the recently-proposed structural correspondence learning ( scl ) algorithm , reducing the relative error due to adaptation between domains by an average of 30 % over the original scl algorithm and 46 % over a supervised baseline . second , we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another . this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains .

mood at work : ramses versus pharaoh rali diro
we present here the translation system we used in this years wmt shared task . the main objective of our participation was to test ramses , an open source phrasebased decoder . for that purpose , we used the baseline system made available by the organizers of the shared task1 to build the necessary models . we then carried out a pair-to-pair comparison of ramses with pharaoh on the six different translation directions that we were asked to perform . we present this comparison in this paper .

getalp : propagation of a lesk measure through an ant colony algorithm
this article presents the getalp system for the participation to semeval-2013 task 12 , based on an adaptation of the lesk measure propagated through an ant colony algorithm , that yielded good results on the corpus of semeval 2007 task 7 ( wordnet 2.1 ) as well as the trial data for task 12 semeval 2013 ( babelnet 1.0 ) . we approach the parameter estimation to our algorithm from two perspectives : edogenous estimation where we maximised the sum the local lesk scores ; exogenous estimation where we maximised the f1 score on trial data . we proposed three runs of out system , exogenous estimation with babelnet 1.1.1 synset id annotations , endogenous estimation with babelnet 1.1.1 synset id annotations and endogenous estimation with wordnet 3.1 sense keys . a bug in our implementation led to incorrect results and here , we present an amended version thereof . our system arrived third on this task and a more fine grained analysis of our results reveals that the algorithms performs best on general domain texts with as little named entities as possible . the presence of many named entities leads the performance of the system to plummet greatly .

answering definition questions using web knowledge bases
is capable of mining textual definitions from large collections of documents . in order to automatically identify definition sentences from a large collection of documents , we utilize the existing definitions in the web knowledge bases instead of hand-crafted rules or annotated corpus . effective methods are adopted to make full use of web knowledge bases , and they promise high quality response to definition questions . we applied our system in the trec 2004 definition question-answering task and achieved an encouraging performance with the fmeasure score of 0.404 , which was ranked second among all the submitted runs .

entailment-based question answering for structured data
this paper describes a question answering system which retrieves answers from structured data regarding cinemas and movies . the system represents the first prototype of a multilingual and multimodal qa system for the domain of tourism . based on specially designed domain ontology and using textual entailment as a means for semantic inference , the system can be used in both monolingual and cross-language settings with slight adjustments for new input languages .

morphemes and pos tags for n-gram based evaluation metrics language technology ( lt ) , berlin , germany
we propose the use of morphemes for automatic evaluation of machine translation output , and systematically investigate a set of f score and bleu score based metrics calculated on words , morphemes and pos tags along with all corresponding combinations . correlations between the new metrics and human judgments are calculated on the data of the third , fourth and fifth shared tasks of the statistical machine translation workshop . machine translation outputs in five different european languages are used : english , spanish , french , german and czech . the results show that the f scores which take into account morphemes and pos tags are the most promising metrics .

a multi-teraflop constituency parser using gpus
constituency parsing with rich grammars remains a computational challenge . graphics processing units ( gpus ) have previously been used to accelerate cky chart evaluation , but gains over cpu parsers were modest . in this paper , we describe a collection of new techniques that enable chart evaluation at close to the gpus practical maximum speed ( a teraflop ) , or around a half-trillion rule evaluations per second . net parser performance on a 4-gpu system is over 1 thousand length30 sentences/second ( 1 trillion rules/sec ) , and 400 general sentences/second for the berkeley parser grammar . the techniques we introduce include grammar compilation , recursive symbol blocking , and cache-sharing .

exploiting heterogeneous treebanks for parsing
we address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems , converting grammar formalisms of the treebanks to the same one , and parsing on these homogeneous treebanks . first we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion , eliminating predefined heuristic rules as required in previous methods . then we provide two strategies to refine conversion results , and adopt a corpus weighting technique for parsing on homogeneous treebanks . results on the penn treebank show that our conversion method achieves 42 % error reduction over the previous best result . evaluation on the penn chinese treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2 % , resulting in 6 % error reduction over the previous best result .

using predicate argument clustering
this paper suggests two ways of improving semantic role labeling ( srl ) . first , we introduce a novel transition-based srl algorithm that gives a quite different approach to srl . our algorithm is inspired by shift-reduce parsing and brings the advantages of the transitionbased approach to srl . second , we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain . for better generalization of the statistical models , we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions . all approaches are evaluated on the conll09 english data . the new algorithm shows comparable results to another state-of-the-art system . the clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks .

towards a model of face-to-face grounding
we investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction . we analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task . the distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded , and the overall pattern reflected a monitoring of lack of negative feedback . based on these results , we present an eca that uses verbal and nonverbal grounding acts to update dialogue state .

a lemma-based approach to a maximum entropy word sense disambiguation system for dutch
in this paper , we present a corpus-based supervised word sense disambiguation ( wsd ) system for dutch which combines statistical classification ( maximum entropy ) with linguistic information . instead of building individual classifiers per ambiguous wordform , we introduce a lemma-based approach . the advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier , therefore augmenting the training material available to the algorithm . testing the lemmabased model on the dutch senseval-2 test data , we achieve a significant increase in accuracy over the wordform model . also , the wsd system based on lemmas is smaller and more robust .

evaluating the inferential utility of lexical-semantic resources
lexical-semantic resources are used extensively for applied semantic inference , yet a clear quantitative picture of their current utility and limitations is largely missing . we propose system- and application-independent evaluation and analysis methodologies for resources performance , and systematically apply them to seven prominent resources . our findings identify the currently limited recall of available resources , and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods . further , our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid , as well as its prior validity likelihood .

lexical reference : a semantic matching subtask
semantic lexical matching is a prominent subtask within text understanding applications . yet , it is rarely evaluated in a direct manner . this paper proposes a definition for lexical reference which captures the common goals of lexical matching . based on this definition we created and analyzed a test dataset that was utilized to directly evaluate , compare and improve lexical matching models . we suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual components .

evaluation technology from speaker identification to affective analysis : a multi-step system for analyzing childrens stories
we propose a multi-step system for the analysis of childrens stories that is intended to be part of a larger text-to-speechbased storytelling system . a hybrid approach is adopted , where pattern-based and statistical methods are used along with utilization of external knowledge sources . this system performs the following story analysis tasks : identification of characters in each story ; attribution of quotes to specific story characters ; identification of character age , gender and other salient personality attributes ; and finally , affective analysis of the quoted material . the different types of analyses were evaluated using several datasets . for the quote attribution , as well as for the gender and age estimation , substantial improvement over baseline was realized , whereas results for personality attribute estimation and valence estimation are more modest .

robust word sense translation by em learning of frame semantics
we propose a robust method of automatically constructing a bilingual word sense dictionary from readily available monolingual ontologies by using estimation-maximization , without any annotated training data or manual tuning . we demonstrate our method on the english framenet and chinese hownet structures . owing to the robustness of em iterations in improving translation likelihoods , our word sense translation accuracies are very high , at 82 % on average , for the 11 most ambiguous words in the english framenet with 5 senses or more . we also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidates and show the first significant evidence that frame semantics are useful for translation disambiguation . translation disambiguation accuracy using frame semantics is 75 % , compared to 15 % by using dictionary glossing only . these results demonstrate the great potential for future application of bilingual frame semantics to machine translation tasks .

finding deceptive opinion spam by any stretch of the imagination
consumers increasingly rate , review and research products online ( jansen , 2010 ; litvin et al , 2008 ) . consequently , websites containing consumer reviews are becoming targets of opinion spam . while recent work has focused primarily on manually identifiable instances of opinion spam , in this work we study deceptive opinion spamfictitious opinions that have been deliberately written to sound authentic . integrating work from psychology and computational linguistics , we develop and compare three approaches to detecting deceptive opinion spam , and ultimately develop a classifier that is nearly 90 % accurate on our gold-standard opinion spam dataset . based on feature analysis of our learned models , we additionally make several theoretical contributions , including revealing a relationship between deceptive opinions and imaginative writing .

patent claim processing for readability - structure analysis and term explanation
patent corpus processing should be centered around patent claim processing because claims are the most important part in patent specifications . it is common that claims written in japanese are described in one sentence with peculiar style and wording and are difficult to understand for ordinary people . the peculiarity is caused by structural complexity of the sentences and many difficult terms used in the description . we have already proposed a framework to represent the structure of patent claims and a method to automatically analyze it . we are currently investigating a method to clarify terms in patent claims and to find the explanatory portions from the detailed description part of the patent specifications . through both approaches , we believe we can improve readability of patent claims .

context and learning in novelty detection
we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conferences novelty track in 2004. in order to determine whether information within a sentence has been seen in material read previously , our system integrates information about the context of the sentence with novel words and named entities within the sentence , and uses a specialized learning algorithm to tune the system parameters .

bayes risk-based dialogue management for document retrieval system with speech interface and communications technology
we propose an efficient dialogue management for an information navigation system based on a document knowledge base with a spoken dialogue interface . in order to perform robustly for fragmental speech input and erroneous output of an automatic speech recognition ( asr ) , the system should selectively use n-best hypotheses of asr and contextual information . the system also has several choices in generating responses or confirmations . in this work , we formulate the optimization of the choices based on a unified criterion : bayes risk , which is defined based on reward for correct information presentation and penalty for redundant turns . we have evaluated this strategy with a spoken dialogue system which also has questionanswering capability . effectiveness of the proposed framework was confirmed in the success rate of retrieval and the average number of turns .

generating spatio-temporal descriptions in pollen forecasts
we describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype natural language generation ( nlg ) system that produces pollen forecasts for scotland .

engineering linguistic creativity : bird flight and jet planes
man achieved flight by studying how birds fly , and yet the solution that engineers came up with ( jet planes ) is very different from the one birds apply . in this paper i review a number of efforts in automated story telling and poetry generation , identifying which human abilities are being modelled in each case . in an analogy to the classic example of bird-flight and jet planes , i explore how the computational models relate to ( the little we know about ) human performance , what the similarities are between the case for linguistic creativity and the case for flight , and what the analogy might have to say about artificial linguistic creativity if it were valid .

engineering of syntactic features for shallow semantic parsing
recent natural language learning research has shown that structural kernels can be effectively used to induce accurate models of linguistic phenomena . in this paper , we show that the above properties hold on a novel task related to predicate argument classification . a tree kernel for selecting the subtrees which encodes argument structures is applied . experiments with support vector machines on large data sets ( i.e . the propbank collection ) show that such kernel improves the recognition of argument boundaries .

identification of basic phrases for kazakh language using maximum entropy model +the base of kazakh and kirghiz language of national language resource monitoring and
this paper proposes the definition , classification and structure of the kazakh basic phrases , and sets up a framework for classifying them according to their syntactic functions . meanwhile , the structure of the kazakh basic phrases were analyzed ; and the determination of the kazakh basic phrases collocation and extraction of the kazakh basic phrases based on rules were followed . the maximum entropy ( me ) model uses for the identification of the phrases from texts and achieved a result of automatic identification of kazakh phrases with an accuracy of 78.22 % based on rules system and additional artificial modification . design feature of this me model join rely on templates of kazakh word , part of speech , affixes . experimental results show that the accuracy rate reached 87.89

towards answering opinion questions : separating facts from opinions and identifying the polarity of opinion sentences
opinion question answering is a challenging task for natural language processing . in this paper , we discuss a necessary component for an opinion question answering system : separating opinions from fact , at both the document and sentence level . we present a bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories , and describe three unsupervised , statistical techniques for the significantly harder task of detecting opinions at the sentence level . we also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion . results from a large collection of news stories and a human evaluation of 400 sentences are reported , indicating that we achieve very high performance in document classification ( upwards of 97 % precision and recall ) , and respectable performance in detecting opinions and classifying them at the sentence level as positive , negative , or neutral ( up to 91 % accuracy ) .

ellipsis resolution by controlled default unification for multi-modal and speech dialog systems artificial intelligence - dfki artificial intelligence - dfki
we present a default-unification-based approach to ellipsis resolution that is based on experience in long running multimodal dialog projects , where it played an essential role in discourse processing . we extend default unification to non-parallel structures , which is important for speech and multimodal dialog systems . we introduce new control mechanisms for ellipsis resolution by considering dialog structure with respect to specification , variation and results of tasks and combine this with the analysis of relations between the information elements contained in antecedent and elliptic structures .

generating english determiners in phrase-based translation with synthetic translation options
we propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsphrasal translations that are generated by auxiliary translation and postediting processesto augment the default phrase inventory learned from parallel data . we apply our technique to the problem of producing english determiners when translating from russian and czech , languages that lack definiteness morphemes . our approach augments the english side of the phrase table using a classifier to predict where english articles might plausibly be added or removed , and then we decode as usual . doing so , we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier .

improved lexical acquisition through dpp-based verb clustering
subcategorization frames ( scfs ) , selectional preferences ( sps ) and verb classes capture related aspects of the predicateargument structure . we present the first unified framework for unsupervised learning of these three types of information . we show how to utilize determinantal point processes ( dpps ) , elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets , for clustering . our novel clustering algorithm constructs a joint scf-dpp dpp kernel matrix and utilizes the efficient sampling algorithms of dpps to cluster together verbs with similar scfs and sps . we evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1 .

a study on position information in document summarization
position information has been proved to be very effective in document summarization , especially in generic summarization . existing approaches mostly consider the information of sentence positions in a document , based on a sentence position hypothesis that the importance of a sentence decreases with its distance from the beginning of the document . in this paper , we consider another kind of position information , i.e. , the word position information , which is based on the ordinal positions of word appearances instead of sentence positions . an extractive summarization model is proposed to provide an evaluation framework for the position information . the resulting systems are evaluated on various data sets to demonstrate the effectiveness of the position information in different summarization tasks . experimental results show that word position information is more effective and adaptive than sentence position information .

dependency tree kernels for relation extraction
we extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences . using this kernel within a support vector machine , we detect and classify relations between entities in the automatic content extraction ( ace ) corpus of news articles . we examine the utility of different features such as wordnet hypernyms , parts of speech , and entity types , and find that the dependency tree kernel achieves a 20 % f1 improvement over a bag-of-words kernel .

learning document-level semantic properties from free-text annotations
this paper demonstrates a new method for leveraging free-text annotations to infer semantic properties of documents . free-text annotations are becoming increasingly abundant , due to the recent dramatic growth in semistructured , user-generated online content . an example of such content is product reviews , which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value . to exploit such noisy annotations , we simultaneously find a hidden paraphrase structure of the keyphrases , a model of the document texts , and the underlying semantic properties that link the two . this allows us to predict properties of unannotated documents . our approach is implemented as a hierarchical bayesian model with joint inference , which increases the robustness of the keyphrase clustering and encourages the document model to correlate with semantically meaningful properties . we perform several evaluations of our model , and find that it substantially outperforms alternative approaches .

speaker-independent context update rules for dialogue management
this paper describes a dialogue management system in which an attempt is made to factor out a declarative theory of context updates in dialogue from a procedural theory of generating and interpreting utterances in dialogue .

a joint information model for n-best ranking marina del rey , ca
in this paper , we present a method for modeling joint information when generating n-best lists . we apply the method to a novel task of characterizing the similarity of a group of terms where only a small set of many possible semantic properties may be displayed to a user . we demonstrate that considering the results jointly , by accounting for the information overlap between results , generates better n-best lists than considering them independently . we propose an information theoretic objective function for modeling the joint information in an n-best list and show empirical evidence that humans prefer the result sets produced by our joint model . our results show with 95 % confidence that the n-best lists generated by our joint ranking model are significantly different from a baseline independent model 50.0 % 3.1 % of the time , out of which they are preferred 76.6 % 5.2 % of the time .

dependency parsing by belief propagation
we formulate dependency parsing as a graphical model with the novel ingredient of global constraints . we show how to apply loopy belief propagation ( bp ) , a simple and effective tool for approximate learning and inference . as a parsing algorithm , bp is both asymptotically and empirically efficient . even with second-order features or latent variables , which would make exact parsing considerably slower or np-hard , bp needs only o ( n3 ) time with a small constant factor . furthermore , such features significantly improve parse accuracy over exact first-order methods . incorporating additional features would increase the runtime additively rather than multiplicatively .

mildly non-projective dependency structures
syntactic parsing requires a fine balance between expressivity and complexity , so that naturally occurring structures can be accurately parsed without compromising efficiency . in dependency-based parsing , several constraints have been proposed that restrict the class of permissible structures , such as projectivity , planarity , multi-planarity , well-nestedness , gap degree , and edge degree . while projectivity is generally taken to be too restrictive for natural language syntax , it is not clear which of the other proposals strikes the best balance between expressivity and complexity . in this paper , we review and compare the different constraints theoretically , and provide an experimental evaluation using data from two treebanks , investigating how large a proportion of the structures found in the treebanks are permitted under different constraints . the results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data .

inferring the semantics of temporal prepositions in italian tommaso caselli valeria quochi
in this work we report on the results of a preliminary corpus study of italian on the semantics of temporal prepositions , which is part of a wider project on the automatic recognition of temporal relations . the corpus data collected supports our hypothesis that each temporal preposition can be associated with one prototypical temporal relation , and that deviations from the prototype can be explained as determined by the occurrence of different semantic patterns . the motivation behind this approach is to improve methods for temporal annotation of texts for content based access to information . the corpus study described in this paper led to the development of a preliminary set of heuristics for automatic annotation of temporal relations in text/discourse .

the c-score proposing a reading comprehension metrics as a common evaluation measure for text simplification
this article addresses the lack of common approaches for text simplification evaluation , by presenting the first attempt for a common evaluation metrics . the article proposes reading comprehension evaluation as a method for evaluating the results of text simplification ( ts ) . an experiment , as an example application of the evaluation method , as well as three formulae to quantify reading comprehension , are presented . the formulae produce an unique score , the c-score , which gives an estimation of users reading comprehension of a certain text . the score can be used to evaluate the performance of a text simplification engine on pairs of complex and simplified texts , or to compare the performances of different ts methods using the same texts . the approach can be particularly useful for the modern crowdsourcing approaches , such as those employing the amazons mechanical turk1 or crowdflower2 . the aim of this paper is thus to propose an evaluation approach and to motivate the ts community to start a relevant discussion , in order to come up with a common evaluation metrics for this task .

speech summarization without lexical features for mandarin broadcast news
we present the first known empirical study on speech summarization without lexical features for mandarin broadcast news . we evaluate acoustic , lexical and structural features as predictors of summary sentences . we find that the summarizer yields good performance at the average fmeasure of 0.5646 even by using the combination of acoustic and structural features alone , which are independent of lexical features . in addition , we show that structural features are superior to lexical features and our summarizer performs surprisingly well at the average f-measure of 0.3914 by using only acoustic features . these findings enable us to summarize speech without placing a stringent demand on speech recognition accuracy .

interaction quality estimation in spoken dialogue systems using
research trends on sds evaluation are recently focusing on objective assessment methods . most existing methods , which derive quality for each systemuser-exchange , do not consider temporal dependencies on the quality of previous exchanges . in this work , we investigate an approach for determining interaction quality for human-machine dialogue based on methods modeling the sequential characteristics using hmm modeling . our approach significantly outperforms conventional approaches by up to 4.5 % relative improvement based on unweighted average recall metrics .

deriving generalized knowledge from corpora using wordnet
benjamin van durme , phillip michalak and lenhart k. schubert department of computer science university of rochester rochester , ny 14627 , usa abstract existing work in the extraction of commonsense knowledge from text has been primarily restricted to factoids that serve as statements about what may possibly obtain in the world . we present an approach to deriving stronger , more general claims by abstracting over large sets of factoids . our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types , obtained as wordnet synsets . the results can be construed as generically quantified sentences restricting the semantic type of an argument position of a predicate .

a cross-lingual annotation projection approach for relation detection
while extensive studies on relation extraction have been conducted in the last decade , statistical systems based on supervised learning are still limited because they require large amounts of training data to achieve high performance . in this paper , we develop a cross-lingual annotation projection method that leverages parallel corpora to bootstrap a relation detector without significant annotation efforts for a resource-poor language . in order to make our method more reliable , we introduce three simple projection noise reduction methods . the merit of our method is demonstrated through a novel korean relation detection task .

question answering as question-biased term extraction : a new approach toward multilingual qa
this paper regards question answering ( qa ) as question-biased term extraction ( qbte ) . this new qbte approach liberates qa systems from the heavy burden imposed by question types ( or answer types ) . in conventional approaches , a qa system analyzes a given question and determines the question type , and then it selects answers from among answer candidates that match the question type . consequently , the output of a qa system is restricted by the design of the question types . the qbte directly extracts answers as terms biased by the question . to confirm the feasibility of our qbte approach , we conducted experiments on the crl qa data based on 10-fold cross validation , using maximum entropy models ( mems ) as an ml technique . experimental results showed that the trained system achieved 0.36 in mrr and 0.47 in top5 accuracy .

new parameterizations and features for pscfg-based machine
we propose several improvements to the hierarchical phrase-based mt model of chiang ( 2005 ) and its syntax-based extension by zollmann and venugopal ( 2006 ) . we add a source-span variance model that , for each rule utilized in a probabilistic synchronous context-free grammar ( pscfg ) derivation , gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules , with the distributions of these source span sizes estimated during training time . we further propose different methods of combining hierarchical and syntax-based pscfg models , by merging the grammars as well as by interpolating the translation models . finally , we compare syntax-augmented mt , which extracts rules based on targetside syntax , to a corresponding variant based on source-side syntax , and experiment with a model extension that jointly takes source and target syntax into account .

an empirical study of information synthesis tasks enrique amigo julio gonzalo vctor peinado anselmo penas felisa verdejo
this paper describes an empirical study of the information synthesis task , defined as the process of ( given a complex information need ) extracting , organizing and inter-relating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . two main results are presented : a ) the creation of an information synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b ) an empirical comparison of similarity metrics between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . a metric based on key concepts overlap gives better results than metrics based on n-gram overlap ( such as rouge ) or sentence overlap .

correcting esl errors using phrasal smt techniques
this paper presents a pilot study of the use of phrasal statistical machine translation ( smt ) techniques to identify and correct writing errors made by learners of english as a second language ( esl ) . using examples of mass noun errors found in the chinese learner error corpus ( clec ) to guide creation of an engineered training set , we show that application of the smt paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers . our system was able to correct 61.81 % of mistakes in a set of naturallyoccurring examples of mass noun errors found on the world wide web , suggesting that efforts to collect alignable corpora of pre- and post-editing esl writing samples offer can enable the development of smt-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of esl learners .

corry : a system for coreference resolution
corry is a system for coreference resolution in english . it supports both local ( soon et al . ( 2001 ) -style ) and global ( integer linear programming , denis and baldridge ( 2007 ) style ) models of coreference . corry relies on a rich linguistically motivated feature set , which has , however , been manually reduced to 64 features for efficiency reasons . three runs have been submitted for the semeval task 1 on coreference resolution ( recasens et al , 2010 ) , optimizing corrys performance for blanc ( recasens and hovy , in prep ) , muc ( vilain et al , 1995 ) and ceaf ( luo , 2005 ) . corry runs have shown the best performance level among all the systems in their track for the corresponding metric .

dependency based logical form transformations
this paper describes a system developed for the transformation of english sentences into a first order logical form representation . the metho dology is centered on the use of a dependency grammar based parser . we demonstrate the suitability of applying a dependency parser based solution to the given task and in turn explain some of the limitations and challenges involved when using such an approach . the efficiencies and deficiencies of our approach are discussed as well as considerations for further enhanc ements .

shift-reduce ccg parsing
ccgs are directly compatible with binarybranching bottom-up parsing algorithms , in particular cky and shift-reduce algorithms . while the chart-based approach has been the dominant approach for ccg , the shift-reduce method has been little explored . in this paper , we develop a shift-reduce ccg parser using a discriminative model and beam search , and compare its strengths and weaknesses with the chart-based c & c parser . we study different errors made by the two parsers , and show that the shift-reduce parser gives competitive accuracies compared to c & c . considering our use of a small beam , and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the ccg lexical categories which form the shift actions , this is a surprising result .

joint inference for fine-grained opinion extraction
this paper addresses the task of finegrained opinion extraction the identification of opinion-related entities : the opinion expressions , the opinion holders , and the targets of the opinions , and the relations between opinion expressions and their targets and holders . most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner , where the interdependencies among different extraction stages are not captured . we propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction , and seeks a globally optimal solution . experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction .

a survey for multi-document summarization
automatic multi-document summarization is still hard to realize . under such circumstances , we believe , it is important to observe how humans are doing the same task , and look around for different strategies . we prepared 100 document sets similar to the ones used in the duc multi-document summarization task . for each document set , several people prepared the following data and we conducted a survey . a ) free style summarization b ) sentence extraction type summarization c ) axis ( type of main topic ) d ) table style summary in particular , we will describe the last two in detail , as these could lead to a new direction for multisummarization research .

evangelising language technology : a practically-focussed undergraduate program
this paper describes an undergraduate program in language technology that we have developed at macquarie university . we question the industrial relevance of much that is taught in nlp courses , and emphasize the need for a practical orientation as a means to growing the size of the field . we argue that a more evangelical approach , both with regard to students and industry , is required . the paper provides an overview of the material we cover , and makes some observations for the future on the basis of our experiences so far .

graph-based event coreference resolution
in this paper , we address the problem of event coreference resolution as specified in the automatic content extraction ( ace ) program . in contrast to entity coreference resolution , event coreference resolution has not received great attention from researchers . in this paper , we first demonstrate the diverse scenarios of event coreference by an example . we then model event coreference resolution as a spectral graph clustering problem and evaluate the clustering algorithm on ground truth event mentions using ecm-f measure . we obtain the ecm-f scores of 0.8363 and 0.8312 respectively by using two methods for computing coreference matrices .

predictive text entry using syntax and semantics
most cellular telephones use numeric keypads , where texting is supported by dictionaries and frequency models . given a key sequence , the entry system recognizes the matching words and proposes a rankordered list of candidates . the ranking quality is instrumental to an effective entry . this paper describes a new method to enhance entry that combines syntax and language models . we first investigate components to improve the ranking step : language models and semantic relatedness . we then introduce a novel syntactic model to capture the word context , optimize ranking , and then reduce the number of keystrokes per character ( kspc ) needed to write a text . we finally combine this model with the other components and we discuss the results . we show that our syntax-based model reaches an error reduction in kspc of 12.4 % on a swedish corpus over a baseline using word frequencies . we also show that bigrams are superior to all the other models . however , bigrams have a memory footprint that is unfit for most devices .

determining the unithood of word sequences using a probabilistic
most research related to unithood were conducted as part of a larger effort for the determination of termhood . consequently , novelties are rare in this small sub-field of term extraction . in addition , existing work were mostly empirically motivated and derived . we propose a new probabilistically-derived measure , independent of any influences of termhood , that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from google search engine for the measurement of unithood . our comparative study using 1 , 825 test cases against an existing empiricallyderived function revealed an improvement in terms of precision , recall and accuracy .

japanese named entity extraction with redundant morphological analysis
named entity ( ne ) extraction is an important subtask of document processing such as information extraction and question answering . a typical method used for ne extraction of japanese texts is a cascade of morphological analysis , pos tagging and chunking . however , there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of nes , so that extraction of some nes are inherently impossible in this setting . to cope with the unit problem , we propose a character-based chunking method . firstly , the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple ( n-best ) answers . then , each character is annotated with its character types and its possible pos tags of the top n-best answers . finally , a support vector machine-based chunker picks up some portions of the input sentence as nes . this method introduces richer information to the chunker than previous methods that base on a single morphological analysis result . we apply our method to irex ne extraction task . the cross validation result of the f-measure being 87.2 shows the superiority and effectiveness of the method .

estimating effect size across datasets
most nlp tools are applied to text that is different from the kind of text they were evaluated on . common evaluation practice prescribes significance testing across data points in available test data , but typically we only have a single test sample . this short paper argues that in order to assess the robustness of nlp tools we need to evaluate them on diverse samples , and we consider the problem of finding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines . we apply meta-analysis and show experimentally by comparing estimated error reduction over observed error reduction on held-out datasets that this method is significantly more predictive of success than the usual practice of using macro- or micro-averages . finally , we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis .

a machine learning based approach to evaluating retrieval systems
test collections are essential to evaluate information retrieval ( ir ) systems . the relevance assessment set has been recognized as the key bottleneck in test collection building , especially on very large sized document collections . this paper addresses the problem of efficiently selecting documents to be included in the assessment set . we will show how machine learning techniques can fit this task . this leads to smaller pools than traditional round robin pooling , thus reduces significantly the manual assessment workload . experimental results on trec collections1 consistently demonstrate the effectiveness of our approach according to different evaluation criteria .

indonesian-japanese clir using only limited resource
our research aim here is to build a clir system that works for a language pair with poor resources where the source language ( e.g . indonesian ) has limited language resources . our indonesianjapanese clir system employs the existing japanese ir system , and we focus our research on the indonesianjapanese query translation . there are two problems in our limited resource query translation : the oov problem and the translation ambiguity . the oov problem is handled using target languages resources ( english-japanese dictionary and japanese proper name dictionary ) . the translation ambiguity is handled using a japanese monolingual corpus in our translation filtering . we select the final translation set using the mutual information score and the tfidf score . the result on ntcir 3 ( nii-nacsis test collection for ir systems ) web retrieval task shows that the translation method achieved a higher ir score than the transitive machine translation ( using kataku ( indonesian-english ) and babelfish/ excite ( english-japanese ) engine ) result . the best result achieved about 49 % of the monolingual retrieval .

discourse generation using utility-trained coherence models
we describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths . an integral part of this framework are algorithms for searching and training these stochastic coherence models . we evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered .

unn-weps : web person search using co-present names and lexical chains newcastle upon tyne newcastle upon tyne
we describe a system , unn-weps for identifying individuals from web pages using data from semeval task 13. our system is based on using co-presence of person names to form seed clusters . these are then extended with pages that are deemed conceptually similar based on a lexical chaining analysis computed using rogets thesaurus . finally , a single link hierarchical agglomerative clustering algorithm merges the enhanced clusters for individual entity recognition . unn-weps achieved an average purity of 0.6 , and inverse purity of 0.73 .

models and training for unsupervised preposition sense disambiguation
we present a preliminary study on unsupervised preposition sense disambiguation ( psd ) , comparing different models and training techniques ( em , map-em with l0 norm , bayesian inference using gibbs sampling ) . to our knowledge , this is the first attempt at unsupervised preposition sense disambiguation . our best accuracy reaches 56 % , a significant improvement ( at p < .001 ) of 16 % over the most-frequent-sense baseline .

learning decision lists with known rules for text mining
many real-world systems for handling unstructured text data are rule-based . examples of such systems are named entity annotators , information extraction systems , and text classifiers . in each of these applications , ordering rules into a decision list is an important issue . in this paper , we assume that a set of rules is given and study the problem ( maxdl ) of ordering them into an optimal decision list with respect to a given training set . we formalize this problem and show that it is np-hard and can not be approximated within any reasonable factors . we then propose some heuristic algorithms and conduct exhaustive experiments to evaluate their performance . in our experiments we also observe performance improvement over an existing decision list learning algorithm , by merely re-ordering the rules output by it .

poly-co : a multilayer perceptron approach for coreference detection
this paper presents the coreference resolution system poly-co submitted to the closed track of the conll-2011 shared task . our system integrates a multilayer perceptron classifier in a pipeline approach . we describe the heuristic used to select the pairs of coreference candidates that are feeded to the network for training , and our feature selection method . the features used in our approach are based on similarity and identity measures , filtering informations , like gender and number , and other syntactic information .

fuzzy syntactic reordering for phrase-based statistical machine translation
the quality of arabic-english statistical machine translation often suffers as a result of standard phrase-based smt systems inability to perform long-range re-orderings , specifically those needed to translate vso-ordered arabic sentences . this problem is further exacerbated by the low performance of arabic parsers on subject and subject span detection . in this paper , we present two parse fuzzification techniques which allow the translation system to select among a range of possible sv re-orderings . with this approach , we demonstrate a 0.3-point improvement in bleu score ( 69 % of the maximum possible using gold parses ) , and a corresponding improvement in the percentage of syntactically well-formed subjects under a manual evaluation .

a new sentence compression dataset and its use in an abstractive
generate-and-rank sentence compressor dimitrios galanis and ion androutsopoulos+ department of informatics , athens university of economics and business , greece +digital curation unit imis , research center athena , greece abstract sentence compression has attracted much interest in recent years , but most sentence compressors are extractive , i.e. , they only delete words . there is a lack of appropriate datasets to train and evaluate abstractive sentence compressors , i.e. , methods that apart from deleting words can also rephrase expressions . we present a new dataset that contains candidate extractive and abstractive compressions of source sentences . the candidate compressions are annotated with human judgements for grammaticality and meaning preservation . we discuss how the dataset was created , and how it can be used in generate-and-rank abstractive sentence compressors . we also report experimental results with a novel abstractive sentence compressor that uses the dataset .

a low-complexity , broad-coverage probabilistic dependency parser for
large-scale parsing is still a complex and timeconsuming process , often so much that it is infeasible in real-world applications . the parsing system described here addresses this problem by combining finite-state approaches , statistical parsing techniques and engineering knowledge , thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance . the parser is robust and fast and at the same time based on strong linguistic foundations .

optimal search for minimum error rate training
minimum error rate training is a crucial component to many state-of-the-art nlp applications , such as machine translation and speech recognition . however , common evaluation functions such as bleu or word error rate are generally highly non-convex and thus prone to search errors . in this paper , we present lp-mert , an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming . given a set of n -best lists produced from s input sentences , this algorithm finds a linear model that is globally optimal with respect to this set . we find that this algorithm is polynomial in n and in the size of the model , but exponential in s. we present extensions of this work that let us scale to reasonably large tuning sets ( e.g. , one thousand sentences ) , by either searching only promising regions of the parameter space , or by using a variant of lp-mert that relies on a beam-search approximation . experimental results show improvements over the standard och algorithm .

contrast and variability in gene names
we studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain . based on our findings , we developed heuristics for mapping weakly matching gene names to their official gene names . we then tested these heuristics against a large body of medline abstracts , and found that using these heuristics can increase recall , with varying levels of precision . our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes , proteins , rna , and a variety of other referents for performing entity identification with high precision .

jointly learning to extract and compress
we learn a joint model of sentence extraction and compression for multi-document summarization . our model scores candidate summaries according to a combined linear model whose features factor over ( 1 ) the n-gram types in the summary and ( 2 ) the compressions used . we train the model using a marginbased objective whose loss captures end summary quality . because of the exponentially large set of candidate summaries , we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently . inference in our model can be cast as an ilp and thereby solved in reasonable time ; we also present a fast approximation scheme which achieves similar performance . our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both rouge and pyramid , without a drop in judged linguistic quality . we achieve the highest published rouge results to date on the tac 2008 data set .

survey in sentiment , polarity and function analysis of citation
in this paper we proposed a survey in sentiment , polarity and function analysis of citations . this is an interesting area that has had an increased development in recent years but still has plenty of room for growth and further research . the amount of scientific information in the web makes it necessary innovate the analysis of the influence of the work of peers and leaders in the scientific community . we present an overview of general concepts , review contributions to the solution of related problems such as context identification , function and polarity classification , identify some trends and suggest possible future research directions .

the flarenet thematic network : a global forum for cooperation
the aim of this short paper is to present the flarenet thematic network for language resources and language technologies to the asian language resources community . creation of a wide and committed community and of a shared policy in the field of language resources is essential in order to foster a substantial advancement of the field . this paper presents the background , overall objectives and methodology of work of the project , as well as a set of preliminary results .

empirical lower bounds on alignment error rates in syntax-based machine
the empirical adequacy of synchronous context-free grammars of rank two ( 2-scfgs ) ( satta and peserico , 2005 ) , used in syntaxbased machine translation systems such as wu ( 1997 ) , zhang et al ( 2006 ) and chiang ( 2007 ) , in terms of what alignments they induce , has been discussed in wu ( 1997 ) and wellington et al ( 2006 ) , but with a one-sided focus on so-called inside-out alignments . other alignment configurations that can not be induced by 2-scfgs are identified in this paper , and their frequencies across a wide collection of hand-aligned parallel corpora are examined . empirical lower bounds on two measures of alignment error rate , i.e . the one introduced in och and ney ( 2000 ) and one where only complete translation units are considered , are derived for 2-scfgs and related formalisms .

an archive for all of europe : the tractor initiative
tractor is the telri research archive of computational tools and resources . it features monolingual , bilingual , and multilingual corpora and lexicons in a wide variety of languages , as well as tools for language processing . tractor is a key element of telri ii , a paneuropean alliance of focal national language technology institutions with the emphasis on central and eastern european and nis countries . tractor hopes to complement other archives by providing a service for languages and users who are currently under-represented in existing archives . tractor 's unique strength lies in the amount of resources provided by centres in central and eastern europe , and its role at the hub of a network of resource creation , standardisation and distribution which links the eu and non-eu european research communities . the tractor user community brings together resource providers , academic users and industrial users in an ongoing relationship , which is designed to foster the emergence of joint research projects in language engineering . the tractor philosophy is to accept deposits of resources in any format , and to distribute them in the form in which they are received ( with small changes if possible such as additional documentation , and putting a browsable version or sample online . ) in addition , certain standards are recommended and help is offered to providers who wish to make their resources conformant with the standards . this lack of standardisation is not simply a pragmatic measure in the face of problems of heterogeneity , but is based on a profound scepticism towards current resource standardisation practice . in the future , tractor aims to build up particularly parallel corpora and tools for processing and extracting meaning from such resources .

donghui feng gully burns jingbo zhu eduard hovy
in this paper , we present an empirical study on adapting conditional random fields ( crf ) models to conduct semantic analysis on biomedical articles using active learning . we explore uncertaintybased active learning with the crf model to dynamically select the most informative training examples . this abridges the power of the supervised methods and expensive human annotation cost .

the textcap semantic
the lack of large amounts of readily available , explicitly represented knowledge has long been recognized as a barrier to applications requiring semantic knowledge such as machine translation and question answering . this problem is analogous to that facing machine translation decades ago , where one proposed solution was to use human translators to post-edit automatically produced , low quality translations rather than expect a computer to independently create high-quality translations . this paper describes an attempt at implementing a semantic parser that takes unrestricted english text , uses publically available computational linguistics tools and lexical resources and as output produces semantic triples which can be used in a variety of tasks such as generating knowledge bases , providing raw material for question answering systems , or creating rdf structures . we describe the textcap system , detail the semantic triple representation it produces , illustrate step by step how textcap processes a short text , and use its results on unseen texts to discuss the amount of post-editing that might be realistically required . 327 328 callaway

computing with getaruns
we present a system for text understanding called getaruns , in its deep version applicable only to closed domains . we will present the low level component organized according to lfg theory . the system also does pronominal binding , quantifier raising and temporal interpretation . then we will introduce the high level component where the discourse model is created from a text . texts belonging to closed domains are characterized by the fact that their semantics is controlled or under command of the system ; and most importantly , sentences making up the texts are fully parsed without failures . in practice , these texts are short and sentences are also below a certain threshold , typically less than 25 words . for longer sentences the system switches from the topdown to the bottomup system . in case of failure it will backoff to the partial system which produces a very lean and shallow semantics with no inference rules . the small text we will present contains what is called a psychological statement sentence which contributes an important bias as to the linking of the free pronominal expression contained in the last sentence . 287 288 delmonte 1 the system getaruns getaruns , the system for text understanding developed at the university of venice , is equipped with three main modules : a lower module for parsing where sentence strategies are implemented ; a middle module for semantic interpretation and discourse model construction which is cast into situation semantics ; and a higher module where reasoning and generation takes place .

interactive gesture in dialogue : a ptt model
gestures are usually looked at in isolation or from an intra-propositional perspective essentially tied to one speaker . the bielefeld multi-modal speech-andgesture-alignment ( saga ) corpus has many interactive gestures relevant for the structure of dialogue ( rieser 2008 , 2009 ) . to describe them , a dialogue theory is needed which can serve as a speechgesture interface . ptt ( poesio and traum 1997 , poesio and rieser submitted a ) can do this job in principle , how this can be achieved is the main topic of this paper . as a precondition , the empirical research procedure from systematic corpus annotation via gesture typology to a partial ontology for gestures is described . it is then explained how ptt is extended to provide an incremental modelling of speech plus gesture in an assertion-acknowledgement adjacency pair where grounding between dialogue participants is obtained through gesture .

im a belieber : social roles via self-identification and conceptual attributes , benjamin van durme
motivated by work predicting coarsegrained author categories in social media , such as gender or political preference , we explore whether twitter contains information to support the prediction of finegrained categories , or social roles . we find that the simple self-identification pattern i am a supports significantly richer classification than previously explored , successfully retrieving a variety of fine-grained roles . for a given role ( e.g. , writer ) , we can further identify characteristic attributes using a simple possessive construction ( e.g. , writers ) . tweets that incorporate the attribute terms in first person possessives ( my ) are confirmed to be an indicator that the author holds the associated social role .

translating dialectal arabic to english
we present a dialectal egyptian arabic to english statistical machine translation system that leverages dialectal to modern standard arabic ( msa ) adaptation . in contrast to previous work , we first narrow down the gap between egyptian and msa by applying an automatic characterlevel transformational model that changes egyptian to eg , which looks similar to msa . the transformations include morphological , phonological and spelling changes . the transformation reduces the out-of-vocabulary ( oov ) words from 5.2 % to 2.6 % and gives a gain of 1.87 bleu points . further , adapting large msa/english parallel data increases the lexical coverage , reduces oovs to 0.7 % and leads to an absolute bleu improvement of 2.73 points .

opinion mining using econometrics : a case study on reputation systems
deriving the polarity and strength of opinions is an important research topic , attracting significant attention over the last few years . in this work , to measure the strength and polarity of an opinion , we consider the economic context in which the opinion is evaluated , instead of using human annotators or linguistic resources . we rely on the fact that text in on-line systems influences the behavior of humans and this effect can be observed using some easy-to-measure economic variables , such as revenues or product prices . by reversing the logic , we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable . in effect , we use econometrics to identify the economic value of text and assign a dollar value to each opinion phrase , measuring sentiment effectively and without the need for manual labeling . we argue that by interpreting opinions using econometrics , we have the first objective , quantifiable , and contextsensitive evaluation of opinions . we make the discussion concrete by presenting results on the reputation system of amazon.com . we show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings .

error-driven analysis of challenges in coreference resolution
coreference resolution metrics quantify errors but do not analyze them . here , we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types . using this tool , we first compare the error distributions across a large set of systems , then analyze common errors across the top ten systems , empirically characterizing the major unsolved challenges of the coreference resolution task .

interactively exploring a machine translation model
this paper describes a method of interactively visualizing and directing the process of translating a sentence . the method allows a user to explore a model of syntax-based statistical machine translation ( mt ) , to understand the models strengths and weaknesses , and to compare it to other mt systems . using this visualization method , we can find and address conceptual and practical problems in an mt system . in our demonstration at acl , new users of our tool will drive a syntaxbased decoder for themselves .

inducing frame semantic verb classes from wordnet and ldoce
this paper presents semframe , a system that induces frame semantic verb classes from wordnet and ldoce . semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many languagebased applications . when compared to the handcrafted framenet , semframe achieves its best recall-precision balance with 83.2 % recall ( based on semframe 's coverage of framenet frames ) and 73.8 % precision ( based on semframe verbs semantic relatedness to frame-evoking verbs ) . the next best performing semantic verb classes achieve 56.9 % recall and 55.0 % precision .

discriminative language modeling with conditional random fields and the perceptron algorithm
this paper describes discriminative language modeling for a large vocabulary speech recognition task . we contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( crfs ) . the models are encoded as deterministic weighted finite state automata , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer . the perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data . however , using the feature set output from the perceptron algorithm ( initialized with their weights ) , crf training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % .

towards a data model for the universal corpus
we describe the design of a comparable corpus that spans all of the worlds languages and facilitates large-scale cross-linguistic processing . this universal corpus consists of text collections aligned at the document and sentence level , multilingual wordlists , and a small set of morphological , lexical , and syntactic annotations . the design encompasses submission , storage , and access . submission preserves the integrity of the work , allows asynchronous updates , and facilitates scholarly citation . storage employs a cloud-hosted filestore containing normalized source data together with a database of texts and annotations . access is permitted to the filestore , the database , and an application programming interface . all aspects of the universal corpus are open , and we invite community participation in its design and implementation , and in supplying and using its data .

error measures and bayes decision rules revisited with applications to pos tagging
starting from first principles , we re-visit the statistical approach and study two forms of the bayes decision rule : the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors . the bayes decision rule for minimizing the number of string errors is widely used , e.g . in speech recognition , pos tagging and machine translation , but its justification is rarely questioned . to minimize the number of symbol errors as is more suitable for a task like pos tagging , we show that another form of the bayes decision rule can be derived . the major purpose of this paper is to show that the form of the bayes decision rule should not be taken for granted ( as it is done in virtually all statistical nlp work ) , but should be adapted to the error measure being used . we present first experimental results for pos tagging tasks .

computational mechanisms for pun generation
computer pun-generators have so far relied on arbitrary semantic content , not linked to the immediate context . the mechanisms used , although tractable , may be of limited applicability . integrating puns into normal text may involve complex search .

a simple baseline for discriminating similar languages
this paper describes an approach to discriminating similar languages using word- and characterbased features , submitted as the queen mary university of london entry to the discriminating similar languages shared task . our motivation was to investigate how well a simple , datadriven , linguistically naive method could perform , in order to provide a baseline by which more linguistically complex or knowledge-rich approaches can be judged . using a standard supervised classifier with word and character n-grams as features , we achieved over 90 % accuracy in the test ; on fixing simple file handling and feature extraction bugs , this improved to over 95 % , comparable to the best submitted systems . similar accuracy is achieved using only word unigram features .

with a maximum entropy classifier
selection of natural-sounding referring expressions is useful in text generation and information summarization ( kan et al , 2001 ) . we use discourse-level feature predicates in a maximum entropy classifier ( berger et al , 1996 ) with binary and n-class classification to select referring expressions from a list . we find that while mention-type n-class classification produces higher accuracy of type , binary classification of individual referring expressions helps to avoid use of awkward referring expressions .

creative discovery in lexical ontologies
compound terms play a surprisingly key role in the organization of lexical ontologies . however , their inclusion forces one to address the issues of completeness and consistency that naturally arise from this organizational role . in this paper we show how creative exploration in the space of literal compounds can reveal not only additional compound terms to systematically balance an ontology , but can also discover new and potentially innovative concepts in their own right .

mining lexical variants from microblogs : an unsupervised multilingual
user-generated content has become a recurrent resource for nlp tools and applications , hence many efforts have been made lately in order to handle the noise present in short social media texts . the use of normalisation techniques has been proven useful for identifying and replacing lexical variants on some of the most informal genres such as microblogs . but annotated data is needed in order to train and evaluate these systems , which usually involves a costly process . until now , most of these approaches have been focused on english and they were not taking into account demographic variables such as the user location and gender . in this paper we describe the methodology used for automatically mining a corpus of variant and normalisation pairs from english and spanish tweets .

lekbot : a talking and playing robot for children with disabilities
this paper describes an ongoing project where we develop and evaluate a setup involving a communication board and a toy robot , which can communicate with each other via synthesised speech . the purpose is to provide children with communicative disabilities with a toy that is fun and easy to use together with peers , with and without disabilities . when the child selects a symbol on the communication board , the board speaks and the robot responds . this encourages the child to use language and learn to cooperate to reach a common goal . throughout the project , three children with cerebral palsy and their peers use the robot and provide feedback for further development . the multimodal interaction with the robot is video recorded and analysed together with observational data in activity diaries .

parallel algorithms for unsupervised tagging
we propose a new method for unsupervised tagging that finds minimal models which are then further improved by expectation maximization training . in contrast to previous approaches that rely on manually specified and multi-step heuristics for model minimization , our approach is a simple greedy approximation algorithm dmlc ( distributedminimum-label-cover ) that solves this objective in a single step . we extend the method and show how to efficiently parallelize the algorithm on modern parallel computing platforms while preserving approximation guarantees . the new method easily scales to large data and grammar sizes , overcoming the memory bottleneck in previous approaches . we demonstrate the power of the new algorithm by evaluating on various sequence labeling tasks : part-of-speech tagging for multiple languages ( including lowresource languages ) , with complete and incomplete dictionaries , and supertagging , a complex sequence labeling task , where the grammar size alone can grow to millions of entries . our results show that for all of these settings , our method achieves state-of-the-art scalable performance that yields high quality tagging outputs .

mine the easy , classify the hard : a semi-supervised approach to automatic sentiment classification
supervised polarity classification systems are typically domain-specific . building these systems involves the expensive process of annotating a large amount of data for each domain . a potential solution to this corpus annotation bottleneck is to build unsupervised polarity classification systems . however , unsupervised learning of polarity is difficult , owing in part to the prevalence of sentimentally ambiguous reviews , where reviewers discuss both the positive and negative aspects of a product . to address this problem , we propose a semi-supervised approach to sentiment classification where we first mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning , transductive learning , and ensemble learning .

active learning for multilingual statistical machine translation
statistical machine translation ( smt ) models require bilingual corpora for training , and these corpora are often multilingual with parallel text in multiple languages simultaneously . we introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructing high quality mt systems , from each language in the collection into this new target language . we show that adding a new language using active learning to the europarl corpus provides a significant improvement compared to a random sentence selection baseline . we also provide new highly effective sentence selection methods that improve al for phrase-based smt in the multilingual and single language pair setting .

a latent variable model for viewpoint discovery from threaded forum posts
threaded discussion forums provide an important social media platform . its rich user generated content has served as an important source of public feedback . to automatically discover the viewpoints or stances on hot issues from forum threads is an important and useful task . in this paper , we propose a novel latent variable model for viewpoint discovery from threaded forum posts . our model is a principled generative latent variable model which captures three important factors : viewpoint specific topic preference , user identity and user interactions . evaluation results show that our model clearly outperforms a number of baseline models in terms of both clustering posts based on viewpoints and clustering users with different viewpoints .

zipfr : word frequency distributions in r
we introduce the zipfr package , a powerful and user-friendly open-source tool for lnre modeling of word frequency distributions in the r statistical environment . we give some background on lnre models , discuss related software and the motivation for the toolkit , describe the implementation , and conclude with a complete sample session showing a typical lnre analysis .

genre-based paragraph classification for sentiment analysis
we present a taxonomy and classification system for distinguishing between different types of paragraphs in movie reviews : formal vs. functional paragraphs and , within the latter , between description and comment . the classification is used for sentiment extraction , achieving improvement over a baseline without paragraph classification .

realizing the costs : template-based surface realisation in the graph approach to referring expression generation
we describe a new realiser developed for the tuna 2009 challenge , and present its evaluation scores on the development set , showing a clear increase in performance compared to last years simple realiser .

is this a wampimuk cross-modal mapping between distributional semantics and the visual world
following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word . we then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task , in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts . by combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words . our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts .

paradigmatic modiability statistics for the extraction of complex multi-word terms language & information engineering language & information engineering
we here propose a new method which sets apart domain-specific terminology from common non-specific noun phrases . it is based on the observation that terminological multi-word groups reveal a considerably lesser degree of distributional variation than non-specific noun phrases . we define a measure for the observable amount of paradigmatic modifiability of terms and , subsequently , test it on bigram , trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus . using a community-wide curated biomedical terminology system as an evaluation gold standard , we show that our algorithm significantly outperforms a variety of standard term identification measures . we also provide empirical evidence that our methodolgy is essentially domain- and corpus-size-independent .

atypical prosodic structure as an indicator of reading level and text difficulty
automatic assessment of reading ability builds on applying speech recognition tools to oral reading , measuring words correct per minute . this work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study . experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase , i.e . in less appropriate locations . the results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions .

kernel-based reranking for named-entity extraction
we present novel kernels based on structured and unstructured features for reranking the n-best hypotheses of conditional random fields ( crfs ) applied to entity extraction . the former features are generated by a polynomial kernel encoding entity features whereas tree kernels are used to model dependencies amongst tagged candidate examples . the experiments on two standard corpora in two languages , i.e . the italian evalita 2009 and the english conll 2003 datasets , show a large improvement on crfs in f-measure , i.e . from 80.34 % to 84.33 % and from 84.86 % to 88.16 % , respectively . our analysis reveals that both kernels provide a comparable improvement over the crfs baseline . additionally , their combination improves crfs much more than the sum of the individual contributions , suggesting an interesting kernel synergy .

hello emily , how are you today personalised dialogue in a toy to engage children
in line with the growing interest in conversational agents as companions , we are developing a toy companion for children that is capable of engaging interactions and of developing a long-term relationship with them , and is extensible so as to evolve with them . in this paper , we investigate the importance of personalising interaction both for engagement and for long-term relationship development . in particular , we propose a framework for representing , gathering and using personal knowledge about the child during dialogue interaction .

using machine translation evaluation techniques to determine sentence-level semantic equivalence keihanna science city keihanna science city keihanna science city
the task of machine translation ( mt ) evaluation is closely related to the task of sentence-level semantic equivalence classification . this paper investigates the utility of applying standard mt evaluation methods ( bleu , nist , wer and per ) to building classifiers to predict semantic equivalence and entailment . we also introduce a novel classification method based on per which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence . our results show that mt evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment . our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .

polysemy in verbs : systematic relations between senses and their effect on annotation
sense inventories for polysemous predicates are often comprised by a number of related senses . in this paper , we examine different types of relations within sense inventories and give a qualitative analysis of the effects they have on decisions made by the annotators and annotator error . we also discuss some common traps and pitfalls in design of sense inventories . we use the data set developed specifically for the task of annotating sense distinctions dependent predominantly on semantics of the arguments and only to a lesser extent on syntactic frame .

a comparison of smoothing techniques for bilingual lexicon extraction from comparable corpora
smoothing is a central issue in language modeling and a prior step in different natural language processing ( nlp ) tasks . however , less attention has been given to it for bilingual lexicon extraction from comparable corpora . if a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging ( pekar et al , 2006 ) , no investigation of the many smoothing techniques has been carried out so far . in this paper , we present a study of some widelyused smoothing algorithms for language n-gram modeling ( laplace , good-turing , kneser-ney ... ) . our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach ( fung , 1998 ) traditionally used for bilingual lexicon extraction . we show that using smoothing as a preprocessing step of the standard approach increases its performance significantly .

american sign language generation : multimodal nlg with multiple linguistic channels
software to translate english text into american sign language ( asl ) animation can improve information accessibility for the majority of deaf adults with limited english literacy . asl natural language generation ( nlg ) is a special form of multimodal nlg that uses multiple linguistic output channels . asl nlg technology has applications for the generation of gesture animation and other communication signals that are not easily encoded as text strings .

sequential conditional generalized iterative scaling
we describe a speedup for training conditional maximum entropy models . the algorithm is a simple variation on generalized iterative scaling , but converges roughly an order of magnitude faster , depending on the number of constraints , and the way speed is measured . rather than attempting to train all model parameters simultaneously , the algorithm trains them sequentially . the algorithm is easy to implement , typically uses only slightly more memory , and will lead to improvements for most maximum entropy problems .

sentence type based reordering model for statistical machine
many reordering approaches have been proposed for the statistical machine translation ( smt ) system . however , the information about the type of source sentence is ignored in the previous works . in this paper , we propose a group of novel reordering models based on the source sentence type for chinese-toenglish translation . in our approach , an svm-based classifier is employed to classify the given chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . the different reordering models are developed oriented to the different sentence types . our experiments show that the novel reordering models have obtained an improvement of more than 2.65 % in bleu for a phrase-based spoken language translation system .

evaluating the impact of alternative dependency graph encodings on solving event extraction tasks
in state-of-the-art approaches to information extraction ( ie ) , dependency graphs constitute the fundamental data structure for syntactic structuring and subsequent knowledge elicitation from natural language documents . the top-performing systems in the bionlp 2009 shared task on event extraction all shared the idea to use dependency structures generated by a variety of parsers either directly or in some converted manner and optionally modified their output to fit the special needs of ie . as there are systematic differences between various dependency representations being used in this competition , we scrutinize on different encoding styles for dependency information and their possible impact on solving several ie tasks . after assessing more or less established dependency representations such as the stanford and conll-x dependencies , we will then focus on trimming operations that pave the way to more effective ie . our evaluation study covers data from a number of constituency- and dependency-based parsers and provides experimental evidence which dependency representations are particularly beneficial for the event extraction task . based on empirical findings from our study we were able to achieve the performance of 57.2 % f-score on the development data set of the bionlp shared task 2009 .

efficient third-order dependency parsers
we present algorithms for higher-order dependency parsing that are third-order in the sense that they can evaluate substructures containing three dependencies , and efficient in the sense that they require only o ( n4 ) time . importantly , our new parsers can utilize both sibling-style and grandchild-style interactions . we evaluate our parsers on the penn treebank and prague dependency treebank , achieving unlabeled attachment scores of 93.04 % and 87.38 % , respectively .

dialog state tracking using conditional random fields
this paper presents our approach to dialog state tracking for the dialog state tracking challenge task . in our approach we use discriminative general structured conditional random fields , instead of traditional generative directed graphic models , to incorporate arbitrary overlapping features . our approach outperforms the simple 1-best tracking approach .

linguistically-based sub-sentential alignment for terminology extraction from a bilingual automotive corpus language and translation technology team
we present a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical correspondences and syntactic similarity . we compare the performance of our subsentential alignment system with different symmetrization heuristics that combine the giza++ alignments of both translation directions . we demonstrate that the aligned linguistically motivated phrases are a useful means to extract bilingual terminology and more specifically complex multiword terms .

modeling interestingness with deep neural networks
this paper presents a deep semantic similarity model ( dssm ) , a special type of deep neural networks designed for text analysis , for recommending target documents to be of interest to a user based on a source document that she is reading . we observe , identify , and detect naturally occurring signals of interestingness in click transitions on the web between source and target documents , which we collect from commercial web browser logs . the dssm is trained on millions of web transitions , and maps source-target document pairs to feature vectors in a latent space in such a way that the distance between source documents and their corresponding interesting targets in that space is minimized . the effectiveness of the dssm is demonstrated using two interestingness tasks : automatic highlighting and contextual entity search . the results on large-scale , real-world datasets show that the semantics of documents are important for modeling interestingness and that the dssm leads to significant quality improvement on both tasks , outperforming not only the classic document models that do not use semantics but also state-of-the-art topic models .

abstractive meeting summarization with entailment and fusion
we propose a novel end-to-end framework for abstractive meeting summarization . we cluster sentences in the input into communities and build an entailment graph over the sentence communities to identify and select the most relevant sentences . we then aggregate those selected sentences by means of a word graph model . we exploit a ranking strategy to select the best path in the word graph as an abstract sentence . despite not relying on the syntactic structure , our approach significantly outperforms previous models for meeting summarization in terms of informativeness . moreover , the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality .

log-linear weight optimisation via bayesian adaptation in statistical
we present an adaptation technique for statistical machine translation , which applies the well-known bayesian learning paradigm for adapting the model parameters . since state-of-the-art statistical machine translation systems model the translation process as a log-linear combination of simpler models , we present the formal derivation of how to apply such paradigm to the weights of the log-linear combination . we show empirical results in which a small amount of adaptation data is able to improve both the non-adapted system and a system which optimises the abovementioned weights on the adaptation set only , while gaining both in reliability and speed .

mixture model-based minimum bayes risk decoding using multiple machine translation systems
we present mixture model-based minimum bayes risk ( mmmbr ) decoding , an approach that makes use of multiple smt systems to improve translation accuracy . unlike existing mbr decoding methods defined on the basis of single smt systems , an mmmbr decoder reranks translation outputs in the combined search space of multiple systems using the mbr decision rule and a mixture distribution of component smt models for translation hypotheses . mmmbr decoding is a general method that is independent of specific smt models and can be applied to various commonly used search spaces . experimental results on the nist chinese-to-english mt evaluation tasks show that our approach brings significant improvements to single system-based mbr decoding and outperforms a stateof-the-art system combination method .

a phonotactic language model for spoken language identification
we have established a phonotactic language model as the solution to spoken language identification ( lid ) . in this framework , we define a single set of acoustic tokens to represent the acoustic activities in the worlds spoken languages . a voice tokenizer converts a spoken document into a text-like document of acoustic tokens . thus a spoken document can be represented by a count vector of acoustic tokens and token n-grams in the vector space . we apply latent semantic analysis to the vectors , in the same way that it is applied in information retrieval , in order to capture salient phonotactics present in spoken documents . the vector space modeling of spoken utterances constitutes a paradigm shift in lid technology and has proven to be very successful . it presents a 12.4 % error rate reduction over one of the best reported results on the 1996 nist language recognition evaluation database .

fast and adaptive online training of feature-rich translation models
we present a fast and scalable online method for tuning statistical machine translation models with large feature sets . the standard tuning algorithmmertonly scales to tens of features . recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems . our method , which is based on stochastic gradient descent with an adaptive learning rate , scales to millions of features and tuning sets with tens of thousands of sentences , while still converging after only a few epochs . large-scale experiments on arabic-english and chinese-english show that our method produces significant translation quality gains by exploiting sparse features . equally important is our analysis , which suggests techniques for mitigating overfitting and domain mismatch , and applies to other recent discriminative methods for machine translation .

synalp-empathic : a valence shifting hybrid system for sentiment
this paper describes the synalp-empathic system that competed in semeval-2014 task 9b sentiment analysis in twitter . our system combines syntactic-based valence shifting rules with a supervised learning algorithm ( sequential minimal optimization ) . we present the system , its features and evaluate their impact . we show that both the valence shifting mechanism and the supervised model enable to reach good results .

learning recurrent event queries for web search
recurrent event queries ( req ) constitute a special class of search queries occurring at regular , predictable time intervals . the freshness of documents ranked for such queries is generally of critical importance . req forms a significant volume , as much as 6 % of query traffic received by search engines . in this work , we develop an improved req classifier that could provide significant improvements in addressing this problem . we analyze req queries , and develop novel features from multiple sources , and evaluate them using machine learning techniques . from historical query logs , we develop features utilizing query frequency , click information , and user intent dynamics within a search session . we also develop temporal features by time series analysis from query frequency . other generated features include word matching with recurrent event seed words and time sensitivity of search result set . we use naive bayes , svm and decision tree based logistic regression model to train req classifier . the results on test data show that our models outperformed baseline approach significantly .

fast and simple semantic class assignment for biomedical text computational bioscience program computational bioscience program computational bioscience program computational bioscience program
a simple and accurate method for assigning broad semantic classes to text strings is presented . the method is to map text strings to terms in ontologies based on a pipeline of exact matches , normalized strings , headword matching , and stemming headwords . the results of three experiments evaluating the technique are given . five semantic classes are evaluated against the craft corpus of full-text journal articles . twenty semantic classes are evaluated against the corresponding full ontologies , i.e . by reflexive matching . one semantic class is evaluated against a structured test suite . precision , recall , and f-measure on the corpus when evaluating against only the ontologies in the corpus is micro-averaged 67.06/78.49/72.32 and macro-averaged 69.84/83.12/75.31 . accuracy on the corpus when evaluating against all twenty semantic classes ranges from 77.12 % to 95.73 % . reflexive matching is generally successful , but reveals a small number of errors in the implementation .

investigating the contribution of distributional semantic information for dialogue act classification
this paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification . in contrast to the widely used bag-ofwords approach , we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication . we investigate the contribution of word sequence , dialogue act sequence , and distributional information to the performance , and compare with the current state of the art approaches . our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence ; more advanced approaches ( e.g . sequence- or grammar-driven , such as categorical , word vector composition ) are required .

fast and robust neural network joint models for statistical machine
recent work has shown success in using neural network language models ( nnlms ) as features in mt systems . here , we present a novel formulation for a neural network joint model ( nnjm ) , which augments the nnlm with a source context window . our model is purely lexicalized and can be integrated into any mt decoder . we also present several variations of the nnjm which provide significant additive improvements . although the model is quite simple , it yields strong empirical results . on the nist openmt12 arabic-english condition , the nnjm features produce a gain of +3.0 bleu on top of a powerful , featurerich baseline which already includes a target-only nnlm . the nnjm features also produce a gain of +6.3 bleu on top of a simpler baseline equivalent to chiangs ( 2007 ) original hiero implementation . additionally , we describe two novel techniques for overcoming the historically high cost of using nnlm-style models in mt decoding . these techniques speed up nnjm computation by a factor of 10,000x , making the model as fast as a standard back-off lm . this work was supported by darpa/i2o contract no .

unsupervised learning of name structure from coreference data
we present two methods for learning the structure of personal names from unlabeled data . the rst simply uses a few implicit constraints governing this structure to gain a toehold on the problem | e.g. , descriptors come before rst names , which come before middle names , etc . the second model also uses possible coreference information . we found that coreference constraints on names improve the performance of the model from 92.6 % to 97.0 % . we are interested in this problem in its own right , but also as a possible way to improve named entity recognition ( by recognizing the structure of different kinds of names ) and as a way to improve noun-phrase coreference determination .

learning word meanings and descriptive parameter spaces from music
the audio bitstream in music encodes a high amount of statistical , acoustic , emotional and cultural information . but music also has an important linguistic accessory ; most musical artists are described in great detail in record reviews , fan sites and news items . we highlight current and ongoing research into extracting relevant features from audio and simultaneously learning language features linked to the music . we show results in a query-bydescription task in which we learn the perceptual meaning of automatically-discovered single-term descriptive components , as well as a method of automatically uncovering semantically attached terms ( terms that have perceptual grounding . ) we then show recent work in semantic basis functions parameter spaces of description ( such as fast ... slow or male ... female ) that encode the highest descriptive variance in a semantic space .

modernizing historical slovene words with character-based smt
we propose a language-independent word normalization method exemplified on modernizing historical slovene words . our method relies on character-based statistical machine translation and uses only shallow knowledge . we present the relevant lexicons and two experiments . in one , we use a lexicon of historical word contemporary word pairs and a list of contemporary words ; in the other , we only use a list of historical words and one of contemporary ones . we show that both methods produce significantly better results than the baseline .

chinese word segmentation based on mixing multiple preprocessor
this paper describes the chinese word segmenter for our participation in cipssighan-2010 bake-off task of chinese word segmentation . we formalize the tasks as sequence tagging problems , and implemented them using conditional random fields ( crfs ) model . the system contains two modules : multiple preprocessor and basic segmenter . the basic segmenter is designed as a problem of character-based tagging , and using named entity recognition and chunk recognition based on boundary to preprocess . we participated in the open training on simplified chinese text and traditional chinese text , and our system achieved one rank # 5 and four rank # 2 best in all four domain corpus .

syntactic patterns versus word alignment : extracting opinion targets from online reviews
mining opinion targets is a fundamental and important task for opinion mining from online reviews . to this end , there are usually two kinds of methods : syntax based and alignment based methods . syntax based methods usually exploited syntactic patterns to extract opinion targets , which were however prone to suffer from parsing errors when dealing with online informal texts . in contrast , alignment based methods used word alignment model to fulfill this task , which could avoid parsing errors without using parsing . however , there is no research focusing on which kind of method is more better when given a certain amount of reviews . to fill this gap , this paper empirically studies how the performance of these two kinds of methods vary when changing the size , domain and language of the corpus . we further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not . in our experiments , we verify that our combination is effective on the corpus with small and medium size .

pre-reordering model of chinese special sentences for patent machine translation
chinese prepositions play an important role in sentence reordering , especially in patent texts . in this paper , a rule-based model is proposed to deal with the long distance reordering of sentences with special prepositions . we firstly identify the prepositions and their syntax levels . after that , sentences are parsed and transformed to be much closer to english word order with reordering rules . after integrating our method into a patent mt system , the reordering and translation results of source language are effectively improved .

online relative margin maximization for statistical machine translation
recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization , such as the spread of the data . however , these solutions are impractical in complex structured prediction problems such as statistical machine translation . we present an online gradient-based algorithm for relative margin maximization , which bounds the spread of the projected data while maximizing the margin . we evaluate our optimizer on chinese-english and arabicenglish translation tasks , each with small and large feature sets , and show that our learner is able to achieve significant improvements of 1.2-2 bleu and 1.7-4.3 ter on average over state-of-the-art optimizers with the large feature set .

icelandic data driven part of speech tagging
data driven pos tagging has achieved good performance for english , but can still lag behind linguistic rule based taggers for morphologically complex languages , such as icelandic . we extend a statistical tagger to handle fine grained tagsets and improve over the best icelandic pos tagger . additionally , we develop a case tagger for non-local case and gender decisions . an error analysis of our system suggests future directions .

phonological constraints and morphological preprocessing for ibm deutschland entwicklung
grapheme-to-phoneme conversion ( g2p ) is a core component of any text-to-speech system . we show that adding simple syllabification and stress assignment constraints , namely one nucleus per syllable and one main stress per word , to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy . secondly , we assessed morphological preprocessing for g2p conversion . while morphological information has been incorporated in some past systems , its contribution has never been quantitatively assessed for german . we compare the relevance of morphological preprocessing with respect to the morphological segmentation method , training set size , the g2p conversion algorithm , and two languages , english and german .

teaching dialogue to interdisciplinary teams through toolkits
we present some lessons we have learned from using software infrastructure to support coursework in natural language dialogue and embodied conversational agents . we have a new appreciation for the differences between coursework and research infrastructuresupporting teaching may be harder , because students require a broader spectrum of implementation , a faster learning curve and the ability to explore mistaken ideas as well as promising ones . we outline the collaborative discussion and effort we think is required to create better teaching infrastructure in the future .

broadly improving user classification via communication-based name and location clustering on twitter
hidden properties of social media users , such as their ethnicity , gender , and location , are often reflected in their observed attributes , such as their first and last names . furthermore , users who communicate with each other often have similar hidden properties . we propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of twitter users . attributes such as user names are grouped together if users with those names communicate with other similar users . we separately cluster millions of unique first names , last names , and userprovided locations . the efficacy of these clusters is then evaluated on a diverse set of classification tasks that predict hidden users properties such as ethnicity , geographic location , gender , language , and race , using only profile names and locations when appropriate . our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile , substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied .

large-coverage root lexicon extraction for hindi
this paper describes a method using morphological rules and heuristics , for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus . we cast the problem of high-coverage lexicon extraction as one of stemming followed by root word-form selection . we examine the use of pos tagging to improve precision and recall of stemming and thereby the coverage of the lexicon . we present accuracy , precision and recall scores for the system on a hindi corpus .

towards a generation-based semantic web authoring tool
widespread use of semantic web technologies requires interfaces through which knowledge can be viewed and edited without deep understanding of description logic and formalisms like owl and rdf . several groups are pursuing approaches based on controlled natural languages ( cnls ) , so that editing can be performed by typing in sentences which are automatically interpreted as statements in owl . we suggest here a variant of this approach which relies entirely on natural language generation ( nlg ) , and propose requirements for a system that can reliably generate transparent realisations of statements in description logic .

modelling and detecting decisions in multi-party dialogue
we describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party , human-human meetings . extending our previous work on action item identification , we propose a structured approach that takes into account the different roles utterances play in the decisionmaking process . we show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on flat annotations , while enabling the extraction of more fine-grained information that can be used for summarization and reporting .

learning with compositional semantics as structural inference for subsentential sentiment analysis
determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach . in particular , words or constituents within the expression can interact with each other to yield a particular overall polarity . in this paper , we view such subsentential interactions in light of compositional semantics , and present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure . our experiments show that ( 1 ) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics ( accuracy of 89.7 % vs. 89.1 % ) , but ( 2 ) a method that integrates compositional semantics into learning performs better than all other alternatives ( 90.7 % ) . we also find that contentword negators , not widely employed in previous work , play an important role in determining expression-level polarity . finally , in contrast to conventional wisdom , we find that expression-level classification accuracy uniformly decreases as additional , potentially disambiguating , context is considered .

noise as a tool for spoken language identification
segmental snr ( signal to noise ratio ) is considered to be a reasonable measure of perceptual quality of speech . however it only reflects the distortion in time dependent contour of the signal due to noise . objective measures such as log area ratio ( lar ) , itakura-saitio distortion ( is ) , log-likelihood ratio ( llr ) and weighted spectral slope ( wss ) are better measures of perceptual speech quality as they represent deviation in the spectrum . noise affects the speech time contour and the corresponding frequency content . different languages have some peculiar characteristics due to variation in the phonetic content and their distribution . distortion introduced by noise and application of enhancement algorithm varies for different phonemes . in this paper a novel idea of using noise and speech enhancement as means of identifying a language is presented , using objective measures of speech quality . study is done on three spoken indian regional languages namely kashmiri , bangla and manipuri , when corrupted by white noise . it is found that the objective measures of noisy speech , when determined using corresponding clear and enhanced speech are different for different languages over a range of snr , giving clue to the type of the language in use .

generating artificial errors for grammatical error correction
this paper explores the generation of artificial errors for correcting grammatical mistakes made by learners of english as a second language . artificial errors are injected into a set of error-free sentences in a probabilistic manner using statistics from a corpus . unlike previous approaches , we use linguistic information to derive error generation probabilities and build corpora to correct several error types , including open-class errors . in addition , we also analyse the variables involved in the selection of candidate sentences . experiments using the nucle corpus from the conll 2013 shared task reveal that : 1 ) training on artificially created errors improves precision at the expense of recall and 2 ) different types of linguistic information are better suited for correcting different error types .

better arabic parsing : baselines , evaluations , and analysis
in this paper , we offer broad insight into the underperformance of arabic constituency parsing by analyzing the interplay of linguistic phenomena , annotation choices , and model design . first , we identify sources of syntactic ambiguity understudied in the existing parsing literature . second , we show that although the penn arabic treebank is similar to other treebanks in gross statistical terms , annotation consistency remains problematic . third , we develop a human interpretable grammar that is competitive with a latent variable pcfg . fourth , we show how to build better models for three different parsers . finally , we show that in application settings , the absence of gold segmentation lowers parsing performance by 25 % f1 .

language computer corporation
this paper reports on lccs participation at the third pascal recognizing textual entailment challenge . first , we summarize our semantic logical-based approach which proved successful in the previous two challenges . then we highlight this years innovations which contributed to an overall accuracy of 72.25 % for the rte 3 test data . the novelties include new resources , such as extended wordnet kb which provides a large number of world knowledge axioms , event and temporal information provided by the tarsqi toolkit , logic form representations of events , negation , coreference and context , and new improvements of lexical chain axiom generation . finally , the systems performance and error analysis are discussed .

issues under negotiation
in this paper , we give an account of a simple kind of collaborative negotiative dialogue . we also sketch a formalization of this account and discuss its implementation in a dialogue system .

automatic extraction of definitions from german court decisions
this paper deals with the use of computational linguistic analysis techniques for information access and ontology learning within the legal domain . we present a rule-based approach for extracting and analysing definitions from parsed text and evaluate it on a corpus of about 6000 german court decisions . the results are applied to improve the quality of a text based ontology learning method on this corpus.

an automatic filter for non-parallel texts
numerous cross-lingual applications , including state-of-the-art machine translation systems , require parallel texts aligned at the sentence level . however , collections of such texts are often polluted by pairs of texts that are comparable but not parallel . bitext maps can help to discriminate between parallel and comparable texts . bitext mapping algorithms use a larger set of document features than competing approaches to this task , resulting in higher accuracy . in addition , good bitext mapping algorithms are not limited to documents with structural mark-up such as web pages . the task of filtering non-parallel text pairs represents a new application of bitext mapping algorithms .

supervised coreference resolution with sucre
in this paper we present sucre ( kobdani and schutze , 2010 ) that is a modular coreference resolution system participating in the conll-2011 shared task : modeling unrestricted coreference in ontonote ( pradhan et al. , 2011 ) . the sucres modular architecture provides a clean separation between data storage , feature engineering and machine learning algorithms .

creating a manually error-tagged and shallow-parsed learner corpus edward whittaker vera sheinman
the availability of learner corpora , especially those which have been manually error-tagged or shallow-parsed , is still limited . this means that researchers do not have a common development and test set for natural language processing of learner english such as for grammatical error detection . given this background , we created a novel learner corpus that was manually error-tagged and shallowparsed . this corpus is available for research and educational purposes on the web . in this paper , we describe it in detail together with its data-collection method and annotation schemes . another contribution of this paper is that we take the first step toward evaluating the performance of existing postagging/chunking techniques on learner corpora using the created corpus . these contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring .

semi-supervised dependency parsing using generalized tri-training
martins et al ( 2008 ) presented what to the best of our knowledge still ranks as the best overall result on the conllx shared task datasets . the paper shows how triads of stacked dependency parsers described in martins et al ( 2008 ) can label unlabeled data for each other in a way similar to co-training and produce end parsers that are significantly better than any of the stacked input parsers . we evaluate our system on five datasets from the conll-x shared task and obtain 1020 % error reductions , incl . the best reported results on four of them . we compare our approach to other semisupervised learning algorithms .

an extractive supervised two-stage method for sentence compression
we present a new method that compresses sentences by removing words . in a first stage , it generates candidate compressions by removing branches from the source sentences dependency tree using a maximum entropy classifier . in a second stage , it chooses the best among the candidate compressions using a support vector machine regression model . experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules .

discriminant ranking for efficient treebanking
treebank annotation is a labor-intensive and time-consuming task . in this paper , we show that a simple statistical ranking model can significantly improve treebanking efficiency by prompting human annotators , well-trained in disambiguation tasks for treebanking but not necessarily grammar experts , to the most relevant linguistic disambiguation decisions . experiments were carried out to evaluate the impact of such techniques on annotation efficiency and quality . the detailed analysis of outputs from the ranking model shows strong correlation to the human annotator behavior . when integrated into the treebanking environment , the model brings a significant annotation speed-up with improved inter-annotator agreement .

multi-dimensional annotation and alignment in an english-german computational linguistics &
this paper presents the compilation of the croco corpus , an english-german translation corpus . corpus design , annotation and alignment are described in detail . in order to guarantee the searchability and exchangeability of the corpus , xml stand-off mark-up is used as representation format for the multi-layer annotation . on this basis it is shown how the corpus can be queried using xquery . furthermore , the generalisation of results in terms of linguistic and translational research questions is briefly discussed .

training neural network language models on very large corpora
during the last years there has been growing interest in using neural networks for language modeling . in contrast to the well known back-off n-gram language models , the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space . this type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available . in this paper we present new algorithms to train a neural network language model on very large text corpora . this makes possible the use of the approach in domains where several hundreds of millions words of texts are available . the neural network language model is evaluated in a state-ofthe-art real-time continuous speech recognizer for french broadcast news . word error reductions of 0.5 % absolute are reported using only a very limited amount of additional processing time .

extracting the unextractable : a case study on verb-particles
this paper proposes a series of techniques for extracting english verbparticle constructions from raw text corpora . we initially propose three basic methods , based on tagger output , chunker output and a chunk grammar , respectively , with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verbpreposition pairs in ambiguous constructs . we then combine the three methods together into a single classifier , and add in a number of extra lexical and frequentistic features , producing a final f-score of 0.865 over the wsj .

a syntax-based statistical translation model
we present a syntax-based statistical translation model . our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node . these operations capture linguistic differences such as word order and case marking . model parameters are estimated in polynomial time using an em algorithm . the model produces word alignments that are better than those produced by ibm model 5 .

extracting latent attributes from video scenes using text as
we explore the novel task of identifying latent attributes in video scenes , such as the mental states of actors , using only large text collections as background knowledge and minimal information about the videos , such as activity and actor types . we formalize the task and a measure of merit that accounts for the semantic relatedness of mental state terms . we develop and test several largely unsupervised information extraction models that identify the mental states of human participants in video scenes . we show that these models produce complementary information and their combination significantly outperforms the individual models as well as other baseline methods .

a vague sense classifier for detecting vague definitions in ontologies
vagueness is a common human knowledge and linguistic phenomenon , typically manifested by predicates that lack clear applicability conditions and boundaries such as high , expert or bad . in the context of ontologies and semantic data , the usage of such predicates within ontology element definitions ( classes , relations etc . ) can hamper the latters quality , primarily in terms of shareability and meaning explicitness . with that in mind , we present in this paper a vague word sense classifier that may help both ontology creators and consumers to automatically detect vague ontology definitions and , thus , assess their quality better .

uba : using automatic translation and wikipedia for cross-lingual lexical substitution
this paper presents the participation of the university of bari ( uba ) at the semeval2010 cross-lingual lexical substitution task . the goal of the task is to substitute a word in a language l s , which occurs in a particular context , by providing the best synonyms in a different language l t which fit in that context . this task has a strict relation with the task of automatic machine translation , but there are some differences : cross-lingual lexical substitution targets one word at a time and the main goal is to find as many good translations as possible for the given target word . moreover , there are some connections with word sense disambiguation ( wsd ) algorithms . indeed , understanding the meaning of the target word is necessary to find the best substitutions . an important aspect of this kind of task is the possibility of finding synonyms without using a particular sense inventory or a specific parallel corpus , thus allowing the participation of unsupervised approaches . uba proposes two systems : the former is based on an automatic translation system which exploits google translator , the latter is based on a parallel corpus approach which relies on wikipedia in order to find the best substitutions .

czech named entity corpus and svm-based recognizer
this paper deals with recognition of named entities in czech texts . we present a recently released corpus of czech sentences with manually annotated named entities , in which a rich two-level classification scheme was used . there are around 6000 sentences in the corpus with roughly 33000 marked named entity instances . we use the data for training and evaluating a named entity recognizer based on support vector machine classification technique . the presented recognizer outperforms the results previously reported for ne recognition in czech .

detecting compositionality of multi-word expressions using nearest neighbours in vector space models
we present a novel unsupervised approach to detecting the compositionality of multi-word expressions . we compute the compositionality of a phrase through substituting the constituent words with their neighbours in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases . several methods of obtaining neighbours are presented . the results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings .

language-independent hybrid mt with presemt george tambouratzis sokratis sofianopoulos marina vassiliou
the present article provides a comprehensive review of the work carried out on developing presemt , a hybrid language-independent machine translation ( mt ) methodology . this methodology has been designed to facilitate rapid creation of mt systems for unconstrained language pairs , setting the lowest possible requirements on specialised resources and tools . given the limited availability of resources for many languages , only a very small bilingual corpus is required , while language modelling is performed by sampling a large target language ( tl ) monolingual corpus . the article summarises implementation decisions , using the greek-english language pair as a test case . evaluation results are reported , for both objective and subjective metrics . finally , main error sources are identified and directions are described to improve this hybrid mt methodology .

netease automatic chinese word segmentation
this document analyses the bakeoff results from netease co. in the sighan5 word segmentation task and named entity recognition task . the netease ws system is designed to facilitate research in natural language processing and information retrieval . it supports chinese and english word segmentation , chinese named entity recognition , chinese part of speech tagging and phrase conglutination . evaluation result shows our ws system has a passable precision in word segmentation except for the unknown words recognition .

unsupervised morphology-based vocabulary expansion
we present a novel way of generating unseen words , which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages . we test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set . the languages we study have very different morphological properties ; we show how our results differ depending on the morphological complexity of the language . in our best result ( on assamese ) , our approach can predict 29 % of the token-based out-of-vocabulary with a small amount of unlabeled training data .

may all your wishes come true : a study of wishes and how to recognize them
a wish is a desire or hope for something to happen . in december 2007 , people from around the world offered up their wishes to be printed on confetti and dropped from the sky during the famous new years eve ball drop in new york citys times square . we present an in-depth analysis of this collection of wishes . we then leverage this unique resource to conduct the first study on building general wish detectors for natural language text . wish detection complements traditional sentiment analysis and is valuable for collecting business intelligence and insights into the worlds wants and desires . we demonstrate the wish detectors effectiveness on domains as diverse as consumer product reviews and online political discussions .

joint emotion analysis via multi-task gaussian processes
we propose a model for jointly predicting multiple emotions in natural language sentences . our model is based on a low-rank coregionalisation approach , which combines a vector-valued gaussian process with a rich parameterisation scheme . we show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset . the proposed model outperforms both singletask baselines and other multi-task approaches .

anafora : a web-based general purpose annotation tool
anafora is a newly-developed open source web-based text annotation tool built to be lightweight , flexible , easy to use and capable of annotating with a variety of schemas , simple and complex . anafora allows secure web-based annotation of any plaintext file with both spanned ( e.g . named entity or markable ) and relation annotations , as well as adjudication for both types of annotation . anafora offers automatic set assignment and progress-tracking , centralized and humaneditable xml annotation schemas , and filebased storage and organization of data in a human-readable single-file xml format .

joint pos tagging and transition-based constituent parsing in chinese with non-local features
we propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers . first , to resolve the error propagation problem of the traditional pipeline approach , we incorporate pos tagging into the syntactic parsing process . second , to alleviate the negative influence of size differences among competing action sequences , we align parser states during beam-search decoding . third , to enhance the power of parsing models , we enlarge the feature set with non-local features and semisupervised word cluster features . experimental results show that these modifications improve parsing performance significantly . evaluated on the chinese treebank ( ctb ) , our final performance reaches 86.3 % ( f1 ) when trained on ctb 5.1 , and 87.1 % when trained on ctb 6.0 , and these results outperform all state-of-the-art parsers .

exploring text and image features to classify images in bioscience literature
a picture is worth a thousand words . biomedical researchers tend to incorporate a significant number of images ( i.e. , figures or tables ) in their publications to report experimental results , to present research models , and to display examples of biomedical objects . unfortunately , this wealth of information remains virtually inaccessible without automatic systems to organize these images . we explored supervised machine-learning systems using support vector machines to automatically classify images into six representative categories based on text , image , and the fusion of both . our experiments show a significant improvement in the average fscore of the fusion classifier ( 73.66 % ) as compared with classifiers just based on image ( 50.74 % ) or text features ( 68.54 % ) .

synchronous linear context-free rewriting systems for machine translation
we propose synchronous linear context-free rewriting systems as an extension to synchronous context-free grammars in which synchronized non-terminals span k 1 continuous blocks on each side of the bitext . such discontinuous constituents are required for inducing certain alignment configurations that occur relatively frequently in manually annotated parallel corpora and that can not be generated with less expressive grammar formalisms . as part of our investigations concerning the minimal k that is required for inducing manual alignments , we present a hierarchical aligner in form of a deduction system . we find that by restricting k to 2 on both sides , 100 % of the data can be covered .

data selection in semi-supervised learning for name tagging
we present two semi-supervised learning techniques to improve a state-of-the-art multi-lingual name tagger . for english and chinese , the overall system obtains 1.7 % - 2.1 % improvement in f-measure , representing a 13.5 % - 17.4 % relative reduction in the spurious , missing , and incorrect tags . we also conclude that simply relying upon large corpora is not in itself sufficient : we must pay attention to unlabeled data selection too . we describe effective measures to automatically select documents and sentences .

flors : fast and simple domain adaptation for part-of-speech tagging
we present flors , a new part-of-speech tagger for domain adaptation . flors uses robust representations that work especially well for unknown words and for known words with unseen tags . flors is simpler and faster than previous domain adaptation methods , yet it has significantly better accuracy than several baselines .

splitting complex temporal questions for question answering systems
this paper presents a multi-layered question answering ( q.a . ) architecture suitable for enhancing current q.a . capabilities with the possibility of processing complex questions . that is , questions whose answer needs to be gathered from pieces of factual information scattered in different documents . specifically , we have designed a layer oriented to process the different types of temporal questions . complex temporal questions are first decomposed into simpler ones , according to the temporal relationships expressed in the original question . in the same way , the answers of each simple question are re-composed , fulfilling the temporal restrictions of the original complex question . using this architecture , a temporal q.a . system has been developed . in this paper , we focus on explaining the first part of the process : the decomposition of the complex questions .

adapting discriminative reranking to grounded language learning
we adapt discriminative reranking to improve the performance of grounded language acquisition , specifically the task of learning to follow navigation instructions from observation . unlike conventional reranking used in syntactic and semantic parsing , gold-standard reference trees are not naturally available in a grounded setting . instead , we show how the weak supervision of response feedback ( e.g . successful task completion ) can be used as an alternative , experimentally demonstrating that its performance is comparable to training on gold-standard parse trees .

from a surface analysis to a dependency structure
this paper describes how we use the arrows properties from the 5p paradigm to generate a dependency structure from a surface analysis . besides the arrows properties , two modules , algas and ogre , are presented . moreover , we show how we express linguistic descriptions away from parsing decisions .

which words are hard to recognize prosodic , lexical , and disfluency factors that increase asr error rates
many factors are thought to increase the chances of misrecognizing a word in asr , including low frequency , nearby disfluencies , short duration , and being at the start of a turn . however , few of these factors have been formally examined . this paper analyzes a variety of lexical , prosodic , and disfluency factors to determine which are likely to increase asr error rates . findings include the following . ( 1 ) for disfluencies , effects depend on the type of disfluency : errors increase by up to 15 % ( absolute ) for words near fragments , but decrease by up to 7.2 % ( absolute ) for words near repetitions . this decrease seems to be due to longer word duration . ( 2 ) for prosodic features , there are more errors for words with extreme values than words with typical values . ( 3 ) although our results are based on output from a system with speaker adaptation , speaker differences are a major factor influencing error rates , and the effects of features such as frequency , pitch , and intensity may vary between speakers .

using parse features for preposition selection and error detection educational testing service
we evaluate the effect of adding parse features to a leading model of preposition usage . results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an esl error detection task . analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information .

quadratic-time dependency parsing for machine translation
efficiency is a prime concern in syntactic mt decoding , yet significant developments in statistical parsing with respect to asymptotic efficiency havent yet been explored in mt . recently , mcdonald et al ( 2005b ) formalized dependency parsing as a maximum spanning tree ( mst ) problem , which can be solved in quadratic time relative to the length of the sentence . they show that mst parsing is almost as accurate as cubic-time dependency parsing in the case of english , and that it is more accurate with free word order languages . this paper applies mst parsing to mt , and describes how it can be integrated into a phrase-based decoder to compute dependency language model scores . our results show that augmenting a state-ofthe-art phrase-based system with this dependency language model leads to significant improvements in ter ( 0.92 % ) and bleu ( 0.45 % ) scores on five nist chinese-english evaluation test sets .

verbose , laconic or just right : a simple computational model of content appropriateness under length constraints
length constraints impose implicit requirements on the type of content that can be included in a text . here we propose the first model to computationally assess if a text deviates from these requirements . specifically , our model predicts the appropriate length for texts based on content types present in a snippet of constant length . we consider a range of features to approximate content type , including syntactic phrasing , constituent compression probability , presence of named entities , sentence specificity and intersentence continuity . weights for these features are learned using a corpus of summaries written by experts and on high quality journalistic writing . during test time , the difference between actual and predicted length allows us to quantify text verbosity . we use data from manual evaluation of summarization systems to assess the verbosity scores produced by our model . we show that the automatic verbosity scores are significantly negatively correlated with manual content quality scores given to the summaries .

toward better chinese word segmentation for smt via bilingual
this study investigates on building a better chinese word segmentation model for statistical machine translation . it aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model . we propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised crfs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) . the induced word boundary information is encoded as a graph propagation constraint . the constrained model induction is accomplished by using posterior regularization algorithm . the experiments on a chinese-to-english machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality .

arabic spelling correction using supervised learning
in this work , we address the problem of spelling correction in the arabic language utilizing the new corpus provided by qalb ( qatar arabic language bank ) project which is an annotated corpus of sentences with errors and their corrections . the corpus contains edit , add before , split , merge , add after , move and other error types . we are concerned with the first four error types as they contribute more than 90 % of the spelling errors in the corpus . the proposed system has many models to address each error type on its own and then integrating all the models to provide an efficient and robust system that achieves an overall recall of 0.59 , precision of 0.58 and f1 score of 0.58 including all the error types on the development set . our system participated in the qalb 2014 shared task automatic arabic error correction and achieved an f1 score of 0.6 , earning the sixth place out of nine participants .

towards discipline-independent argumentative zoning : evidence from chemistry and computational linguistics
argumentative zoning ( az ) is an analysis of the argumentative and rhetorical structure of a scientific paper . it has been shown to be reliably used by independent human coders , and has proven useful for various information access tasks . annotation experiments have however so far been restricted to one discipline , computational linguistics ( cl ) . here , we present a more informative az scheme with 15 categories in place of the original 7 , and show that it can be applied to the life sciences as well as to cl . we use a domain expert to encode basic knowledge about the subject ( such as terminology and domain specific rules for individual categories ) as part of the annotation guidelines . our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains , chemistry and computational linguistics .

thoughtland : natural language descriptions for machine learning n-dimensional error functions
this demo showcases thoughtland , an end-to-end system that takes training data and a selected machine learning model , produces a cloud of points via crossvalidation to approximate its error function , then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an english text summarizing the error function .

controlled language for geographical information system queries
natural language interfaces to spatial databases have not received a lot of attention in computational linguistics , in spite of the potential value of such systems for users of geographical information systems ( giss ) . this paper presents a controlled language for gis queries , solves some of the semantic problems for spatial inference in this language , and introduces a system that implements this controlled language as a novel interface for gis .

search-based structured prediction applied to biomedical event extraction
we develop an approach to biomedical event extraction using a search-based structured prediction framework , searn , which converts the task into cost-sensitive classification tasks whose models are learned jointly . we show that searn improves on a simple yet strong pipeline by 8.6 points in f-score on the bionlp 2009 shared task , while achieving the best reported performance by a joint inference method . additionally , we consider the issue of cost estimation during learning and present an approach called focused costing that improves improves efficiency and predictive accuracy .

jane : open source hierarchical translation , extended with reordering and lexicon models
we present jane , rwths hierarchical phrase-based translation system , which has been open sourced for the scientific community . this system has been in development at rwth for the last two years and has been successfully applied in different machine translation evaluations . it includes extensions to the hierarchical approach developed by rwth as well as other research institutions . in this paper we give an overview of its main features . we also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance , and analyze the effect some recent extended lexicon models have on the performance of the system .

on a specific domain faculteit der letteren
domain portability and adaptation of nlp components and word sense disambiguation systems present new challenges . the difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based wsd systems . unfortunately , all existing evaluation datasets for specific domains are lexical-sample corpora . this task presented all-words datasets on the environment domain for wsd in four languages ( chinese , dutch , english , italian ) . 11 teams participated , with supervised and knowledge-based systems , mainly in the english dataset . the results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora . the most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain .

mostly passive information delivery a prototype
in this paper we introduce a new ui paradigm that mimics radio broadcast along with a prototype called radio one . the approach aims to present useful information from multiple domains to mobile users ( e.g . drivers on the go or cell phone users ) . the information is served in an entertaining manner in a mostly passive style without the user having to ask for it as in real radio broadcast . the content is generated on the fly by a machine and integrates a mix of personal ( calendar , emails ) and publicly available but customized information ( news , weather , pois ) . most of the spoken audio output is machine synthesized . the implemented prototype permits passive listening as well as interaction using voice commands or buttons . initial feedback gathered while testing the prototype while driving indicates good acceptance of the system and relatively low distraction levels .

towards statistical paraphrase generation : preliminary evaluations of div of information communication sciences
summary sentences are often paraphrases of existing sentences . they may be made up of recycled fragments of text taken from important sentences in an input document . we investigate the use of a statistical sentence generation technique that recombines words probabilistically in order to create new sentences . given a set of event-related sentences , we use an extended version of the viterbi algorithm which employs dependency relation and bigram probabilities to find the most probable summary sentence . using precision and recall metrics for verb arguments as a measure of grammaticality , we find that our system performs better than a bigram baseline , producing fewer spurious verb arguments .

elaine u dhonnchadha , josef van genabith
this paper looks at how computational linguistics ( cl ) and natural language processing ( nlp ) resources can be deployed in computerassisted language learning ( call ) materials for primary school learners . we draw a broad distinction between cl and nlp technology and briefly review the use of cl/nlp in e-learning in general , how it has been deployed in call to date and specifically in the primary school context . we outline how cl/nlp resources can be used in a project to teach irish and german to primary school children in ireland . this paper focuses on the use of finite state morphological analysis ( fst ) resources for irish and part of speech ( pos ) taggers for german .

accurate context-free parsing with combinatory categorial grammar
the definition of combinatory categorial grammar ( ccg ) in the literature varies quite a bit from author to author . however , the differences between the definitions are important in terms of the language classes of each ccg . we prove that a wide range of ccgs are strongly context-free , including the ccg of ccgbank and of the parser of clark and curran ( 2007 ) . in light of these new results , we train the pcfg parser of petrov and klein ( 2007 ) on ccgbank and achieve state of the art results in supertagging accuracy , parseval measures and dependency accuracy .

pos-tagging different varieties of occitan with single-dialect resources
in this study , we tackle the question of pos-tagging written occitan , a lesser-resourced language with multiple dialects each containing several varieties . for pos-tagging , we use a supervised machine learning approach , requiring annotated training and evaluation corpora and optionally a lexicon , all of which were prepared as part of the study . although we evaluate two dialects of occitan , lengadocian and gascon , the training material and lexicon concern only lengadocian . we concluded that reasonable results ( > 89 % accuracy ) are possible with a very limited training corpus ( 2500 tokens ) , as long as it is compensated by intensive use of the lexicon . results are much lower across dialects , and pointers are provided for improvement . finally , we compare the relative contribution of more training material vs. a larger lexicon , and conclude that within our configuration , spending effort on lexicon construction yields higher returns .

comparing automatic evaluation measures for image description
image description is a new natural language generation task , where the aim is to generate a human-like description of an image . the evaluation of computer-generated text is a notoriously difficult problem , however , the quality of image descriptions has typically been measured using unigram bleu and human judgements . the focus of this paper is to determine the correlation of automatic measures with human judgements for this task . we estimate the correlation of unigram and smoothed bleu , ter , rouge-su4 , and meteor against human judgements on two data sets . the main finding is that unigram bleu has a weak correlation , and meteor has the strongest correlation with human judgements .

linguistic features for automatic evaluation of heterogenous mt systems
evaluation results recently reported by callison-burch et al ( 2006 ) and koehn and monz ( 2006 ) , revealed that , in certain cases , the bleu metric may not be a reliable mt quality indicator . this happens , for instance , when the systems under evaluation are based on different paradigms , and therefore , do not share the same lexicon . the reason is that , while mt quality aspects are diverse , bleu limits its scope to the lexical dimension . in this work , we suggest using metrics which take into account linguistic features at more abstract levels . we provide experimental results showing that metrics based on deeper linguistic information ( syntactic/shallow-semantic ) are able to produce more reliable system rankings than metrics based on lexical matching alone , specially when the systems under evaluation are of a different nature .

kernels and transductive inference for biological entity recognition
we address the problem of using partially labelled data , eg large collections were only little data is annotated , for extracting biological entities . our approach relies on a combination of probabilistic models , which we use to model the generation of entities and their context , and kernel machines , which implement powerful categorisers based on a similarity measure and some labelled data . this combination takes the form of the so-called fisher kernels which implement a similarity based on an underlying probabilistic model . such kernels are compared with transductive inference , an alternative approach to combining labelled and unlabelled data , again coupled with support vector machines . experiments are performed on a database of abstracts extracted from medline .

testing a grammar customization system with sahaptin
i briefly describe a system for automatically creating an implemented grammar of a natural language based on answers to a web-based questionnaire , then present a grammar of sahaptin , a language of the pacific northwest with complex argument-marking and agreement patterns , that was developed to test the system . the development of this grammar has proved useful in three ways : ( 1 ) verifying the correct functioning of the grammar customization system , ( 2 ) motivating the addition of a new pattern of agreement to the system , and ( 3 ) making detailed predictions that uncovered gaps in the linguistic descriptions of sahaptin .

the complexity of phrase alignment problems
many phrase alignment models operate over the combinatorial space of bijective phrase alignments . we prove that finding an optimal alignment in this space is np-hard , while computing alignment expectations is # p-hard . on the other hand , we show that the problem of finding an optimal alignment can be cast as an integer linear program , which provides a simple , declarative approach to viterbi inference for phrase alignment models that is empirically quite efficient .

modifying so-pmi for japanese weblog opinion mining by using a balancing factor and detecting neutral expressions
we propose a variation of the so-pmi algorithm for japanese , for use in weblog opinion mining . so-pmi is an unsupervised approach proposed by turney that has been shown to work well for english . we first used the so-pmi algorithm on japanese in a way very similar to turneys original idea . the result of this trial leaned heavily toward positive opinions . we then expanded the reference words to be sets of words , tried to introduce a balancing factor and to detect neutral expressions . after these modifications , we achieved a wellbalanced result : both positive and negative accuracy exceeded 70 % . this shows that our proposed approach not only adapted the so-pmi for japanese , but also modified it to analyze japanese opinions more effectively .

margin-based decomposed amortized inference
given that structured output prediction is typically performed over entire datasets , one natural question is whether it is possible to re-use computation from earlier inference instances to speed up inference for future instances . amortized inference has been proposed as a way to accomplish this . in this paper , first , we introduce a new amortized inference algorithm called the margin-based amortized inference , which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal . second , we introduce decomposed amortized inference , which is designed to address very large inference problems , where earlier amortization methods become less effective . this approach works by decomposing the output structure and applying amortization piece-wise , thus increasing the chance that we can re-use previous solutions for parts of the output structure . these parts are then combined to a global coherent solution using lagrangian relaxation . in our experiments , using the nlp tasks of semantic role labeling and entityrelation extraction , we demonstrate that with the margin-based algorithm , we need to call the inference engine only for a third of the test examples . further , we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls .

corporate news classification and valence prediction : a supervised syed aqueel haider rishabh mehrotra
news articles have always been a prominent force in the formation of a companys financial image in the minds of the general public , especially the investors . given the large amount of news being generated these days through various websites , it is possible to mine the general sentiment of a particular company being portrayed by media agencies over a period of time , which can be utilized to gauge the long term impact on the investment potential of the company . however , given such a vast amount of news data , we need to first separate corporate news from other kinds namely , sports , entertainment , science & technology , etc . we propose a system which takes news as , checks whether it is of corporate nature , and then identifies the polarity of the sentiment expressed in the news . the system is also capable of distinguishing the company/organization which is the subject of the news from other organizations which find mention , and this is used to pair the sentiment polarity with the identified company .

an ensemble method for selection of high quality parses
while the average performance of statistical parsers gradually improves , they still attach to many sentences annotations of rather low quality . the number of such sentences grows when the training and test data are taken from different domains , which is the case for major web applications such as information retrieval and question answering . in this paper we present a sample ensemble parse assessment ( sepa ) algorithm for detecting parse quality . we use a function of the agreement among several copies of a parser , each of which trained on a different sample from the training data , to assess parse quality . we experimented with both generative and reranking parsers ( collins , charniak and johnson respectively ) . we show superior results over several baselines , both when the training and test data are from the same domain and when they are from different domains . for a test setting used by previous work , we show an error reduction of 31 % as opposed to their 20 % .

emotive or non-emotive : that is the question michal ptaszynski fumito masui rafal rzepka kenji araki
in this research we focus on discriminating between emotive ( emotionally loaded ) and non-emotive sentences . we define the problem from a linguistic point of view assuming that emotive sentences stand out both lexically and grammatically . we verify this assumption experimentally by comparing two sets of such sentences in japanese . the comparison is based on words , longer n-grams as well as more sophisticated patterns . in the classification we use a novel unsupervised learning algorithm based on the idea of language combinatorics . the method reached results comparable to the state of the art , while the fact that it is fully automatic makes it more efficient and language independent .

a best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora
translation systems that automatically extract transfer mappings ( rules or examples ) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings . we describe an algorithm that uses a bestfirst strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted . for each mapping , frequencies are computed and sufficient context is retained to distinguish competing mappings during translation . variants of the algorithm are run against a corpus containing 200k sentence pairs and evaluated based on the quality of resulting translations .

unsupervised classification of sentiment and objectivity in chinese text
we address the problem of sentiment and objectivity classification of product reviews in chinese . our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum ; we argue that this is desirable from the perspective of would-be customers who read the reviews . we use novel unsupervised techniques , including a one-word 'seed ' vocabulary and iterative retraining for sentiment processing , and a criterion of 'sentiment density ' for determining the extent to which a document is opinionated . the classifier achieves up to 87 % f-measure for sentiment polarity detection .

flexible , corpus-based modelling of human plausibility judgements
in this paper , we consider the computational modelling of human plausibility judgements for verb-relation-argument triples , a task equivalent to the computation of selectional preferences . such models have applications both in psycholinguistics and in computational linguistics . by extending a recent model , we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements . it rivals or exceeds deeper , resource-driven models while exhibiting higher coverage . moreover , we show that our model can be combined with deeper models to obtain better predictions than from either model alone .

quantitative portraits of lexical elements
this paper clarifies the basic concepts and theoretical perspectives by and from which quantitative weighting of lexical elements are defined , and then draws , quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined .

automatic set expansion for list question answering
this paper explores the use of set expansion ( se ) to improve question answering ( qa ) when the expected answer is a list of entities belonging to a certain class . given a small set of seeds , se algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds . we explore the hypothesis that a noise-resistant se algorithm can be used to extend candidate answers produced by a qa system and generate a new list of answers that is better than the original list produced by the qa system . we further introduce a hybrid approach which combines the original answers from the qa system with the output from the se algorithm . experimental results for several state-of-the-art qa systems show that the hybrid system performs better than the qa systems alone when tested on list question data from past trec evaluations .

a pattern approach for biomedical event annotation
we describe our approach for the genia event extraction in the main task of bionlp shared task 2011. there are two important parts in our method : event trigger annotation and event extraction . we use rules and dictionary to annotate event triggers . event extraction is based on patterns created from dependent graphs . we apply uima framework to support all stages in our system .

learning to follow navigational directions
we present a system that learns to follow navigational natural language directions . where traditional models learn from linguistic annotation or word distributions , our approach is grounded in the world , learning by apprenticeship from routes through a map paired with english descriptions . lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route . we learn this correspondence with a reinforcement learning algorithm , using the deviation of the route we follow from the intended path as a reward signal . we demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths .

on the parameterized complexity of linear context-free rewriting systems
we study the complexity of uniform membership for linear context-free rewriting systems , i.e. , the problem where we are given a string w and a grammar g and are asked whether w l ( g ) . in particular , we use parameterized complexity theory to investigate how the complexity depends on various parameters . while we focus primarily on rank and fan-out , derivation length is also considered .

efficient , correct , unsupervised learning of context-sensitive languages
a central problem for nlp is grammar induction : the development of unsupervised learning algorithms for syntax . in this paper we present a lattice-theoretic representation for natural language syntax , called distributional lattice grammars . these representations are objective or empiricist , based on a generalisation of distributional learning , and are capable of representing all regular languages , some but not all context-free languages and some noncontext-free languages . we present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm .

constraint-driven rank-based learning for information extraction sameer singh limin yao sebastian riedel andrew mccallum
most learning algorithms for undirected graphical models require complete inference over at least one instance before parameter updates can be made . samplerank is a rankbased learning framework that alleviates this problem by updating the parameters during inference . most semi-supervised learning algorithms also perform full inference on at least one instance before each parameter update . we extend samplerank to semi-supervised learning in order to circumvent this computational bottleneck . different approaches to incorporate unlabeled data and prior knowledge into this framework are explored . when evaluated on a standard information extraction dataset , our method significantly outperforms the supervised method , and matches results of a competing state-of-the-art semi-supervised learning approach .

structural semantic interconnection : a knowledge-based approach to word dipartimento di informatica , dipartimento di informatica ,
in this paper we describe the ssi algorithm , a structural pattern matching algorithm for wsd . the algorithm has been applied to the gloss disambiguation task of senseval-3 .

a compositional approach toward dynamic phrasal
to enhance the technology for computing semantic equivalence , we introduce the notion of phrasal thesaurus which is a natural extension of conventional word-based thesaurus . among a variety of phrases that conveys the same meaning , i.e. , paraphrases , we focus on syntactic variants that are compositionally explainable using a small number of atomic knowledge , and develop a system which dynamically generates such variants . this paper describes the proposed system and three sorts of knowledge developed for dynamic phrasal thesaurus in japanese : ( i ) transformation pattern , ( ii ) generation function , and ( iii ) lexical function .

an explicit statistical model of learning lexical segmentation using
this paper presents an unsupervised and incremental model of learning segmentation that combines multiple cues whose use by children and adults were attested by experimental studies . the cues we exploit in this study are predictability statistics , phonotactics , lexical stress and partial lexical information . the performance of the model presented in this paper is competitive with the state-of-the-art segmentation models in the literature , while following the child language acquisition more faithfully . besides the performance improvements over the similar models in the literature , the cues are combined in an explicit manner , allowing easier interpretation of what the model learns .

empirical methods for evaluating dialog systems paper id : sigdial_tp
we examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose . the methods include a protocol for conducting a wizard-of-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or gold standard for comparative judgments . the methods also provide a practical means of optimizing the system through component analysis and cost valuation . empirical methods for evaluating dialog systems abstract we examine what purpose a dialog metric serves and then propose empirical methods for evaluating systems that meet that purpose . the methods include a protocol for conducting a wizardof-oz experiment and a basic set of descriptive statistics for substantiating performance claims using the data collected from the experiment as an ideal benchmark or gold standard for comparative judgments . the methods also provide a practical means of optimizing the system through component analysis and cost valuation .

a corpus study of evaluative and speculative language
this paper presents a corpus study of evaluative and speculative language . knowledge of such language would be useful in many applications , such as text categorization and summarization . analyses of annotator agreement and of characteristics of subjective language are performed . this study yields knowledge needed to design e ective machine learning systems for identifying subjective language .

dialog & task performance when vision is bandwidth-limited
the prospect of human commanders teaming with mobile robots smart enough to undertake joint exploratory tasksespecially tasks that neither commander nor robot could perform alonerequires novel methods of preparing and testing human-robot teams for these ventures prior to real-time operations . in this paper , we report work-in-progress that maintains face validity of selected configurations of resources and people , as would be available in emergency circumstances . more specifically , from an off-site post , we ask human commanders ( c ) to perform an exploratory task in collaboration with a remotely located human robot-navigator ( rn ) who controls the navigation of , but can not see the physical robot ( r ) . we impose network bandwidth restrictions in two mission scenarios comparable to real circumstances by varying the availability of sensor , image , and video signals to rn , in effect limiting the human rn to function as an automation stand-in . to better understand the capabilities and language required in such configurations , we constructed multi-modal corpora of time-synced dialog , video , and lidar files recorded during task sessions . we can now examine commander/robot dialogs while replaying what c and rn saw , to assess their task performance under these varied conditions .

wlv : a confidence-based machine learning method
this article presents the machine learning approach used by the university of wolverhampton in the grec-neg09 task . a classifier based on j48 decision tree and a meta-classifier were used to produce two runs . evaluation on the development set shows that the metaclassifier achieves a better performance .

retrieval of reading materials for vocabulary and reading practice
finding appropriate , authentic reading materials is a challenge for language instructors . the web is a vast resource of texts , but most pages are not suitable for reading practice , and commercial search engines are not well suited to finding texts that satisfy pedagogical constraints such as reading level , length , text quality , and presence of target vocabulary . we present a system that uses various language technologies to facilitate the retrieval and presentation of authentic reading materials gathered from the web . it is currently deployed in two english as a second language courses at the university of pittsburgh .

description of the ncu chinese word segmentation and part-of-speech
in chinese , most of the language processing starts from word segmentation and part-of-speech ( pos ) tagging . these two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word . in this paper , we present two distinct sequential tagging models for the above two tasks . the first word segmentation model was basically similar to previous work which made use of conditional random fields ( crf ) and set of predefined dictionaries to recognize word boundaries . second , we revise and modify support vector machine-based chunking model to label the pos tag in the tagging task . our method in the ws task achieves moderately rank among all participants , while in the pos tagging task , it reaches very competitive results .

emotion cause detection with linguistic constructions
this paper proposes a multi-label approach to detect emotion causes . the multi-label model not only detects multi-clause causes , but also captures the long-distance information to facilitate emotion cause detection . in addition , based on the linguistic analysis , we create two sets of linguistic patterns during feature extraction . both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes . experiments show that our system achieves a performance much higher than a baseline model .

improving arabic-to-english statistical machine translation by reordering post-verbal subjects for alignment
we study the challenges raised by arabic verb and subject detection and reordering in statistical machine translation ( smt ) . we show that post-verbal subject ( vs ) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to english . in addition , implementing reordering is difficult because the boundaries of vs constructions are hard to detect accurately , even with a state-of-the-art arabic dependency parser . we therefore propose to reorder vs constructions into sv order for smt word alignment only . this strategy significantly improves bleu and ter scores , even on a strong large-scale baseline and despite noisy parses .

a new set of norms for semantic relatedness measures
we have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into a new set of relatedness norms that we call rel-122 . judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means ( r = 0.77 , = 0.09 , n = 73 ) , although not as high as resniks ( 1995 ) upper bound for expected average human correlation to similarity means ( r = 0.90 ) . this suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness . we compare the results of several wordnet-based similarity and relatedness measures to our rel-122 norms and demonstrate the limitations of wordnet for discovering general indications of semantic relatedness . we also offer a critique of the fields reliance upon similarity norms to evaluate relatedness measures .

using word similarity lists for resolving indirect anaphora pipca - unisinos
in this work we test the use of word similarity lists for anaphora resolution in portuguese corpora . we applied an automatic lexical acquisition technique over parsed texts to identify semantically similar words . after that , we made use of this lexical knowledge to resolve coreferent definite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent , which we call indirect anaphora .

multigraph clustering for unsupervised coreference resolution
we present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph . the model outperforms most systems participating in the english track of the conll12 shared task .

feeding owl : extracting and representing the content of pathology reports
this paper reports on an ongoing project that combines nlp with semantic web technologies to support a content-based storage and retrieval of medical pathology reports . we describe the nlp component of the project ( a robust parser ) and the background knowledge component ( a domain ontology represented in owl ) , and how they work together during extraction of domain specific information from natural language reports . the system provides a good example of how nlp techniques can be used to populate the semantic web .

social ( distributed ) language modeling , clustering and dialectometry
we present ongoing work in a scalable , distributed implementation of over 200 million individual language models , each capturing a single users dialect in a given language ( multilingual users have several models ) . these have a variety of practical applications , ranging from spam detection to speech recognition , and dialectometrical methods on the social graph . users should be able to view any content in their language ( even if it is spoken by a small population ) , and to browse our site with appropriately translated interface ( automatically generated , for locales with little crowd-sourced community effort ) .

trainable sentence planning for complex information presentation in spoken dialog systems
a challenging problem for spoken dialog systems is the design of utterance generation modules that are fast , flexible and general , yet produce high quality output in particular domains . a promising approach is trainable generation , which uses general-purpose linguistic knowledge automatically adapted to the application domain . this paper presents a trainable sentence planner for the match dialog system . we show that trainable sentence planning can produce output comparable to that of matchs template-based generator even for quite complex information presentations .

( semi- ) automatic detection of errors in pos-tagged corpora
this paper presents a simple yet in practice very efficient technique serving for automatic detection of those positions in a partof-speech tagged corpus where an error is to be suspected . the approach is based on the idea of learning and later application of '' negative bigrams '' , i.e . on the search for pairs of adjacent tags which constitute an incorrect configuration in a text of a particular language ( in english , e.g. , the bigram article - finite verb ) . further , the paper describes the generalization of the `` negative bigrams '' into `` negative n-grams '' , for any natural n , which indeed provides a powerful tool for error detection in a corpus . the implementation is also discussed , as well as evaluation of results of the approach when used for error detection in the negra corpus of german , and the general implications for the quality of results of statistical taggers . illustrative examples in the text are taken from german , and hence at least a basic command of this language would be helpful for their understanding - due to the complexity of the necessary accompanying explanation , the examples are neither glossed nor translated . however , the central ideas of the paper should be understandable also without any knowledge of german .

modeling machine transliteration as a phrase based statistical machine
in this paper we use the popular phrasebased smt techniques for the task of machine transliteration , for english-hindi language pair . minimum error rate training has been used to learn the model weights . we have achieved an accuracy of 46.3 % on the test set . our results show these techniques can be successfully used for the task of machine transliteration .

transformation-based learning in the fast lane and radu florian
transformation-based learning has been successfully employed to solve many natural language processing problems . it achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily . however , it does have a serious drawback : the training time is often intorelably long , especially on the large corpora which are often used in nlp . in this paper , we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacricing performance . the paper compares and contrasts the training time needed and performance achieved by our modied learner with two other systems : a standard transformation-based learner , and the ica system ( hepple , 2000 ) . the results of these experiments show that our system is able to achieve a signicant improvement in training time while still achieving the same performance as a standard transformation-based learner . this is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution .

inducing sound segment differences using pair hidden markov models
pair hidden markov models ( pairhmms ) are trained to align the pronunciation transcriptions of a large contemporary collection of dutch dialect material , the goemantaeldeman-van reenen-project ( gtrp , collected 19801995 ) . we focus on the question of how to incorporate information about sound segment distances to improve sequence distance measures for use in dialect comparison . pairhmms induce segment distances via expectation maximisation ( em ) . our analysis uses a phonologically comparable subset of 562 items for all 424 localities in the netherlands . we evaluate the work first via comparison to analyses obtained using the levenshtein distance on the same dataset and second , by comparing the quality of the induced vowel distances to acoustic differences .

fitting sentence level translation evaluation with many dense features
sentence level evaluation in mt has turned out far more difficult than corpus level evaluation . existing sentence level metrics employ a limited set of features , most of which are rather sparse at the sentence level , and their intricate models are rarely trained for ranking . this paper presents a simple linear model exploiting 33 relatively dense features , some of which are novel while others are known but seldom used , and train it under the learning-to-rank framework . we evaluate our metric on the standard wmt12 data showing that it outperforms the strong baseline meteor . we also analyze the contribution of individual features and the choice of training data , language-pair vs. target-language data , providing new insights into this task .

using structured events to predict stock price movement : an empirical investigation
it has been shown that news events influence the trends of stock price movements . however , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence can not represent complete and exact events . recent advances in open information extraction ( open ie ) techniques enable the extraction of structured events from web-scale data . we propose to adapt open ie technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts . both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market . largescale experiments show that the accuracy of s & p 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % . our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on s & p 500 stock historical data .

evaluating unsupervised learning for natural language processing tasks
the development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research . the primary advantage of these methods is that they do not require annotated data to learn a model . however , this advantage makes them difficult to evaluate against a manually labeled gold standard . using unsupervised part-of-speech tagging as our case study , we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods . instead , we argue that the rarely used in-context evaluation is more appropriate and more informative , as it takes into account the way these methods are likely to be applied . finally , bearing the issue of evaluation in mind , we propose directions for future work in unsupervised natural language processing .

parsing mildly non-projective dependency structures
we present parsing algorithms for various mildly non-projective dependency formalisms . in particular , algorithms are presented for : all well-nested structures of gap degree at most 1 , with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power ; all well-nested structures with gap degree bounded by any constant k ; and a new class of structures with gap degree up to k that includes some ill-nested structures . the third case includes all the gap degree k structures in a number of dependency treebanks .

optimizing grammars for minimum dependency length
we examine the problem of choosing word order for a set of dependency trees so as to minimize total dependency length . we present an algorithm for computing the optimal layout of a single tree as well as a numerical method for optimizing a grammar of orderings over a set of dependency types . a grammar generated by minimizing dependency length in unordered trees from the penn treebank is found to agree surprisingly well with english word order , suggesting that dependency length minimization has influenced the evolution of english .

incorporating content structure into text analysis applications
in this paper , we investigate how modeling content structure can benefit text analysis applications such as extractive summarization and sentiment analysis . this follows the linguistic intuition that rich contextual information should be useful in these tasks . we present a framework which combines a supervised text analysis application with the induction of latent content structure . both of these elements are learned jointly using the em algorithm . the induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task . we demonstrate that exploiting content structure yields significant improvements over approaches that rely only on local context.1

towards automatic scoring of non-native spontaneous speech educational testing service
this paper investigates the feasibility of automated scoring of spoken english proficiency of non-native speakers . unlike existing automated assessments of spoken english , our data consists of spontaneous spoken responses to complex test items . we perform both a quantitative and a qualitative analysis of these features using two different machine learning approaches . ( 1 ) we use support vector machines to produce a score and evaluate it with respect to a mode baseline and to human rater agreement . we find that scoring based on support vector machines yields accuracies approaching inter-rater agreement in some cases . ( 2 ) we use classification and regression trees to understand the role of different features and feature classes in the characterization of speaking proficiency by human scorers . our analysis shows that across all the test items most or all the feature classes are used in the nodes of the trees suggesting that the scores are , appropriately , a combination of multiple components of speaking proficiency . future research will concentrate on extending the set of features and introducing new feature classes to arrive at a scoring model that comprises additional relevant aspects of speaking proficiency .

constraining the phrase-based , joint probability statistical translation
the joint probability model proposed by marcu and wong ( 2002 ) provides a strong probabilistic framework for phrase-based statistical machine translation ( smt ) . the models usefulness is , however , limited by the computational complexity of estimating parameters at the phrase level . we present the first model to use word alignments for constraining the space of phrasal alignments searched during expectation maximization ( em ) training . constraining the joint model improves performance , showing results that are very close to stateof-the-art phrase-based models . it also allows it to scale up to larger corpora and therefore be more widely applicable .

integrating surprisal and uncertain-input models in online sentence comprehension : formal techniques and empirical results
a system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input . under some circumstances , such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input . here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation , combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty . we also present a behavioral experiment confirming the key empirical predictions of the theory .

experiments with cst-based multidocument summarization
recently , with the huge amount of growing information in the web and the little available time to read and process all this information , automatic summaries have become very important resources . in this work , we evaluate deep content selection methods for multidocument summarization based on the cst model ( cross-document structure theory ) . our methods consider summarization preferences and focus on the overall main problems of multidocument treatment : redundancy , complementarity , and contradiction among different information sources . we also evaluate the impact of the cst model over superficial summarization systems . our results show that the use of cst model helps to improve informativeness and quality in automatic summaries .

dkie : open source information extraction for danish camilla vilhelmsen field
danish is a major scandinavian language spoken daily by around six million people . however , it lacks a unified , open set of nlp tools . this demonstration will introduce dkie , an extensible open-source toolkit for processing danish text . we implement an information extraction architecture for danish within gate , including integrated third-party tools . this implementation includes the creation of a substantial set of corpus annotations for dataintensive named entity recognition . the final application and dataset is made are openly available , and the part-of-speech tagger and ner model also operate independently or with the stanford nlp toolkit .

swiss-chocolate : sentiment detection using sparse svms and part-of-speech n-grams
we describe a classifier to predict the message-level sentiment of english microblog messages from twitter . this paper describes the classifier submitted to the semeval-2014 competition ( task 9b ) . our approach was to build up on the system of the last years winning approach by nrc canada 2013 , with some modifications and additions of features , and additional sentiment lexicons . furthermore , we used a sparse ( ` 1 -regularized ) svm , instead of the more commonly used ` 2 -regularization , resulting in a very sparse linear classifier .

monolingual machine translation for paraphrase generation chris quirk , chris brockett and william dolan
we apply statistical machine translation ( smt ) tools to generate novel paraphrases of input sentences in the same language . the system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the world wide web . alignment error rate ( aer ) is measured to gauge the quality of the resulting corpus . a monotone phrasal decoder generates contextual replacements . human evaluation shows that this system outperforms baseline paraphrase generation techniques and , in a departure from previous work , offers better coverage and scalability than the current best-of-breed paraphrasing approaches .

measuring non-native speakers proficiency of english by using a test with automatically-generated fill-in-the-blank questions spoken language communication spoken language communication
this paper proposes the automatic generation of fill-in-the-blank questions ( fbqs ) together with testing based on item response theory ( irt ) to measure english proficiency . first , the proposal generates an fbq from a given sentence in english . the position of a blank in the sentence is determined , and the word at that position is considered as the correct choice . the candidates for incorrect choices for the blank are hypothesized through a thesaurus . then , each of the candidates is verified by using the web . finally , the blanked sentence , the correct choice and the incorrect choices surviving the verification are together laid out to form the fbq . second , the proficiency of nonnative speakers who took the test consisting of such fbqs is estimated through irt . our experimental results suggest that : ( 1 ) the generated questions plus irt estimate the non-native speakers english proficiency ; ( 2 ) while on the other hand , the test can be completed almost perfectly by english native speakers ; and ( 3 ) the number of questions can be reduced by using item information in irt . the proposed method provides teachers and testers with a tool that reduces time and expenditure for testing english proficiency .

multilingual summarization evaluation without human models taln - dtic iria da cunha
we study correlation of rankings of text summarization systems using evaluation methods with and without human models . we apply our comparison framework to various well-established contentbased evaluation measures in text summarization such as coverage , responsiveness , pyramids and rouge studying their associations in various text summarization tasks including generic and focus-based multi-document summarization in english and generic single-document summarization in french and spanish . the research is carried out using a new content-based evaluation framework called fresa to compute a variety of divergences among probability distributions .

word sense induction for graded and non-graded senses dipartimento di informatica
most work on word sense disambiguation has assumed that word usages are best labeled with a single sense . however , contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage . we present a new semeval task for evaluating word sense induction and disambiguation systems in a setting where instances may be labeled with multiple senses , weighted by their applicability . four teams submitted nine systems , which were evaluated in two settings .

medeval a swedish medical test collection
medeval is a swedish medical test collection where assessments have been made , not only for topical relevance , but also for target reader group : doctors or patients . the user of the test collection can choose if s/he wishes to search in the doctors or the patients scenarios where the topical relevance assessments have been adjusted with consideration to user group , or to search in a scenario which regards only topical relevance . medeval makes it possible to compare the effectiveness of search terms when it comes to retrieving documents aimed at the different user groups . medeval is also the first medical swedish test collection .

enriching entity translation discovery using selective temporality
this paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities . our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1 % .

dynamic path prediction and recommendation in a museum environment
this research is concerned with making recommendations to museum visitors based on their history within the physical environment , and textual information associated with each item in their history . we investigate a method of providing such recommendations to users through a combination of language modelling techniques , geospatial modelling of the physical space , and observation of sequences of locations visited by other users in the past . this study compares and analyses different methods of path prediction including an adapted naive bayes method , document similarity , visitor feedback and measures of lexical similarity .

vowel and diacritic restoration for social media texts
in this paper , we focus on two important problems of social media text normalization , namely : vowel and diacritic restoration . for these two problems , we propose a hybrid model consisting both a discriminative sequence classifier and a language validator in order to select one of the morphologically valid outputs of the first stage . the proposed model is language independent and has no need for manual annotation of the training data . we measured the performance both on synthetic data specifically produced for these two problems and on real social media data . our model ( with 97.06 % on synthetic data ) improves the state of the art results for diacritization of turkish by 3.65 percentage points on ambiguous cases and for the vowel restoration by 45.77 percentage points over a rule based baseline with 62.66 % accuracy . the results on real data are 95.43 % and 69.56 % accordingly .

a lexically-driven algorithm for disfluency detection
this paper describes a transformation-based learning approach to disfluency detection in speech transcripts using primarily lexical features . our method produces comparable results to two other systems that make heavy use of prosodic features , thus demonstrating that reasonable performance can be achieved without extensive prosodic cues . in addition , we show that it is possible to facilitate the identification of less frequently disfluent discourse markers by taking speaker style into account .

techniques for text planning with
we describe an approach to text planning that uses the xslt template-processing engine to create logical forms for an external surface realizer . using a realizer that can process logical forms with embedded alternatives provides a substitute for backtracking in the text-planning process . this allows the text planner to combine the strengths of the ai-planning and template-based traditions in natural language generation .

from newspaper to microblogging : what does it take to find opinions applied computational linguistics
we compare the performance of two lexiconbased sentiment systems sentistrength ( thelwall et al , 2012 ) and so-cal ( taboada et al , 2011 ) on the two genres of newspaper text and tweets . while sentistrength has been geared specifically toward short social-media text , so-cal was built for general , longer text . after the initial comparison , we successively enrich the so-cal-based analysis with tweet-specific mechanisms and observe that in some cases , this improves the performance . a qualitative error analysis then identifies classes of typical problems the two systems have with tweets .

personage : personality generation for dialogue
over the last fifty years , the big five model of personality traits has become a standard in psychology , and research has systematically documented correlations between a wide range of linguistic variables and the big five traits . a distinct line of research has explored methods for automatically generating language that varies along personality dimensions . we present personage ( personality generator ) , the first highly parametrizable language generator for extraversion , an important aspect of personality . we evaluate two personality generation methods : ( 1 ) direct generation with particular parameter settings suggested by the psychology literature ; and ( 2 ) overgeneration and selection using statistical models trained from judges ratings . results show that both methods reliably generate utterances that vary along the extraversion dimension , according to human judges .

language-independent parsing with empty elements
we present a simple , language-independent method for integrating recovery of empty elements into syntactic parsing . this method outperforms the best published method we are aware of on english and a recently published method on chinese .

wordnet : :similarity - measuring the relatedness of concepts
wordnet : :similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts ( or synsets ) . it provides six measures of similarity , and three measures of relatedness , all of which are based on the lexical database wordnet . these measures are implemented as perl modules which take as input two concepts , and return a numeric value that represents the degree to which they are similar or related .

improving statistical machine translation in the medical domain using the unified medical language system
texts from the medical domain are an important task for natural language processing . this paper investigates the usefulness of a large medical database ( the unified medical language system ) for the translation of dialogues between doctors and patients using a statistical machine translation system . we are able to show that the extraction of a large dictionary and the usage of semantic type information to generalize the training data significantly improves the translation performance .

a unicode based adaptive segmentor
this paper presents a unicode based chinese word segmentor . it can handle chinese text in simplified , traditional , or mixed mode . the system uses the strategy of divide-and-conquer to handle the recognition of personal names , numbers , time and numerical values , etc in the preprocessing stage . the segmentor further uses tagging information to work on disambiguation . adopting a modular design approach , different functional parts are separately implemented using different modules and each module tackles one problem at a time providing more flexibility and extensibility . results show that with added pre-processing modules and accessorial modules , the accuracy of the segmentor is increased and the system is easily adaptive to different applications .

random restarts in minimum error rate training for statistical
ochs ( 2003 ) minimum error rate training ( mert ) procedure is the most commonly used method for training feature weights in statistical machine translation ( smt ) models . the use of multiple randomized starting points in mert is a well-established practice , although there seems to be no published systematic study of its benefits . we compare several ways of performing random restarts with mert . we find that all of our random restart methods outperform mert without random restarts , and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time .

noise-aware character alignment for bootstrapping statistical machine transliteration from bilingual corpora katsuhito sudoh shinsuke mori masaaki nagata
this paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs . the model is an extension of a bayesian many-to-many alignment method for distinguishing nontransliteration ( noise ) parts in phrase pairs . it worked effectively in the experiments of bootstrapping japanese-to-english statistical machine transliteration in patent domain using patent bilingual corpora .

easy as abc facilitating pictorial communication via semantically enhanced layout
pictorial communication systems convert natural language text into pictures to assist people with limited literacy . we define a novel and challenging problem : picture layout optimization . given an input sentence , we seek the optimal way to lay out word icons such that the resulting picture best conveys the meaning of the input sentence . to this end , we propose a family of intuitive abc layouts , which organize icons in three groups . we formalize layout optimization as a sequence labeling problem , employing conditional random fields as our machine learning method . enabled by novel applications of semantic role labeling and syntactic parsing , our trained model makes layout predictions that agree well with human annotators . in addition , we conduct a user study to compare our abc layout versus the standard linear layout . the study shows that our semantically enhanced layout is preferred by non-native speakers , suggesting it has the potential to be useful for people with other forms of limited literacy , too .

annotating semantic consistency of speech recognition hypotheses
recent work on natural language processing systems is aimed at more conversational , context-adaptive systems in multiple domains . an important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses . we report a pilot study addressing these tasks , the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators .

improved pronunciation features for construct-driven assessment of non-native spontaneous speech educational testing service
this paper describes research on automatic assessment of the pronunciation quality of spontaneous non-native adult speech . since the speaking content is not known prior to the assessment , a two-stage method is developed to first recognize the speaking content based on non-native speech acoustic properties and then forced-align the recognition results with a reference acoustic model reflecting native and near-native speech properties . features related to hidden markov model likelihoods and vowel durations are extracted . words with low recognition confidence can be excluded in the extraction of likelihood-related features to minimize erroneous alignments due to speech recognition errors . our experiments on the toefl rpractice online test , an english language assessment , suggest that the recognition/forced-alignment method can provide useful pronunciation features . our new pronunciation features are more meaningful than an utterance-based normalized acoustic model score used in previous research from a construct point of view .

global topology of word co-occurrence networks : beyond the two-regime power-law
word co-occurrence networks are one of the most common linguistic networks studied in the past and they are known to exhibit several interesting topological characteristics . in this article , we investigate the global topological properties of word co-occurrence networks and , in particular , present a detailed study of their spectrum . our experiments reveal certain universal trends found across the networks for seven different languages from three different language families , which are neither reported nor explained by any of the previous studies and models of word-cooccurrence networks . we hypothesize that since word co-occurrences are governed by syntactic properties of a language , the network has much constrained topology than that predicted by the previously proposed growth model . a deeper empirical and theoretical investigation into the evolution of these networks further suggests that they have a coreperiphery structure , where the core hardly evolves with time and new words are only attached to the periphery of the network . these properties are fundamental to the nature of word co-occurrence across languages .

influence of preprocessing on dependency syntax annotation : speed and
when creating a new resource , preprocessing the source texts before annotation is both ubiquitous and obvious . how the preprocessing affects the annotation effort for various tasks is for the most part an open question , however . in this paper , we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline . we also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.1

dependency-based statistical machine translation
we present a czech-english statistical machine translation system which performs tree-to-tree translation of dependency structures . the only bilingual resource required is a sentence-aligned parallel corpus . all other resources are monolingual . we also refer to an evaluation method and plan to compare our systems output with a benchmark system .

a spoken dialogue interface for tv operations based on data collected by using woz method
the development of multi-channel digital broadcasting has generated a demand not only for new services but also for smart and highly functional capabilities in all broadcast-related devices . this is especially true of the television receivers on the viewer 's side . with the aim of achieving a friendly interface that anybody can use with ease , we built a prototype interface system that operates a television through voice interactions using natural language . at the current stage of our research , we are using this system to investigate the usefulness and problem areas of the spoken dialogue interface for television operations .

a graph-based approach to contextualizing distributional similarity
we introduce an interactive visualization component for the jobimtext project . jobimtext is an open source platform for large-scale distributional semantics based on graph representations . first we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features , and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking . then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization . the visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering , and is provided as an open source tool .

looking at unbalanced specialized comparable corpora for bilingual lexicon extraction
the main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced . however , the historical contextbased projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus . within this context , we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments . moreover , we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method . our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons .

speech translation performance of statistical dependency transduction and semantic similarity transduction
in this paper we compare the performance of two methods for speech translation . one is a statistical dependency transduction model using head transducers , the other a case-based transduction model involving a lexical similarity measure . examples of translated utterance transcriptions are used in training both models , though the case-based model also uses semantic labels classifying the source utterances . the main conclusion is that while the two methods provide similar translation accuracy under the experimental conditions and accuracy metric used , the statistical dependency transduction method is significantly faster at computing translations .

